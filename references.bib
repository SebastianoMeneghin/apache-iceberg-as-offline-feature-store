@inproceedings{10.1007/978-3-031-35260-7_5,
  title = {The Impact of Importance-Aware Dataset Partitioning on Data-Parallel Training of Deep Neural Networks},
  booktitle = {Distributed Applications and Interoperable Systems},
  author = {Sheikholeslami, Sina and Payberah, Amir H. and Wang, Tianze and Dowling, Jim and Vlassov, Vladimir},
  editor = {{Pati{\~n}o-Mart{\'i}nez}, Marta and Paulo, Jo{\~a}o},
  year = {2023},
  pages = {74--89},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-35260-7_5},
  abstract = {Deep neural networks used for computer vision tasks are typically trained on datasets consisting of thousands of images, called examples. Recent studies have shown that examples in a dataset are not of equal importance for model training and can be categorized based on quantifiable measures reflecting a notion of ``hardness'' or ``importance''. In this work, we conduct an empirical study of the impact of importance-aware partitioning of the dataset examples across workers on the performance of data-parallel training of deep neural networks. Our experiments with CIFAR-10 and CIFAR-100 image datasets show that data-parallel training with importance-aware partitioning can perform better than vanilla data-parallel training, which is oblivious to the importance of examples. More specifically, the proper choice of the importance measure, partitioning heuristic, and the number of intervals for dataset repartitioning can improve the best accuracy of the model trained for a fixed number of epochs. We conclude that the parameters related to importance-aware data-parallel training, including the importance measure, number of warmup training epochs, and others defined in the paper, may be considered as hyperparameters of data-parallel model training.},
  isbn = {978-3-031-35260-7},
  file = {C:\Users\Sebas\Zotero\storage\Q8CB9D7L\Sheikholeslami et al. - 2023 - The impact of importance-aware dataset partitionin.pdf}
}

@article{10.1145/3591335.3591347,
  title = {{{ANIARA}} Project - Automation of Network Edge Infrastructure and Applications with Artificial Intelligence},
  author = {John, Wolfgang and Balador, Ali and Taghia, Jalil and Johnsson, Andreas and Sj{\"o}berg, Johan and Marsh, Ian and Gustafsson, Jonas and Tonini, Federico and Monti, Paolo and Sk{\"o}ldstr{\"o}m, Pontus and Dowling, Jim},
  year = {2023},
  month = apr,
  journal = {Ada Lett.},
  volume = {42},
  number = {2},
  pages = {92--95},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {1094-3641},
  doi = {10.1145/3591335.3591347},
  abstract = {Emerging use-cases like smart manufacturing and smart cities pose challenges in terms of latency, which cannot be satisfied by traditional centralized infrastructure. Edge networks, which bring computational capacity closer to the users/clients, are a promising solution for supporting these critical low latency services. Different from traditional centralized networks, the edge is distributed by nature and is usually equipped with limited compute capacity. This creates a complex network to handle, subject to failures of different natures, that requires novel solutions to work in practice. To reduce complexity, edge application technology enablers, advanced infrastructure and application orchestration techniques need to be in place where AI and ML are key players.},
  issue_date = {December 2022},
  file = {C:\Users\Sebas\Zotero\storage\ITLFTN2Q\John et al. - 2023 - ANIARA project - automation of network edge infras.pdf}
}

@inproceedings{ahmadBenchmarkingApacheArrow2022,
  title = {Benchmarking {{Apache Arrow Flight}} - {{A}} Wire-Speed Protocol for Data Transfer, Querying and Microservices},
  booktitle = {Benchmarking in the {{Data Center}}: {{Expanding}} to the {{Cloud}}},
  author = {Ahmad, Tanveer},
  year = {2022},
  month = apr,
  pages = {1--10},
  publisher = {ACM},
  address = {Seoul Republic of Korea},
  doi = {10.1145/3527199.3527264},
  urldate = {2024-03-04},
  isbn = {978-1-4503-9324-9},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\UWX6GU4R\Ahmad - 2022 - Benchmarking Apache Arrow Flight - A wire-speed pr.pdf}
}

@mastersthesis{AlSamisti1181210,
  title = {Visual Debugging of Dataflow Systems},
  author = {Al Samisti, Fanti Machmount},
  year = {2017},
  series = {{{TRITA-ICT-EX}}},
  number = {2017:152},
  pages = {62},
  abstract = {Big data processing has seen vast integration into the idea of data analysis in live streaming and batch environments. A plethora of tools have been developed to break down a problem into manageable tasks and to allocate both software and hardware resources in a distributed and fault tolerant manner. Apache Spark is one of the most well known platforms for large-scale cluster computation. In SICS Swedish ICT, Spark runs on top of an in-house developed solution. HopsWorks provides a graphical user interface to the Hops platform that aims to simplify the process of configuring a Hadoop environment and improving upon it. The user interface includes, among other capabilities, an array of tools for executing distributed applications such as Spark, TensorFlow, Flink with a variety of input and output sources, e.g. Kafka, HDFS files etc. Currently the available tools to monitor and instrument a stack that includes the aforementioned technologies come from both the corporate and open source world. The former is usually part of a bigger family of products running on proprietary code. In contrast, the latter offers a wider variety of choices with the most prominent ones lacking either the flexibility in exchange for a more generic approach or the ease of gaining meaningful insight except of the most experienced users. The contribution of this project is a visualization tool in the form of a web user interface, part of the Hops platform, for understanding, debugging and ultimately optimizing the resource allocation and performance of dataflow applications. These processes are based both on the abstraction provided by the dataflow programming paradigm and on systems concepts such as properties of data, how much variability in the data, computation, distribution, and other system wide resources.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT)},
  file = {C:\Users\Sebas\Zotero\storage\FLWZI622\Al Samisti - 2017 - Visual debugging of dataflow systems.pdf}
}

@article{armbrustLakehouseNewGeneration2021,
  title = {Lakehouse: {{A New Generation}} of {{Open Platforms}} That {{Unify Data Warehousing}} and {{Advanced Analytics}}},
  author = {Armbrust, Michael and Ghodsi, Ali and Xin, Reynold and Zaharia, Matei},
  year = {2021},
  abstract = {This paper argues that the data warehouse architecture as we know it today will wither in the coming years and be replaced by a new architectural pattern, the Lakehouse, which will (i) be based on open direct-access data formats, such as Apache Parquet, (ii) have firstclass support for machine learning and data science, and (iii) offer state-of-the-art performance. Lakehouses can help address several major challenges with data warehouses, including data staleness, reliability, total cost of ownership, data lock-in, and limited use-case support. We discuss how the industry is already moving toward Lakehouses and how this shift may affect work in data management. We also report results from a Lakehouse system using Parquet that is competitive with popular cloud data warehouses on TPC-DS.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\V7H69UZK\Armbrust et al. - 2021 - Lakehouse A New Generation of Open Platforms that.pdf}
}

@inproceedings{azaguryObjectStore2003,
  title = {Towards an Object Store},
  booktitle = {20th {{IEEE}}/11th {{NASA Goddard Conference}} on {{Mass Storage Systems}} and {{Technologies}}, 2003. ({{MSST}} 2003). {{Proceedings}}.},
  author = {Azagury, A. and Dreizin, V. and Factor, M. and Henis, E. and Naor, D. and Rinetzky, N. and Rodeh, O. and Satran, J. and Tavory, A. and Yerushalmi, L.},
  year = {2003},
  pages = {165--176},
  publisher = {IEEE Comput. Soc},
  address = {San Diego, CA, USA},
  doi = {10.1109/MASS.2003.1194853},
  urldate = {2024-02-20},
  abstract = {Today's SAN architectures promise unmediated host access to storage (i.e., without going through a server). To achieve this promise, however, we must address several issues and opportunities raised by SANs, including security, scalability and management. Object storage, such as introduced by the NASD work [14], is a means of addressing these issues and opportunities. An object store raises the level of abstraction presented by a storage control unit from an array of 512 byte blocks to a collection of objects. The object store provides ``fine-grain,'' object-level security, improved scalability by localizing space management, and improved management by allowing end-to-end management of semantically meaningful entities.},
  isbn = {978-0-7695-1914-2},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\UKGIYZM7\Azagury et al. - 2003 - Towards an object store.pdf}
}

@article{belovAnalysisBigData2021,
  title = {Analysis of {{Big Data Storage Tools}} for {{Data Lakes}} Based on {{Apache Hadoop Platform}}},
  author = {Belov, Vladimir and Nikulchev, Evgeny},
  year = {2021},
  month = aug,
  journal = {International Journal of Advanced Computer Science and Applications},
  volume = {12},
  pages = {551--557},
  doi = {10.14569/IJACSA.2021.0120864},
  abstract = {When developing large data processing systems, the question of data storage arises. One of the modern tools for solving this problem is the so-called data lakes. Many implementations of data lakes use Apache Hadoop as a basic platform. Hadoop does not have a default data storage format, which leads to the task of choosing a data format when designing a data processing system. To solve this problem, it is necessary to proceed from the results of the assessment according to several criteria. In turn, experimental evaluation does not always give a complete understanding of the possibilities for working with a particular data storage format. In this case, it is necessary to study the features of the format, its internal structure, recommendations for use, etc. The article describes the features of both widely used data storage formats and the currently gaining popularity.},
  file = {C:\Users\Sebas\Zotero\storage\9XXW6REL\Belov and Nikulchev - 2021 - Analysis of Big Data Storage Tools for Data Lakes .pdf}
}

@incollection{bonomiImprovedConstructionCounting2006,
  title = {An {{Improved Construction}} for {{Counting Bloom Filters}}},
  booktitle = {Algorithms -- {{ESA}} 2006},
  author = {Bonomi, Flavio and Mitzenmacher, Michael and Panigrahy, Rina and Singh, Sushil and Varghese, George},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Azar, Yossi and Erlebach, Thomas},
  year = {2006},
  volume = {4168},
  pages = {684--695},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11841036_61},
  urldate = {2024-03-06},
  abstract = {A counting Bloom filter (CBF) generalizes a Bloom filter data structure so as to allow membership queries on a set that can be changing dynamically via insertions and deletions. As with a Bloom filter, a CBF obtains space savings by allowing false positives. We provide a simple hashing-based alternative based on d-left hashing called a d-left CBF (dlCBF). The dlCBF offers the same functionality as a CBF, but uses less space, generally saving a factor of two or more. We describe the construction of dlCBFs, provide an analysis, and demonstrate their effectiveness experimentally.},
  isbn = {978-3-540-38875-3 978-3-540-38876-0},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\JCSMDXLT\Bonomi et al. - 2006 - An Improved Construction for Counting Bloom Filter.pdf}
}

@mastersthesis{Buso1149002,
  title = {{{SQL}} on Hops},
  author = {Buso, Fabio},
  year = {2017},
  series = {{{TRITA-ICT-EX}}},
  number = {2017:146},
  pages = {50},
  abstract = {In today's world data is extremely valuable. Companies and researchers store every sort of data, from users activities to medical records. However, data is useless if one cannot extract meaning and insight from it. In 2004 Dean and Ghemawat introduced the MapReduce framework. This sparked the development of open source frameworks for big data storage (HDFS) and processing (Hadoop). Hops and Apache Hive build on top of this heritage. The former proposes a new distributed file system which achieves higher scalability and throughput by storing metadata in a database called MySQL-Cluster. The latter is an open source data warehousing solution built on top of the Hadoop ecosystems, which allows users to query big data stored on HDFS using a SQL-like query language.Apache Hive is a widely used and mature project, however it lacks of consistency between the data stored on the file system and the metadata describing it, stored on a relational database. This means that if users delete Hive's data from the file system, Hive does not delete the related metadata. This causes two issues: (1) users do not get an error if the data is missing from the filesystem (2) if users forget to delete the metadata, it will become orphaned in the database. In this thesis we exploit the fact that both HopsFS' metadata and Hive's metadata is stored in a relational database, to provide a mechanisms to automatically delete Hive's metadata if the data is delete from the file system.The second objective of this thesis is to integrate Apache Hive into the Hops ecosystem and in particular in the HopsWorks platform. HopsWorks is a multitenant, UI based service which allows users to store and process big data projects. In this thesis we develop a custom authenticator for Hive to allow HopsWorks users to authenticate with Hive and to integrate with its security model.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT)},
  file = {C:\Users\Sebas\Zotero\storage\2LR5YEK5\Buso - 2017 - SQL on hops.pdf}
}

@misc{camacho-rodriguezLSTBenchBenchmarkingLogStructured2024,
  title = {{{LST-Bench}}: {{Benchmarking Log-Structured Tables}} in the {{Cloud}}},
  shorttitle = {{{LST-Bench}}},
  author = {{Camacho-Rodr{\'i}guez}, Jes{\'u}s and Agrawal, Ashvin and Gruenheid, Anja and Gosalia, Ashit and Petculescu, Cristian and {Aguilar-Saborit}, Josep and Floratou, Avrilia and Curino, Carlo and Ramakrishnan, Raghu},
  year = {2024},
  month = jan,
  eprint = {2305.01120},
  primaryclass = {cs},
  doi = {10.1145/3639314},
  urldate = {2024-02-16},
  abstract = {Data processing engines increasingly leverage distributed file systems for scalable, cost-effective storage. While the Apache Parquet columnar format has become a popular choice for data storage and retrieval, the immutability of Parquet files renders it impractical to meet the demands of frequent updates in contemporary analytical workloads. Log-Structured Tables (LSTs), such as Delta Lake, Apache Iceberg, and Apache Hudi, offer an alternative for scenarios requiring data mutability, providing a balance between efficient updates and the benefits of columnar storage. They provide features like transactions, time-travel, and schema evolution, enhancing usability and enabling access from multiple engines. Moreover, engines like Apache Spark and Trino can be configured to leverage the optimizations and controls offered by LSTs to meet specific business needs. Conventional benchmarks and tools are inadequate for evaluating the transformative changes in the storage layer resulting from these advancements, as they do not allow us to measure the impact of design and optimization choices in this new setting. In this paper, we propose a novel benchmarking approach and metrics that build upon existing benchmarks, aiming to systematically assess LSTs. We develop a framework, LST-Bench, which facilitates effective exploration and evaluation of the collaborative functioning of LSTs and data processing engines through tailored benchmark packages. A package is a mix of use patterns reflecting a target workload; LST-Bench makes it easy to define a wide range of use patterns and combine them into a package, and we include a baseline package for completeness. Our assessment demonstrates the effectiveness of our framework and benchmark packages in extracting valuable insights across diverse environments. The code for LST-Bench is open-sourced and is available at https://github.com/microsoft/lst-bench/ .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\CWQ8CNUP\\Camacho-Rodríguez et al. - 2024 - LST-Bench Benchmarking Log-Structured Tables in t.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\QUMDKK3X\\2305.html}
}

@mastersthesis{Chen1705419,
  title = {Data Build Tool ({{DBT}}) Jobs in Hopsworks},
  author = {Chen, Zidi},
  year = {2022},
  series = {{{TRITA-EECS-EX}}},
  number = {2022:402},
  pages = {51},
  abstract = {Feature engineering at scale is always critical and challenging in the machine learning pipeline. Modern data warehouses enable data analysts to do feature engineering by transforming, validating and aggregating data in Structured Query Language (SQL). To help data analysts do this work, Data Build Tool (DBT), an open-source tool, was proposed to build and orchestrate SQL pipelines. Hopsworks, an open-source scalable feature store, would like to add support for DBT so that data scientists can do feature engineering in Python, Spark, Flink, and SQL in a single platform. This project aims to create a concept about how to build this support and then implement it. The project checks the feasibility of the solution using a sample DBT project. According to measurements, this working solution needs around 800 MB of space in the server and it takes more time than executing DBT commands locally. However, it persistently stores the results of each execution in HopsFS, which are available to users. By adding this novel support for SQL using DBT, Hopsworks might be one of the completest platforms for feature engineering so far.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {feature engineering,funktionsteknik,Structured Query Language (SQL),strukturerat fragesprak (SQL)},
  file = {C:\Users\Sebas\Zotero\storage\SRJ8FSCZ\Chen - 2022 - Data build tool (DBT) jobs in hopsworks.pdf}
}

@mastersthesis{Chikafa1614362,
  title = {Project Based Multi-Tenant Managed {{RStudio}} on {{Kubernetes}} for {{Hopsworks}}},
  author = {Chikafa, Gibson},
  year = {2021},
  series = {{{TRITA-EECS-EX}}},
  number = {2021:727},
  pages = {67},
  abstract = {In order to fully benefit from cloud computing, services are designed following the ``multi-tenant'' architectural model which is aimed at maximizing resource sharing among users. However, multi-tenancy introduces challenges of security, performance isolation, scaling and customization. RStudio server is an open source Integrated Development Environment (IDE) accessible over a web browser for R programming language. The purpose of this thesis is to develop an open source multi-user distributed system on Hopsworks, a data intensive and AI platform, following the multi-tenant model, that provides RStudio as Software as a Service (SaaS). Our goal is to promote collaboration among users when using RStudio and the learning and teaching of R by enabling users easily have access to same computational environments and resources while eliminating installation and maintenance tasks. Hopsworks introduces project-based multi-tenancy where users within a project share project resources (e.g datasets, programs and services) for collaboration which introduces one more challenge of sharing project resources in RStudio server instances. To achieve our purpose and goal we therefore needed to solve the following problems: performance isolation, security, project resources sharing, scaling and customization. Our proposed model is on demand single user RStudio server instances per project. Our system is built around Docker and Kubernetes to solve the problems of performance isolation, security and scaling. We introduce HopsFS-mount, that allows securely mounting HopsFS via FUSE to solve the project resources (datasets and programs) sharing problem. We integrate our system with Apache Spark which can scale and handle Big Data processing workloads. Also we provide a UI where users can provide custom configuration and have full control of their own RStudio server instances. Our system was tested on a GCP cluster with four worker nodes each with 30GB of RAM allocated to them. The tests on this cluster showed that 44 RStudio servers, each with 2GB of RAM, can be run concurrently. Our system can scale out to potentially support hundreds of concurrently running RStudio servers by adding more resources (CPUs and RAM) to the cluster or system.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Azure,Cloud computing,Docker,GCP,Kubernetes,Molntjanster,Multi-tenancy,Multitenans,Performance isolation,Prestandaisolering,Sakerhet,Scaling,Security,Skalning},
  file = {C:\Users\Sebas\Zotero\storage\B6PIHFBZ\Chikafa - 2021 - Project based multi-tenant managed RStudio on Kube.pdf}
}

@inproceedings{delaruamartinezHopsworksFeatureStore2024,
  title = {The {{Hopsworks Feature Store}} for {{Machine Learning}}},
  booktitle = {Proceedings of {{SIGMOD}}},
  author = {{de la R{\'u}a Mart{\'i}nez}, Javier and Buso, Fabio and Kouzoupis, Antonios and Ormenisan, Alexandru A. and Niazi, Salman and Bzhalava, Davit and Mak, Kenneth and Jouffrey, Victor and Ronstr{\"o}m, Mikael and Cunningham, Raymond and Zangis, Ralfs and Mukhedkar, Dhananjay and Khazanchi, Ayushman and Vlassov, Vladimir and Dowling, Jim},
  year = {2024},
  address = {Santiago, Chile},
  abstract = {Data management is the most challenging aspect of building Machine Learning (ML) systems. ML systems can read large volumes of historical data when training models, but inference workloads are more varied, depending on whether it is a batch or online ML system. The feature store for ML has recently emerged as a single data platform for managing ML data throughout the ML lifecycle, from feature engineering to model training to inference.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\ITNYNIV7\de la Rúa Martínez et al. - 2024 - The Hopsworks Feature Store for Machine Learning.pdf}
}

@mastersthesis{DessalegnMuruts1091136,
  title = {Multi-Tenant Apache Kafka for Hops : {{Kafka}} Topic-Based Multi-Tenancy and {{ACL-}} Based Authorization for Hops},
  author = {Dessalegn Muruts, Misganu},
  year = {2016},
  series = {{{TRITA-ICT-EX}}},
  number = {2016:120},
  pages = {52},
  abstract = {Apache Kafka is a distributed, high throughput and fault-tolerant publish/subscribe messaging system in the Hadoop ecosystem. It is used as a distributed data streaming and processing platform. Kafka topics are the units of message feeds in the Kafka cluster. Kafka producer publishes messages into these topics and a Kafka consumer subscribes to topics to pull those messages. With the increased usage of Kafka in the data infrastructure of many companies, there are many Kafka clients that publish and consume messages to/from the Kafka topics. In fact, these client operations can be malicious. To mitigate this risk, clients must authenticate themselves and their operation must be authorized before they can access to a given topic. Nowadays, Kafka ships with a pluggable Authorizer interface to implement access control list (ACL) based authorization for client operation. Kafka users can implement the interface differently to satisfy their security requirements. SimpleACLAuthorizer is the out-of-box implementation of the interface and uses a Zookeeper for ACLs storage.HopsWorks, based on Hops a next generation Hadoop distribution, provides support for project-based multi-tenancy, where projects are fully isolated at the level of the Hadoop Filesystem and YARN. In this project, we added Kafka topicbased multi-tenancy in Hops projects. Kafka topic is created from inside Hops project and persisted both at the Zookeeper and the NDBCluster. Persisting a topic into a database enabled us for topic sharing across projects. ACLs are added to Kafka topics and are persisted only into the database. Client access to Kafka topics is authorized based on these ACLs. ACLs are added, updated, listed and/or removed from the HopsWorks WebUI. HopsACLAuthorizer, a Hops implementation of the Authorizer interface, authorizes Kafka client operations using the ACLs in the database. The Apache Avro schema registry for topics enabled the producer and consumer to better integrate by transferring a preestablished message format. The result of this project is the first Hadoop distribution that supports Kafka multi-tenancy.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT)},
  keywords = {ACL Authorization,Hadoop,Hops,HopsWorks,Kafka,Kafka Topics,Messaging Systems,Multi-Tenancy,Schema Registry},
  file = {C:\Users\Sebas\Zotero\storage\NMXE2CME\Dessalegn Muruts - 2016 - Multi-tenant apache kafka for hops  Kafka topic-b.pdf}
}

@inproceedings{factorObjectStorageFuture2005,
  title = {Object Storage: The Future Building Block for Storage Systems},
  shorttitle = {Object Storage},
  booktitle = {2005 {{IEEE International Symposium}} on {{Mass Storage Systems}} and {{Technology}}},
  author = {Factor, M. and Meth, K. and Naor, D. and Rodeh, O. and Satran, J.},
  year = {2005},
  month = jun,
  pages = {119--123},
  doi = {10.1109/LGDI.2005.1612479},
  urldate = {2024-02-20},
  abstract = {The concept of object storage was introduced in the early 1990's by the research community. Since then it has greatly matured and is now in its early stages of adoption by the industry. Yet, object storage is still not widely accepted. Viewing object store technology as the future building block particularly for large storage systems, our team in IBM Haifa Research Lab has invested substantial efforts in this area. In this position paper we survey the latest developments in the area of object store technology, focusing on standardization, research prototypes, and technology adoption and deployment. A major step has been the approval of the TIO OSD protocol (version I) as an OSD standard in late 2004. We also report on prototyping efforts that are carried out in IBM Haifa Research Lab in building an object store. Our latest prototype is compliant with a large subset of the TIO standard. To facilitate deployment of the new technology and protocol in the community at large, our team also implemented a TIO-compliant OSD (iSCSI) initiator for Linux. The initiator is interoperable with object disks of other vendors. The initiator is available as an open source driver for Linux.},
  keywords = {Access control,File servers,Linux,Logic arrays,Paper technology,Protocols,Prototypes,Secure storage,Space technology,Storage area networks},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\FZL8I7VH\\Factor et al. - 2005 - Object storage the future building block for stor.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\4ZW8HYXL\\1612479.html}
}

@mastersthesis{FilotasSiskos1301514,
  title = {Secure Data Streaming into {{HopsWorks}} from Occasionally Connected Mobile Devices},
  author = {Filotas Siskos, Stavros},
  year = {2018},
  series = {{{TRITA-EECS-EX}}},
  number = {2018:20},
  pages = {52},
  abstract = {The number of devices that are connected to the internet has recently surpassed the number of human beings living on our planet. These devices are capable of generating a tremendous amount of data. Organizations, researchers and companies are faced with unique challenges when collecting, storing and analysing the data for the benefit and progress of the human kind. To address such challenges, cutting-edge Big Data platforms are needed like the Hadoop Open Platform-as-a-Service (Hops) ecosystem which is the resulting work of many years of continuous research conducted at RISE SICS in collaboration with KTH Royal Institute of Technology. This master thesis provides a means for ingesting streams of data generated from internet enabled devices into Hops in a secure and reliable way for storage and further stream processing and data analysis. In order to accomplish data ingestion, the HopsWorks component of Hops has been extended with an API where authenticated devices can stream data to. Furthermore, as a use case scenario the hopsworks-android-client library has been built in the Android platform for facilitating the secure and reliable data collection of streams of records from any Android application. The library has also been used as a testbed for performing system reliability and system end-to-end performance experiments for the developed system extension. The results show that this approach works and since the system is based on the well defined and heavily used HTTPS protocol it enables any interested stakeholder to collect data from mobile or IoT devices into Hops in order to not only utilize all the functional features that Hops offers but also to leverage its desired system properties such as high-availability, high-performance and scalability.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)}
}

@phdthesis{Gebremeskel1224181,
  title = {Analysis and Comparison of Distributed Training Techniques for Deep Neural Networks in a Dynamic Environment},
  author = {Gebremeskel, Ermias},
  year = {2018},
  series = {{{TRITA-EECS-EX}}},
  number = {2018:375},
  pages = {72},
  abstract = {Deep learning models' prediction accuracy tends to improve with the size of the model. The implications being that the amount of computational power needed to train models is continuously increasing. Distributed deep learning training tries to address this issue by spreading the computational load onto several devices. In theory, distributing computation onto N devices should give a performance improvement of xN. Yet, in reality the performance improvement is rarely xN, due to communication and other overheads. This thesis will study the communication overhead incurred when distributing deep learning training. Hopsworks is a platform designed for data science. The purpose of this work is to explore a feasible way of deploying distributed deep learning training on a shared cluster and analyzing the performance of different distributed deep learning algorithms to be used on this platform. The findings of this study show that bandwidth-optimal communication algorithms like ring all-reduce scales better than many-to-one communication algorithms like parameter server, but were less fault tolerant. Furthermore, system usage statistics collected revealed a network bottleneck when training is distributed on multiple machines. This work also shows that it is possible to run MPI on a hadoop cluster by building a prototype that orchestrates resource allocation, deployment, and monitoring of MPI based training jobs. Even though the experiments did not cover different cluster configurations, the results are still relevant in showing what considerations need to be made when distributing deep learning training.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {data parallelism,deep learning,large scale distributed deep learning},
  file = {C:\Users\Sebas\Zotero\storage\3YZYBSYD\Gebremeskel - 2018 - Analysis and comparison of distributed training te.pdf}
}

@mastersthesis{GebretsadkanKidane1413103,
  title = {Hudi on Hops : {{Incremental}} Processing and Fast Data Ingestion for Hops},
  author = {Gebretsadkan Kidane, Netsanet},
  year = {2019},
  series = {{{TRITA-EECS-EX}}},
  number = {2019:809},
  pages = {49},
  abstract = {In the era of big data, data is flooding from numerous data sources and many companies have been utilizing different types of tools to load and process data from various sources in a data lake. The major challenges where different companies are facing these days are how to update data into an existing dataset without having to read the entire dataset and overwriting it to accommodate the changes which have a negative impact on the performance. Besides this, finding a way to capture and track changed data in a big data lake as the system gets complex with large amounts of data to maintain and query is another challenge. Web platforms such as Hopsworks are also facing these problems without having an efficient mechanism to modify an existing processed results and pull out only changed data which could be useful to meet the processing needs of an organization. The challenge of accommodating row level changes in an efficient and effective manner is solved by integrating Hudi with Hops. This takes advantage of Hudi's upsert mechanism which uses Bloom indexing to significantly speed up the ability of looking up records across partitions. Hudi indexing maps a record key into the file id without scanning over every record in the dataset. In addition, each successful data ingestion is stored in Apache Hudi format stamped with commit timeline. This commit timeline is needed for the incremental processing mainly to pull updated rows since a specified instant of time and obtain change logs from a dataset. Hence, incremental pulls are realized through the monotonically increasing commit time line. Similarly, incremental updates are realized over a time column (key expression) that allows Hudi to update rows based on this time column. HoodieDeltaStreamer utility and DataSource API are used for the integration of Hudi with Hops and Feature store. As a result, this provided a fabulous way of ingesting and extracting row level updates where its performance can further be enhanced by the configurations of the shuffle parallelism and other spark parameter configurations since Hudi is a spark based library.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Hadoop,Hops,Hudi,Kafka,Spark,SQL,Upsert},
  file = {C:\Users\Sebas\Zotero\storage\IEQCQMFC\Gebretsadkan Kidane - 2019 - Hudi on hops  Incremental processing and fast dat.pdf}
}

@mastersthesis{Gustafson1707793,
  title = {Improving Availability of Stateful Serverless Functions in Apache Flink},
  author = {Gustafson, Christopher},
  year = {2022},
  series = {{{TRITA-EECS-EX}}},
  number = {2022:531},
  pages = {51},
  abstract = {Serverless computing and Function-as-a-Service are rising in popularity due to their ease of use, provided scalability and cost-efficient billing model. One such platform is Apache Flink Stateful Functions. It allows application developers to run serverless functions with state that is persisted using the underlying stream processing engine Apache Flink. Stateful Functions use an embedded RocksDB state backend, where state is stored locally at each worker. One downside of this architecture is that state is lost if a worker fails. To recover, a recent snapshot of the state is fetched from a persistent file system. This can be a costly operation if the size of the state is large. In this thesis, we designed and developed a new decoupled state backend for Apache Flink Stateful Functions, with the goal of increasing availability while measuring potential performance trade-offs. It extends an existing decoupled state backend for Flink, FlinkNDB, to support the operations of Stateful Functions. FlinkNDB stores state in a separate highly available database, RonDB, instead of locally at the worker nodes. This allows for fast recovery as large state does not have to be transferred between nodes. Two new recovery methods were developed, eager and lazy recovery. The results show that lazy recovery can decrease recovery time by up to 60\% compared to RocksDB when the state is large. Eager recovery did not provide any recovery time improvements. The measured performance was similar between RocksDB and FlinkNDB. Checkpointing times in FlinkNDB were however longer, which cause short periodic performance degradation. The evaluation of FlinkNDB suggests that decoupled state can be used to improve availability, but that there might be performance deficits included. The proposed solution could thus be a viable option for applications with high requirements of availability and lower performance requirements.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Apache Flink StateFun,ApacheFlink StateFun,Availability,Function-as-a-Service,RocksDB,RonDB,Stateful Serverless Functions,Tillganglighet,Tillstandsbaserade ServerlosaFunktioner},
  file = {C:\Users\Sebas\Zotero\storage\RT5MXWNH\Gustafson - 2022 - Improving availability of stateful serverless func.pdf}
}

@article{Hagos1601997,
  title = {{{ExtremeEarth}} Meets Satellite Data from Space},
  author = {Hagos, Desta Haileselassie and Kakantousis, Theofilos and Vlassov, Vladimir and Sheikholeslami, Sina and Wang, Tianze and Dowling, Jim and Paris, Claudia and Marinelli, Daniele and Weikmann, Giulio and Bruzzone, Lorenzo and Khaleghian, Salman and Kraemer, Thomas and Eltoft, Torbjorn and Marinoni, Andrea and Pantazi, Despina-Athanasia and Stamoulis, George and Bilidas, Dimitris and Papadakis, George and Mandilaras, George and Koubarakis, Manolis and Troumpoukis, Antonis and Konstantopoulos, Stasinos and Muerth, Markus and Appel, Florian and Fleming, Andrew and Cziferszky, Andreas},
  year = {2021},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume = {14},
  pages = {9038--9063},
  publisher = {VISTA Remote Sensing Geosci GmbH, D-80333 Munich, Germany.},
  doi = {10.1109/JSTARS.2021.3107982},
  abstract = {Bringing together a number of cutting-edge technologies that range from storing extremely large volumes of data all the way to developing scalable machine learning and deep learning algorithms in a distributed manner and having them operate over the same infrastructure poses unprecedented challenges. One of these challenges is the integration of European Space Agency (ESA)'s Thematic Exploitation Platforms (TEPs) and data information access service platforms with a data platform, namely Hopsworks, which enables scalable data processing, machine learning, and deep learning on Copernicus data, and development of very large training datasets for deep learning architectures targeting the classification of Sentinel images. In this article, we present the software architecture of ExtremeEarth that aims at the development of scalable deep learning and geospatial analytics techniques for processing and analyzing petabytes of Copernicus data. The ExtremeEarth software infrastructure seamlessly integrates existing and novel software platforms and tools for storing, accessing, processing, analyzing, and visualizing large amounts of Copernicus data. New techniques in the areas of remote sensing and artificial intelligence with an emphasis on deep learning are developed. These techniques and corresponding software presented in this article are to be integrated with and used in two ESA TEPs, namely Polar and Food Security TEPs. Furthermore, we present the integration of Hopsworks with the Polar and Food Security use cases and the flow of events for the products offered through the TEPs.},
  keywords = {Artificial intelligence (AI),Computer architecture,copernicus,Data models,Deep learning,earth observation (EO),extremeearth,food security,Geospatial analysis,hopsworks,linked geospatial data,Monitoring,polar regions,remote sensing,Satellites,Sea ice},
  file = {C:\Users\Sebas\Zotero\storage\AXPEKCDV\Hagos et al. - 2021 - ExtremeEarth meets satellite data from space.pdf}
}

@article{Hagos1656656,
  title = {Scalable Artificial Intelligence for Earth Observation Data Using Hopsworks},
  author = {Hagos, Desta Haileselassie and Kakantousis, Theofilos and Sheikholeslami, Sina and Wang, Tianze and Vlassov, Vladimir and Payberah, Amir Hossein and Meister, Moritz and Andersson, Robin and Dowling, Jim},
  year = {2022},
  journal = {Remote Sensing},
  volume = {14},
  number = {1889},
  publisher = {MDPI AG},
  doi = {10.3390/rs14081889},
  abstract = {This paper introduces the Hopsworks platform to the entire Earth Observation (EO) data community and the Copernicus programme. Hopsworks is a scalable data-intensive open-source Artificial Intelligence (AI) platform that was jointly developed by Logical Clocks and the KTH Royal Institute of Technology for building end-to-end Machine Learning (ML)/Deep Learning (DL) pipelines for EO data. It provides the full stack of services needed to manage the entire life cycle of data in ML. In particular, Hopsworks supports the development of horizontally scalable DL applications in notebooks and the operation of workflows to support those applications, including parallel data processing, model training, and model deployment at scale. To the best of our knowledge, this is the first work that demonstrates the services and features of the Hopsworks platform, which provide users with the means to build scalable end-to-end ML/DL pipelines for EO data, as well as support for the discovery and search for EO metadata. This paper serves as a demonstration and walkthrough of the stages of building a production-level model that includes data ingestion, data preparation, feature extraction, model training, model serving, and monitoring. To this end, we provide a practical example that demonstrates the aforementioned stages with real-world EO data and includes source code that implements the functionality of the platform. We also perform an experimental evaluation of two frameworks built on top of Hopsworks, namely Maggy and AutoAblation. We show that using Maggy for hyperparameter tuning results in roughly half the wall-clock time required to execute the same number of hyperparameter tuning trials using Spark while providing linear scalability as more workers are added. Furthermore, we demonstrate how AutoAblation facilitates the definition of ablation studies and enables the asynchronous parallel execution of ablation trials.},
  keywords = {ablation studies,artificial intelligence,big data,Copernicus,deep learning,Earth Observation,ExtremeEarth,Hopsworks,machine learning,Maggy,model serving},
  file = {C:\Users\Sebas\Zotero\storage\QNRABXL5\Hagos et al. - 2022 - Scalable artificial intelligence for earth observa.pdf}
}

@mastersthesis{Hasan1044829,
  title = {Quota Based Access-Control for {{Hops}} : {{Improving}} Cluster Utilization with {{Hops-YARN}}},
  author = {Hasan, Muhammed Rizvi},
  year = {2016},
  series = {{{TRITA-ICT-EX}}},
  number = {2016:109},
  pages = {71},
  abstract = {YARN is the resource management framework for Hadoop, and is, in many senses, the modern operating system for the data center. YARN clusters are running at organizations such as Yahoo!, Spotify, and Twitter with clusters of up to 3500 nodes being reported in the literature. To harness the power of so many nodes and manage them efficiently YARN is required to fulfill the requirements like scalability, serviceability, multitenancy, reliability, high cluster utilization, secure and auditable operation. Currently, YARN supports three different schedulers for prioritizing the allocation of resources (CPU, memory) to applications. Existing schedulers have a broken incentive model for popular frameworks like Apache Spark and Apache Flink where applications have gang-scheduling semantics, that is, they need all nodes to be available before they can start work. Users are incentivized to launch and hog their resources, as there may be a substantial delay (in Spotify, up to 1 hour) in getting 100 or more nodes allocated to your application. Users are not penalized for hogging resources. Capacity scheduler is one of the schedulers that has been used as a default scheduler in YARN which is quite good in sharing resources among tenants with a degree of guaranteed resource availability. Still there is room for improvements. In this thesis, we propose the design and implementation of a new system called Quota-based access control system that will work as a layer over capacity scheduler for Hops-YARN, a project developed on Apache YARN. Quota-based access control system involves allocating a quota of resources to projects. A project consists of a number of users who manage a number of data sets and is taken from a new frontend for Hadoop called HopsWorks, (www.hops.io). Project members can spend part of their quota to launch and run applications. In contrast to existing schedulers, our control system will incentivize users for not launching unnecessary applications or hog resources. In this work we also have analyzed the operational model of the scheduler including Quota-based access control system with different application scheduling scenarios. We also have investigated the failure scenarios which includes network partition and failure of different components of YARN and analyzed the consequence of the failure on the scheduling operation. Finally, we have proposed some future improvements for this scheduling system.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT)},
  file = {C:\Users\Sebas\Zotero\storage\THDYD5DU\Hasan - 2016 - Quota based access-control for Hops  Improving cl.pdf}
}

@phdthesis{hellmanStudyComparisonData2023,
  title = {Study and {{Comparison}} of {{Data Lakehouse Systems}}},
  author = {Hellman, Fredrik},
  year = {2023},
  address = {{\AA}bo},
  urldate = {2024-02-16},
  abstract = {This thesis presents a comprehensive study and comparative analysis of three distinct data lakehouse systems: Delta Lake, Apache Iceberg, and Apache Hudi. Data lakehouse systems are an emergent concept that combines the capabilities of data warehouses and data lakes to provide a unified platform for large-scale data management and analysis. Three experimental scenarios were conducted focusing on data ingestion, query performance, and scaling, each assessing a different aspect of the system's capabilities. The results show that each data lakehouse system possesses unique strengths and weaknesses: Apache Iceberg demonstrated the best data ingestion speed, Delta Lake exhibited consistent performance across all testing scenarios, while Apache Hudi excelled with smaller datasets. Furthermore, the study also considered the ease of implementation and use for each system. Apache Iceberg emerged as the most user-friendly, with comprehensive documentation. Delta Lake provided a slightly steeper learning curve, while Apache Hudi was the most challenging to implement. This study underscores the promising potential of data lakehouses as alternatives to traditional database architectures. However, further research is necessary to solidify the positioning of data lakehouses as the new generation of database architectures.},
  langid = {english},
  school = {Vaasa {\AA}bo Akademi University},
  file = {C:\Users\Sebas\Zotero\storage\SKPMEQ98\hellman_fredrik.pdf}
}

@misc{HopsworksBatchRealtime2024,
  title = {Hopsworks - {{Batch}} and {{Real-time ML Platform}}},
  year = {2024},
  urldate = {2024-02-09},
  abstract = {Hopsworks is a flexible and modular feature store that provides seamless integration for existing pipelines, superior performance for any SLA, and increased productivity for data and AI teams.},
  howpublished = {https://www.hopsworks.ai/},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\F5VYNVQK\www.hopsworks.ai.html}
}

@misc{incStorageWarsDatabases2023,
  title = {Storage {{Wars}}: {{Databases Becoming Just Query Engines}} for {{Big Object Stores}}?},
  shorttitle = {Storage {{Wars}}},
  author = {Inc, Data Saint Consulting},
  year = {2023},
  month = may,
  journal = {Medium},
  urldate = {2024-02-21},
  abstract = {Object storage, also known as object-based storage, is a computer data storage architecture designed to handle large amounts of unstructured data. Unlike other architectures, it designates data as{\dots}},
  langid = {english},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\3ED27W23\\Inc - 2023 - Storage Wars Databases Becoming Just Query Engine.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\N3USAQXF\\storage-wars-databases-becoming-just-query-engines-for-big-object-stores-179c3655599c.html}
}

@inproceedings{Ismail1158011,
  title = {Hopsworks : {{Improving}} User Experience and Development on Hadoop with Scalable, Strongly Consistent Metadata},
  shorttitle = {Hopsworks},
  booktitle = {2017 {{IEEE}} 37th {{International Conference}} on {{Distributed Computing Systems}} ({{ICDCS}})},
  author = {Ismail, Mahmoud and Gebremeskel, Ermias and Kakantousis, Theofilos and Berthou, Gautier and Dowling, Jim},
  year = {2017},
  month = jun,
  series = {{{IEEE}} International Conference on Distributed Computing Systems},
  pages = {2525--2528},
  publisher = {{IEEE COMPUTER SOC / KTH, Software and Computer systems, SCS and RISE SICS, Sweden}},
  address = {Atlanta, GA, USA},
  doi = {10.1109/ICDCS.2017.41},
  abstract = {Hadoop is a popular system for storing, managing, and processing large volumes of data, but it has bare-bones internal support for metadata, as metadata is a bottleneck and less means more scalability. The result is a scalable platform with rudimentary access control that is neither user-nor developer friendly. Also, metadata services that are built on Hadoop, such as SQL-on-Hadoop, access control, data provenance, and data governance are necessarily implemented as eventually consistent services, resulting in increased development effort and more brittle software. In this paper, we present a new project-based multi-tenancy model for Hadoop, built on a new distribution of Hadoop that provides a distributed database backend for the Hadoop Distributed Filesystem's (HDFS) metadata layer. We extend Hadoop's metadata model to introduce projects, datasets, and project-users as new core concepts that enable a user-friendly, UI-driven Hadoop experience. As our metadata service is backed by a transactional database, developers can easily extend metadata by adding new tables and ensure the strong consistency of extended metadata using both transactions and foreign keys.},
  isbn = {978-1-5386-1791-5},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\QR9X7XH2\Ismail et al. - 2017 - Hopsworks Improving User Experience and Developme.pdf}
}

@phdthesis{Ismail1500242,
  title = {Distributed File System Metadata and Its Applications},
  author = {Ismail, Mahmoud},
  year = {2020},
  series = {{{TRITA-EECS-AVL}}},
  number = {2020:65},
  abstract = {Distributed hierarchical file systems typically decouple the storage and serving of the file metadata from the file contents (file system blocks) to enable the file system to scale to store more data and support higher throughput. We designed HopsFS to take the scalability of the file system one step further by also decoupling the storage and serving of the file system metadata. HopsFS is an open-source, next- generation distribution of the Apache Hadoop Distributed File System (HDFS) that replaces the main scalability bottleneck in HDFS, the single-node in-memory metadata service, with a distributed metadata service built on a NewSQL database (NDB). HopsFS stores the file system's metadata fully normalized in NDB, then it uses locking primitives and application-defined locks to ensure strongly consistent metadata.In this thesis, we leverage the consistent distributed hierarchical file system meta- data provided by HopsFS to efficiently build new classes of applications that are tightly coupled with the file system as well as to improve the internal file system operations. First, we introduce hbr, a new block reporting protocol for HopsFS that removes a scalability bottleneck that prevented HopsFS from scaling to tens of thousands of servers. Second, we introduce HopsFS-CL, a highly available cloud-native distribution of HopsFS that deploys the file system across Availability Zones in the cloud while maintaining the same file system semantics. Third, we introduce HopsFS-S3, a highly available cloud-native distribution of HopsFS that uses object stores as a backend for the block storage layer in the cloud while again maintaining the same file system semantics. Fourth, we introduce ePipe, a databus that both creates a consistent change stream for HopsFS and eventually delivers the correctly ordered stream with low latency to downstream clients. That is, ePipe extends HopsFS with a change-data-capture (CDC) API that provides not only efficient file system notifications but also enables polyglot storage for file system metadata. Polyglot storage enables us to offload metadata queries to a more appropriate engine - we use Elasticsearch to provide a free-text search of the file system namespace to demonstrate this capability. Finally, we introduce Hopsworks, a scalable, project-based multi-tenant big data platform that provides support for collaborative development and operations for teams through extended metadata.},
  isbn = {978-91-7873-702-4},
  school = {KTH, Software and Computer systems, SCS / KTH, Software and Computer systems, SCS},
  file = {C:\Users\Sebas\Zotero\storage\HG69FLEB\Ismail - 2020 - Distributed file system metadata and its applicati.pdf}
}

@inproceedings{ismailDistributedHierarchicalFile2020,
  title = {Distributed {{Hierarchical File Systems}} Strike Back in the {{Cloud}}},
  booktitle = {40th {{IEEE International Conference}} on {{Distributed Computing Systems}}, {{November}} 29 - {{December}} 1, 2020, {{Singapore}}},
  author = {Ismail, Mahmoud and Niazi, Salman and Sundell, Mauritz and Ronstr{\"o}m, Mikael and Haridi, Seif and Dowling, Jim},
  year = {2020},
  urldate = {2024-02-16},
  abstract = {Cloud service providers have aligned on availability zones as an important unit of failure and replication for storage systems. An availability zone (AZ) has independent power, networking, and cool ...},
  langid = {english},
  keywords = {Cloud computing,File systems,Metadata,Optimization,Protocols,Semantics,Throughput},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\72P5DQ9U\\Ismail et al. - 2020 - Distributed Hierarchical File Systems strike back .pdf;C\:\\Users\\Sebas\\Zotero\\storage\\C6VXY74X\\9355756.html}
}

@inproceedings{ismailHopsFSS3ExtendingObject2020,
  title = {{{HopsFS-S3}}: {{Extending Object Stores}} with {{POSIX-like Semantics}} and More (Industry Track)},
  shorttitle = {{{HopsFS-S3}}},
  booktitle = {Proceedings of the 1st {{International Middleware Conference Industrial Track}}},
  author = {Ismail, Mahmoud and Niazi, Salman and Berthou, Gautier and Ronstr{\"o}m, Mikael and Haridi, Seif and Dowling, Jim},
  year = {2020},
  month = dec,
  pages = {23--30},
  publisher = {ACM},
  address = {Delft Netherlands},
  doi = {10.1145/3429357.3430521},
  urldate = {2024-02-14},
  isbn = {978-1-4503-8201-4},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\H7UL6E8M\Ismail et al. - 2020 - HopsFS-S3 Extending Object Stores with POSIX-like.pdf}
}

@inproceedings{ismailScalingHDFSMore2017,
  title = {Scaling {{HDFS}} to {{More Than}} 1 {{Million Operations Per Second}} with {{HopsFS}}},
  booktitle = {2017 17th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}} ({{CCGRID}})},
  author = {Ismail, Mahmoud and Niazi, Salman and Ronstrom, Mikael and Haridi, Seif and Dowling, Jim},
  year = {2017},
  month = may,
  pages = {683--688},
  publisher = {IEEE},
  address = {Madrid, Spain},
  doi = {10.1109/CCGRID.2017.117},
  urldate = {2024-02-16},
  abstract = {HopsFS is an open-source, next generation distribution of the Apache Hadoop Distributed File System (HDFS) that replaces the main scalability bottleneck in HDFS, single node in-memory metadata service, with a no-shared state distributed system built on a NewSQL database. By removing the metadata bottleneck in Apache HDFS, HopsFS enables significantly larger cluster sizes, more than an order of magnitude higher throughput, and significantly lower client latencies for large clusters.},
  isbn = {978-1-5090-6611-7},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\4IWXCNGL\Ismail et al. - 2017 - Scaling HDFS to More Than 1 Million Operations Per.pdf}
}

@article{jainAnalyzingComparingLakehouse2023,
  title = {Analyzing and {{Comparing Lakehouse Storage Systems}}},
  author = {Jain, Paras and Kraft, Peter and Power, Conor and Das, Tathagata and Stoica, Ion and Zaharia, Matei},
  year = {2023},
  abstract = {Lakehouse storage systems that implement ACID transactions and other management features over data lake storage, such as Delta Lake, Apache Hudi and Apache Iceberg, have rapidly grown in popularity, replacing traditional data lakes at many organizations. These open storage systems with rich management features promise to simplify management of large datasets, accelerate SQL workloads, and offer fast, direct file access for other workloads, such as machine learning. However, the research community has not explored the tradeoffs in designing lakehouse systems in detail. In this paper, we analyze the designs of the three most popular lakehouse storage systems---Delta Lake, Hudi and Iceberg---and compare their performance and features among varying axes based on these designs. We also release a simple benchmark, LHBench, that researchers can use to compare other designs. LHBench is available at https://github.com/lhbench/lhbench.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\GKDLB4NW\Jain et al. - 2023 - Analyzing and Comparing Lakehouse Storage Systems.pdf}
}

@mastersthesis{Kashyap1484512,
  title = {Project-Based Multi-Tenant Container Registry for Hopsworks},
  author = {Kashyap, Pradyumna Krishna},
  year = {2020},
  series = {{{TRITA-EECS-EX}}},
  number = {2020:725},
  pages = {63},
  abstract = {There has been a substantial growth in the usage of data in the past decade, cloud technologies and big data platforms have gained popularity as they help in processing such data on a large scale. Hopsworks is such a managed plat- form for scale out data science. It is an open-source platform for the develop- ment and operation of Machine Learning models, available on-premise and as a managed platform in the cloud. As most of these platforms provide data sci- ence environments to collate the required libraries to work with, Hopsworks provides users with Anaconda environments.Hopsworks provides multi-tenancy, ensuring a secure model to manage sen- sitive data in the shared platform. Most of the Hopsworks features are built around projects, each project includes an Anaconda environment that provides users with a number of libraries capable of processing data. Each project cre- ation triggers a creation of a base Anaconda environment and each added li- brary updates this environment. For an on-premise application, as data science teams are diverse and work towards building repeatable and scalable models, it becomes increasingly important to manage these environments in a central location locally.The purpose of the thesis is to provide a secure storage for these Anaconda en- vironments. As Hopsworks uses a Kubernetes cluster to serve models, these environments can be containerized and stored on a secure container registry on the Kubernetes Cluster. The provided solution also aims to extend the multi- tenancy feature of Hopsworks onto the hosted local storage. The implemen- tation comprises of two parts; First one, is to host a compatible open source container registry to store the container images on a local Kubernetes cluster with fault tolerance and by avoiding a single point of failure. Second one, is to leverage the multi-tenancy feature in Hopsworks by storing the images on the self sufficient secure registry with project level isolation.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Big Data,Cloud,Container,Data Science,Hopsworks,Kubernetes.,Multitenancy,On-premise,Registry},
  file = {C:\Users\Sebas\Zotero\storage\MCR7MBRF\Kashyap - 2020 - Project-based multi-tenant container registry for .pdf}
}

@article{kerstenEverythingYouAlways2018,
  title = {Everything You Always Wanted to Know about Compiled and Vectorized Queries but Were Afraid to Ask},
  author = {Kersten, Timo and Leis, Viktor and Kemper, Alfons and Neumann, Thomas and Pavlo, Andrew and Boncz, Peter},
  year = {2018},
  month = sep,
  journal = {Proceedings of the VLDB Endowment},
  volume = {11},
  number = {13},
  pages = {2209--2222},
  issn = {21508097},
  doi = {10.14778/3275366.3275370},
  urldate = {2024-02-26},
  abstract = {The query engines of most modern database systems are either based on vectorization or data-centric code generation. These two state-of-the-art query processing paradigms are fundamentally different in terms of system structure and query execution code. Both paradigms were used to build fast systems. However, until today it is not clear which paradigm yields faster query execution, as many implementation-specific choices obstruct a direct comparison of architectures. In this paper, we experimentally compare the two models by implementing both within the same test system. This allows us to use for both models the same query processing algorithms, the same data structures, and the same parallelization framework to ultimately create an apples-to-apples comparison. We find that both are efficient, but have different strengths and weaknesses. Vectorization is better at hiding cache miss latency, whereas data-centric compilation requires fewer CPU instructions, which benefits cacheresident workloads. Besides raw, single-threaded performance, we also investigate SIMD as well as multi-core parallelization and different hardware architectures. Finally, we analyze qualitative differences as a guide for system architects.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\PWU2885E\Kersten et al. - 2018 - Everything you always wanted to know about compile.pdf}
}

@mastersthesis{Khazanchi1801362,
  title = {Faster Reading with {{DuckDB}} and Arrow Flight on Hopsworks : {{Benchmark}} and Performance Evaluation of Offline Feature Stores},
  author = {Khazanchi, Ayushman},
  year = {2023},
  series = {{{TRITA-EECS-EX}}},
  number = {2023:661},
  pages = {69},
  abstract = {Over the last few years, Machine Learning has become a huge field with ``Big Tech'' companies sharing their experiences building machine learning infrastructure. Feature Stores, used as centralized data repositories for machine learning features, are seen as a central component to operational and scalable machine learning. With the growth in machine learning, there is, naturally, a tremendous growth in data used for training. Most of this data tends to sit in Parquet files in cloud object stores or data lakes and is used either directly from files or in-memory where it is used in exploratory data analysis and small batches of training. A majority of the data science involved in machine learning is done in Python, but the infrastructure surrounding it is not always directly compatible with Python. Often, query processing engines and feature stores end up having their own Domain Specific Language or require data scientists to write SQL code, thus leading to some level of `transpilation' overhead across the system. This overhead can not only introduce errors but can also add up to significant time and productivity cost down the line. In this thesis, we conduct a systems research on the performance of offline feature stores and identify ways that allow us to pull out data from feature stores in a fast and efficient way. We conduct a model evaluation based on benchmark tests that address common exploratory data analysis and training use cases. We find that in the Hopsworks feature store, with the use of state-of-the-art, storage-optimized, format-aware, and vector execution-based query processing engine as well as using Arrow protocol from start to finish, we are able to see significant improvements in both creating batch training data (feature value reads) and creating Point-In-Time Correct training data. For batch training data created in-memory, Hopsworks shows an average speedup of 27x over Databricks (5M and 10M scale factors), 18x over Vertex, and 8x over Sagemaker across all scale factors. For batch training data as parquet files, Hopsworks shows a speedup of 5x over Databricks (5M, 10M, and 20M scale factors), 13x over Vertex, and 6x over Sagemaker across all scale factors. For creating in-memory Point-In-Time Correct training data, Hopsworks shows an average speedup of 8x over Databricks, 6x over Vertex, and 3x over Sagemaker across all scale factors. Similary for PIT-Correct training data created as file, Hopsworks shows an average speedup of 9x over Databricks, 8x over Vertex, and 6x over Sagemaker across all scale factors. Through the analysis of these experimental results and the underlying infrastructure, we identify the reasons for this performance gap and examine the strengths and limitations of the design.},
  school = {KTH Royal Institute of Technology / KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Distributed Systems,Feature Store,Machine Learning,MLOps},
  file = {C:\Users\Sebas\Zotero\storage\ZMXCBWPZ\Khazanchi - 2023 - Faster reading with DuckDB and arrow flight on hop.pdf}
}

@inproceedings{kuiperTheseRowsAre2023,
  title = {These {{Rows Are Made}} for {{Sorting}} and {{That}}'s {{Just What We}}'ll {{Do}}},
  booktitle = {2023 {{IEEE}} 39th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Kuiper, Laurens and M{\"u}hleisen, Hannes},
  year = {2023},
  month = apr,
  pages = {2050--2062},
  publisher = {IEEE},
  address = {Anaheim, CA, USA},
  doi = {10.1109/ICDE55515.2023.00159},
  urldate = {2024-02-26},
  abstract = {Sorting is one of the most well-studied problems in computer science and a vital operation for relational database systems. Despite this, little research has been published on implementing an efficient relational sorting operator. In this work, we aim to fill this gap. We use micro-benchmarks to explore how to sort relational data efficiently for analytical database systems, taking into account different query execution engines as well as row and columnar data formats. We show that, regardless of architectural differences between query engines, sorting rows is almost always more efficient than sorting columnar data, even if this requires converting the data from columns to rows and back. Sorting rows efficiently is challenging for systems with an interpreted execution engine, as interpreting rows at runtime causes overhead. We show that this overhead can be overcome with several existing techniques. Based on our findings, we implement a highly optimized row-based sorting approach in the DuckDB open-source in-process analytical database management system, which has a vectorized interpreted query engine. We compare DuckDB with four analytical database systems and find that DuckDB's sort implementation outperforms query engines that sort using a columnar data format, and matches or outperforms compiled query engines that sort using a row data format.},
  isbn = {9798350322279},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\QTFF5JY4\Kuiper and Mühleisen - 2023 - These Rows Are Made for Sorting and That’s Just Wh.pdf}
}

@misc{mazumdarDataLakehouseData2023,
  title = {The {{Data Lakehouse}}: {{Data Warehousing}} and {{More}}},
  shorttitle = {The {{Data Lakehouse}}},
  author = {Mazumdar, Dipankar and Hughes, Jason and Onofre, J. B.},
  year = {2023},
  month = oct,
  number = {arXiv:2310.08697},
  eprint = {2310.08697},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-16},
  abstract = {Relational Database Management Systems designed for Online Analytical Processing (RDBMS-OLAP) have been foundational to democratizing data and enabling analytical use cases such as business intelligence and reporting for many years. However, RDBMS-OLAP systems present some well-known challenges. They are primarily optimized only for relational workloads, lead to proliferation of data copies which can become unmanageable, and since the data is stored in proprietary formats, it can lead to vendor lock-in, restricting access to engines, tools, and capabilities beyond what the vendor offers. As the demand for data-driven decision making surges, the need for a more robust data architecture to address these challenges becomes ever more critical. Cloud data lakes have addressed some of the shortcomings of RDBMS-OLAP systems, but they present their own set of challenges. More recently, organizations have often followed a two-tier architectural approach to take advantage of both these platforms, leveraging both cloud data lakes and RDBMS-OLAP systems. However, this approach brings additional challenges, complexities, and overhead. This paper discusses how a data lakehouse, a new architectural approach, achieves the same benefits of an RDBMS-OLAP and cloud data lake combined, while also providing additional advantages. We take today's data warehousing and break it down into implementation independent components, capabilities, and practices. We then take these aspects and show how a lakehouse architecture satisfies them. Then, we go a step further and discuss what additional capabilities and benefits a lakehouse architecture provides over an RDBMS-OLAP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\45VLCJGC\\Mazumdar et al. - 2023 - The Data Lakehouse Data Warehousing and More.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\QWFTHUZG\\2310.html}
}

@misc{MediumParserStorage,
  title = {Medium Parser - {{Storage Wars}}: {{Databases Becoming Just Query Engines}} for {{Big Object Stores}}? {\textbar} by {{Data Saint Consulting Inc}} {\textbar} {{Medium}}},
  urldate = {2024-02-21},
  howpublished = {https://webcache.googleusercontent.com/search?q=cache:https://fahadthedatascientist.medium.com/storage-wars-databases-becoming-just-query-engines-for-big-object-stores-179c3655599c\&strip=0\&vwsrc=1\&referer=medium-parser}
}

@phdthesis{More862135,
  title = {{{HopsWorks}} : {{A}} Project-Based Access Control Model for {{Hadoop}}},
  author = {Mor{\'e}, Andr{\'e} and Gebremeskel, Ermias},
  year = {2015},
  series = {{{TRITA-ICT-EX}}},
  number = {2015:70},
  abstract = {The growth in the global data gathering capacity is producing a vast amount of data which is getting vaster at an increasingly faster rate. This data properly analyzed can represent great opportunity for businesses, but processing it is a resource-intensive task. Sharing can increase efficiency due to reusability but there are legal and ethical questions that arise when data is shared. The purpose of this thesis is to gain an in depth understanding of the different access control methods that can be used to facilitate sharing, and choose one to implement on a platform that lets user analyze, share, and collaborate on, datasets. The resulting platform uses a project based access control on the API level and a fine-grained role based access control on the file system to give full control over the shared data to the data owner.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT) and KTH, School of Information and Communication Technology (ICT)},
  keywords = {Big Data,DataSets,Distributed Computing,Hadoop,Hops,HopsWorks},
  file = {C:\Users\Sebas\Zotero\storage\97KBTXXE\Moré and Gebremeskel - 2015 - HopsWorks  A project-based access control model f.pdf}
}

@article{niaziHopsFSScalingHierarchical,
  title = {{{HopsFS}}: {{Scaling Hierarchical File System Metadata Using NewSQL Databases}}},
  author = {Niazi, Salman and Ismail, Mahmoud and Haridi, Seif and Dowling, Jim and Grohsschmiedt, Steffen and Ronstr{\"o}m, Mikael},
  abstract = {Recent improvements in both the performance and scalability of shared-nothing, transactional, in-memory NewSQL databases have reopened the research question of whether distributed metadata for hierarchical file systems can be managed using commodity databases. In this paper, we introduce HopsFS, a next generation distribution of the Hadoop Distributed File System (HDFS) that replaces HDFS' single node in-memory metadata service, with a distributed metadata service built on a NewSQL database. By removing the metadata bottleneck, HopsFS enables an order of magnitude larger and higher throughput clusters compared to HDFS. Metadata capacity has been increased to at least 37 times HDFS' capacity, and in experiments based on a workload trace from Spotify, we show that HopsFS supports 16 to 37 times the throughput of Apache HDFS. HopsFS also has lower latency for many concurrent clients, and no downtime during failover. Finally, as metadata is now stored in a commodity database, it can be safely extended and easily exported to external systems for online analysis and free-text search.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\IGGVPFPW\Niazi et al. - HopsFS Scaling Hierarchical File System Metadata .pdf}
}

@inproceedings{niaziSizeMattersImproving2018,
  title = {Size {{Matters}}: {{Improving}} the {{Performance}} of {{Small Files}} in {{Hadoop}}},
  shorttitle = {Size {{Matters}}},
  booktitle = {Proceedings of the 19th {{International Middleware Conference}}},
  author = {Niazi, Salman and Ronstr{\"o}m, Mikael and Haridi, Seif and Dowling, Jim},
  year = {2018},
  month = nov,
  pages = {26--39},
  publisher = {ACM},
  address = {Rennes France},
  doi = {10.1145/3274808.3274811},
  urldate = {2024-02-14},
  isbn = {978-1-4503-5702-9},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\CXG9APSL\Niazi et al. - 2018 - Size Matters Improving the Performance of Small F.pdf}
}

@inproceedings{ormenisanTimeTravelProvenance2020,
  title = {Time {{Travel}} and {{Provenance}} for {{Machine Learning Pipelines}}},
  author = {Ormenisan, Alexandru A. and Buso, Fabio and Andersson, Robin and Haridi, Seif and Dowling, Jim and Meister, Moritz},
  year = {2020},
  publisher = {2020 Unesix Conference on Operational Machine Learning},
  abstract = {Machine learning pipelines have become the defacto paradigm for productionizing machine learning applications as they clearly abstract the processing steps involved in transforming raw data into engineered features that are then used to train models. In this paper, we use a bottom-up method for capturing provenance information regarding the processing steps and artifacts produced in ML pipelines. Our approach is based on replacing traditional intrusive hooks in application code (to capture ML pipeline events) with standardized change-data-capture support in the systems involved in ML pipelines: the distributed file system, feature store, resource manager, and applications themselves. In particular, we leverage data versioning and time-travel capabilities in our feature store to show how provenance can enable model reproducibility and debugging.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\BCIZ32ND\Ormenisan et al. - 2020 - Time Travel and Provenance for Machine Learning Pi.pdf}
}

@mastersthesis{Pettersson1695672,
  title = {Resource-Efficient and Fast {{Point-in-Time}} Joins for {{Apache Spark}} : {{Optimization}} of Time Travel Operations for the Creation of Machine Learning Training Datasets},
  author = {Pettersson, Axel},
  year = {2022},
  series = {{{TRITA-EECS-EX}}},
  number = {2022:193},
  pages = {63},
  abstract = {A scenario in which modern machine learning models are trained is to make use of past data to be able to make predictions about the future. When working with multiple structured and time-labeled datasets, it has become a more common practice to make use of a join operator called the Point-in-Time join, or PIT join, to construct these datasets. The PIT join matches entries from the left dataset with entries of the right dataset where the matched entry is the row whose recorded event time is the closest to the left row's timestamp, out of all the right entries whose event time occurred before or at the same time of the left event time. This feature has long only been a part of time series data processing tools but has recently received a new wave of attention due to the rise of the popularity of feature stores. To be able to perform such an operation when dealing with a large amount of data, data engineers commonly turn to large-scale data processing tools, such as Apache Spark. However, Spark does not have a native implementation when performing these joins and there has not been a clear consensus by the community on how this should be achieved. This, along with previous implementations of the PIT join, raises the question: ''How to perform fast and resource efficient Pointin- Time joins in Apache Spark?''. To answer this question, three different algorithms have been developed and compared for performing a PIT join in Spark in terms of resource consumption and execution time. These algorithms were benchmarked using generated datasets using varying physical partitions and sorting structures. Furthermore, the scalability of the algorithms was tested by running the algorithms on Apache Spark clusters of varying sizes. The results received from the benchmarks showed that the best measurements were achieved by performing the join using Early Stop Sort-Merge Join, a modified version of the regular Sort-Merge Join native to Spark. The best performing datasets were the datasets that were sorted by timestamp and primary key, ascending or descending, using a suitable number of physical partitions. Using this new information gathered by this project, data engineers have been provided with general guidelines to optimize their data processing pipelines to be able to perform more resource-efficient and faster PIT joins.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Apache Spark,ASOF,Join,Optimeringar,Optimizations,Point-in-Time,Tidsresning,Time travel},
  file = {C:\Users\Sebas\Zotero\storage\JE2P3YPT\Pettersson - 2022 - Resource-efficient and fast Point-in-Time joins fo.pdf}
}

@inproceedings{raasveldtFairBenchmarkingConsidered2018,
  title = {Fair {{Benchmarking Considered Difficult}}: {{Common Pitfalls In Database Performance Testing}}},
  shorttitle = {Fair {{Benchmarking Considered Difficult}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Testing Database Systems}}},
  author = {Raasveldt, Mark and Holanda, Pedro and Gubner, Tim and M{\"u}hleisen, Hannes},
  year = {2018},
  month = jun,
  pages = {1--6},
  publisher = {ACM},
  address = {Houston TX USA},
  doi = {10.1145/3209950.3209955},
  urldate = {2024-04-18},
  abstract = {Performance benchmarking is one of the most commonly used methods for comparing different systems or algorithms, both in scientific literature and in industrial publications. While performance measurements might seem objective on the surface, there are many different ways to influence benchmark results to favor one system over the other, either by accident or on purpose. In this paper, we perform a study of the common pitfalls in DBMS performance comparisons, and give advice on how they can be spotted and avoided so a fair performance comparison between systems can be made. We illustrate the common pitfalls with a series of mock benchmarks, which show large differences in performance where none should be present.},
  isbn = {978-1-4503-5826-2},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\Q52SD8NU\Raasveldt et al. - 2018 - Fair Benchmarking Considered Difficult Common Pit.pdf}
}

@inproceedings{samundiswaryObjectStorageArchitecture2017,
  title = {Object Storage Architecture in Cloud for Unstructured Data},
  booktitle = {2017 {{International Conference}} on {{Inventive Systems}} and {{Control}} ({{ICISC}})},
  author = {Samundiswary, S. and Dongre, Nilma M},
  year = {2017},
  month = jan,
  pages = {1--6},
  publisher = {IEEE},
  address = {Coimbatore, India},
  doi = {10.1109/ICISC.2017.8068716},
  urldate = {2024-02-20},
  isbn = {978-1-5090-4715-4},
  file = {C:\Users\Sebas\Zotero\storage\VAEY2RCA\Samundiswary and Dongre - 2017 - Object storage architecture in cloud for unstructu.pdf}
}

@inproceedings{schneiderAssessingLakehouseAnalysis2023,
  title = {Assessing the {{Lakehouse}}: {{Analysis}}, {{Requirements}} and {{Definition}}:},
  shorttitle = {Assessing the {{Lakehouse}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Enterprise Information Systems}}},
  author = {Schneider, Jan and Gr{\"o}ger, Christoph and Lutsch, Arnold and Schwarz, Holger and Mitschang, Bernhard},
  year = {2023},
  pages = {44--56},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  address = {Prague, Czech Republic},
  doi = {10.5220/0011840500003467},
  urldate = {2024-03-05},
  isbn = {978-989-758-648-4},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\MQEKQQZF\Schneider et al. - 2023 - Assessing the Lakehouse Analysis, Requirements an.pdf}
}

@mastersthesis{Sheikholeslami1349978,
  title = {Ablation Programming for Machine Learning},
  author = {Sheikholeslami, Sina},
  year = {2019},
  series = {{{TRITA-EECS-EX}}},
  number = {2019:557},
  pages = {52},
  abstract = {As machine learning systems are being used in an increasing number of applications from analysis of satellite sensory data and health-care analytics to smart virtual assistants and self-driving cars they are also becoming more and more complex. This means that more time and computing resources are needed in order to train the models and the number of design choices and hyperparameters will increase as well. Due to this complexity, it is usually hard to explain the effect of each design choice or component of the machine learning system on its performance.A simple approach for addressing this problem is to perform an ablation study, a scientific examination of a machine learning system in order to gain insight on the effects of its building blocks on its overall performance. However, ablation studies are currently not part of the standard machine learning practice. One of the key reasons for this is the fact that currently, performing an ablation study requires major modifications in the code as well as extra compute and time resources.On the other hand, experimentation with a machine learning system is an iterative process that consists of several trials. A popular approach for execution is to run these trials in parallel, on an Apache Spark cluster. Since Apache Spark follows the Bulk Synchronous Parallel model, parallel execution of trials includes several stages, between which there will be barriers. This means that in order to execute a new set of trials, all trials from the previous stage must be finished. As a result, we usually end up wasting a lot of time and computing resources on unpromising trials that could have been stopped soon after their start.We have attempted to address these challenges by introducing MAGGY, an open-source framework for asynchronous and parallel hyperparameter optimization and ablation studies with Apache Spark and TensorFlow. This framework allows for better resource utilization as well as ablation studies and hyperparameter optimization in a unified and extendable API.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Ablation Studies,Apache Spark,Distributed Machine Learning,Distributed Systems,Hopsworks,Keras},
  file = {C:\Users\Sebas\Zotero\storage\RJ695NYZ\Sheikholeslami - 2019 - Ablation programming for machine learning.pdf}
}

@book{shiranApacheIcebergDefinitive2024,
  title = {Apache {{Iceberg}}: {{The Definitive Guide}}},
  shorttitle = {Apache {{Iceberg}}},
  author = {Shiran, Tomer and Hughes, Jason and Merced, Alex},
  year = {2024},
  month = mar,
  publisher = {O'Reilly},
  urldate = {2024-02-19},
  abstract = {Traditional data architecture patterns are severely limited. To use these patterns, you have to ETL data into each tool---a cost-prohibitive process for making warehouse features available to all of...},
  isbn = {978-1-09-814861-4},
  langid = {english},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\RLMCPD3E\\Apache Iceberg, The Definitive Guide.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\DPS442GX\\9781098148614.html}
}

@phdthesis{SvedlundNordstrom1088359,
  title = {A Global Ecosystem for Datasets on Hadoop},
  author = {Svedlund Nordstr{\"o}m, Johan},
  year = {2016},
  series = {{{TRITA-ICT-EX}}},
  number = {2016:131},
  pages = {47},
  abstract = {The immense growth of the web has led to the age of Big Data. Companies like Google, Yahoo and Facebook generates massive amounts of data everyday. In order to gain value from this data, it needs to be effectively stored and processed. Hadoop, a Big Data framework, can store and process Big Data in a scalable and performant fashion. Both Yahoo and Facebook, two major IT companies, deploy Hadoop as their solution to the Big Data problem. Many application areas for Big Data would benefit from the ability to share datasets across cluster boundaries. However, Hadoop does not support searching for datasets either local to a single Hadoop cluster or across many Hadoop clusters. Similarly, there is only limited support for copying datasets between Hadoop clusters (using Distcp). This project presents a solution to this weakness using the Hadoop distribution, Hops, and its frontend Hopsworks. Clusters advertise their peer-to-peer and search endpoints to a central server called Hops-Site. The advertised endpoints builds a global hadoop ecosystem and gives clusters the ability to participate in publicsearch or peer-to-peer sharing of datasets. HopsWorks users are given a choice to write data into Kafka as it's being downloaded. This opens up new possibilities for data scientists who can interactively analyse remote datasets without having to download everything in advance. By writing data into Kafka as its being downloaded, it can be consumed by entities like Spark-streaming or Flink.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT)},
  file = {C:\Users\Sebas\Zotero\storage\4SNLLXST\Svedlund Nordström - 2016 - A global ecosystem for datasets on hadoop.pdf}
}

@phdthesis{vilenMountingExternalStorage2018,
  title = {Mounting {{External Storage}} in {{HopsFS}}},
  author = {Vilen, Gabriel},
  year = {2018},
  address = {Delft},
  abstract = {A desirable feature of distributed file systems is to enable clients to mount external namespaces, allowing them to access physically remote data as if it was transparently stored in the local cluster. This feature is desirable in a variety of scenarios, such as in enterprise hybrid-cloud deployments, where some data may be stored remote on the cloud while other in the local file system. This feature introduces questions as how to design, implement, and handle various file system semantics, such as different consistency levels and operational performance between the remote and local storage. For instance, a POXIS-complient block based file system is strongly consistent and does not pose the same semantics as a more relaxed eventual consistent object-based cloud storage. Further, it is interesting to benchmark how such an integrated remote storage solution performs and what potential speedups that might be possible. To examine such questions and provide the remote storage feature in HopsFS, a scale-out metadata HDFS distribution, this thesis presents an Proof-of-Concept (PoC) implementation that enables the local file system to be transparently integrated with an external remote storage. The PoC is storage provider agnostic and enables a HopsFS client to on-demand pull in and access remote data from an external storage into the local file system. Currently, the solution works by mounting a snapshot of a remote storage into HopsFS, a read-only approach. For write and synchronization work, frameworks that layers stronger consistency models on top of S3 may be used, such as s3mper. Performance of the implemented feature has been benchmarked against remote Amazon S3 buckets, showing results of fast throughput for scanning and persisting metadata, but slower (4 Mb/s) for pulling data into the cluster through the Amazon cloud connector S3A. To improve performance, caching by replication of the remote files onto local disk storage has shown to drastically increase the read speed (up to 100x). The work done in this thesis shows a promising PoC that successfully serves remote blocks into a local file system, with future work including supporting writes, improved caching mechanisms, and reflecting off-band changes between the remote and local storage.},
  langid = {english},
  school = {Delft University of Technology},
  file = {C:\Users\Sebas\Zotero\storage\23SAU4KL\Vilen - Mounting External Storage in HopsFS.pdf}
}

@mastersthesis{Yedurupak1181024,
  title = {Multitenant {{PrestoDB}} as a Service},
  author = {Yedurupak, Aruna Kumari},
  year = {2017},
  series = {{{TRITA-ICT-EX}}},
  number = {2017:181},
  pages = {55},
  abstract = {In recent years, there has been tremendous growth in both the volumes of data that is produced, stored, and queried by organizations. Organizations spend more money to investigate and obtain useful information or knowledge against terabytes and even petabytes of data. Large-scale data analysis is the key functionality provided by Big Data platforms. Previously, data platforms would get the information from unstructured data in the form of files, text, and videos. In recent times, the Hadoop stack has played a vital role in Big Data, becoming the defector open source software used to process and analyze Big Data. Hops is a Hadoop distribution developed by KTH and RISE SICS. Hops modifies the Hadoop stack by moving the meta-data for YARN and HDFS to NDB, an open-source in-memory distributed database. HopsWorks is the User Interface for Hops and provides support for multi-tenant users, as well as self-service, graphical access to frameworks such as Hadoop, Flink, Spark, Kafka, and Kibana. HopsWorks currently does not provide a SQL-on-Hadoop service, although work is ongoing for supporting Hive. Presto is one of the main SQL-on-Hadoop platform, but, currently, Presto does not provide multi-tenancy support for users. This thesis investigates providing multitenancy support to Presto with the help of HopsWorks, including both the security problem and the self-service UI requirements of HopsWorks. Presto is a distributed SQL query Engine which can run SQL queries against up to petabytes of data. As HopsWorks provides UI access to services, we decided to build our UI for Presto on an existing open-source UI for Presto, called Airpal, developed by Airbnb. This provided solution of the thesis divided into two functionalities. First one, maintain two separate Applications (HopsWorks and Airpal Applications) run by the help of two JVMs and maintain ProxyServlet to control traffic between them. Second one HopsWorks-Presto-service leverages HopsWorks access-control (Data owner and Data-scientist) and self-service security model. The evaluation of the thesis used qualitative approach by comparing HopsWorks-PrestoService with standalone PrestoDB and comparing HopsWorks-PrestoService with HopsWorks without Presto-Service.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT)},
  keywords = {Airpal,Hadoop,Hops,HopsWorks,multi-hyresratt,Multi-tenancy,Presto,Proxy servlet,SQL},
  file = {C:\Users\Sebas\Zotero\storage\I5SM648U\Yedurupak - 2017 - Multitenant PrestoDB as a service.pdf}
}

@mastersthesis{Zangis1696783,
  title = {Scaling Apache Hudi by Boosting Query Performance with {{RonDB}} as a Global Index : {{Adopting}} a {{LATS}} Data Store for Indexing},
  author = {Zangis, Ralfs},
  year = {2022},
  series = {{{TRITA-EECS-EX}}},
  number = {2022:194},
  pages = {70},
  abstract = {The storage and use of voluminous data are perplexing issues, the resolution of which has become more pressing with the exponential growth of information. Lakehouses are relatively new approaches that try to accomplish this while hiding the complexity from the user. They provide similar capabilities to a standard database while operating on top of low-cost storage and open file formats. An example of such a system is Hudi, which internally uses indexing to improve the performance of data management in tabular format. This study investigates if the execution times could be decreased by introducing a new engine option for indexing in Hudi. Therefore, the thesis proposes the usage of RonDB as a global index, which is expanded upon by further investigating the viability of different connectors that are available for communication. The research was conducted using both practical experiments and the study of relevant literature. The analysis involved observations made over multiple workloads to document how adequately the solutions can adapt to changes in requirements and types of actions. This thesis recorded the results and visualized them for the convenience of the reader, as well as made them available in a public repository. The conclusions did not coincide with the author's hypothesis that RonDB would provide the fastest indexing solution for all scenarios. Nonetheless, it was observed to be the most consistent approach, potentially making it the best general-purpose solution. As an example, it was noted, that RonDB is capable of dealing with read and write heavy workloads, whilst consistently providing low query latency independent from the file count.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Apache Hudi,Index,Key-value store,Lakehouse,Nyckel-varde butik,Performance,Prestanda,RonDB},
  file = {C:\Users\Sebas\Zotero\storage\53J7EPG8\Zangis - 2022 - Scaling apache hudi by boosting query performance .pdf}
}

	year = {2016},
	pages = {1--7}
}

@article{maguire_jr_new_2014,
	title = {A {New} {Automated} {Way} to {Measure} {Polyethylene} {Wear} in {THA} {Using} a {High} {Resolution} {CT} {Scanner}: {Method} and {Analysis}},
	volume = {2014},
	issn = {2356-6140, 1537-744X},
	shorttitle = {A {New} {Automated} {Way} to {Measure} {Polyethylene} {Wear} in {THA} {Using} a {High} {Resolution} {CT} {Scanner}},
	url = {http://www.hindawi.com/journals/tswj/2014/528407/},
	doi = {10.1155/2014/528407},
	abstract = {As the most advantageous total hip arthroplasty (THA) operation is the first, timely replacement of only the liner is socially and economically important because the utilization of THA is increasing as younger and more active patients are receiving implants and they are living longer. Automatic algorithms were developed to infer liner wear by estimating the separation between the acetabular cup and femoral component head given a computed tomography (CT) volume. Two series of CT volumes of a hip phantom were acquired with the femoral component head placed at 14 different positions relative to the acetabular cup. The mean and standard deviation (SD) of the diameter of the acetabular cup and femoral component head, in addition to the range of error in the expected wear values and the repeatability of all the measurements, were calculated. The algorithms resulted in a mean (±SD) for the diameter of the acetabular cup of 54.21 (±0.011) mm and for the femoral component head of 22.09 (±0.02) mm. The wear error was ±0.1 mm and the repeatability was 0.077 mm. This approach is applicable clinically as it utilizes readily available computed tomography imaging systems and requires only five minutes of human interaction.},
	language = {english},
	urldate = {2020-02-14},
	journal = {The Scientific World Journal},
	author = {Maguire Jr., Gerald Q. and Noz, Marilyn E. and Olivecrona, Henrik and Zeleznik, Michael P. and Weidenhielm, Lars},
	year = {2014},
	pages = {1--9}
}


@inproceedings{bogdanov_nearest_2015,
	address = {Kohala Coast, Hawaii},
	title = {The nearest replica can be farther than you think},
	isbn = {978-1-4503-3651-2},
	url = {http://dl.acm.org/citation.cfm?doid=2806777.2806939},
	doi = {10.1145/2806777.2806939},
	language = {english},
	urldate = {2020-02-14},
	booktitle = {Proceedings of the {Sixth} {ACM} {Symposium} on {Cloud} {Computing} - {SoCC} '15},
	publisher = {ACM Press},
	author = {Bogdanov, Kirill and Peón-Quirós, Miguel and Maguire, Gerald Q. and Kostć, Dejan},
	year = {2015},
	pages = {16--29}
}

@inproceedings{roozbeh_resource_2013,
	address = {Dresden, Germany},
	title = {Resource {Monitoring} in a {Network} {Embedded} {Cloud}: {An} {Extension} to {OSPF}-{TE}},
	isbn = {978-0-7695-5152-4},
	shorttitle = {Resource {Monitoring} in a {Network} {Embedded} {Cloud}},
	url = {http://ieeexplore.ieee.org/document/6809350/},
	doi = {10.1109/UCC.2013.36},
	urldate = {2020-02-14},
	booktitle = {2013 {IEEE}/{ACM} 6th {International} {Conference} on {Utility} and {Cloud} {Computing}},
	publisher = {IEEE},
	author = {Roozbeh, Amir and Sefidcon, Azimeh and Maguire, Gerald Q.},
	month = dec,
	year = {2013},
	pages = {139--146}
}


@Article{einstein2016,
  author = 	 {Einstein, Albert},
  title = 	 {On the Relative Value of Relatives},
  journal = 	 {J. Irreproducible Results},
  year = 	 {2016},
  volume = 	 {17},
  pages = 	 {4711--4721}
}

@InProceedings{heisenberg2015,
  author = 	 {Heisenberg, Werner and Dirac, Paul},
  title = 	 {To be or not to be},
  booktitle =    {Proceedings of the Uncertain Society Annual Meeting},
  year = 	 {2015},
  editor = 	 {Schrödinger, Erwin},
  pages =	 {21--22}
}

 
@mastersthesis{Mao_Ni_2004,
        author = {Mao Ni},
        title = {Automatic extraction of author self contributed metadata for Electronic Theses
and Dissertations},
        school = {University of North Carolina at Chapel Hill, School of Library and Information Science, Chapel Hill, North Carolina, USA},
        url = {https://ils.unc.edu/MSpapers/2997.pdf},
        note = {{A Master’s Paper for the M.S. in I.S. degree, Advisor: Bradley M. Hemminger}},
        year = {20004},
        month = {May},
        pages = 38
}

@misc{hickey_pavani_suleman_2010,
    author={Thom Hickey and Ana Pavani and Hussein Suleman},
    title={ETD-MS v1.1: An interoperability metadata standard for electronic theses and dissertations},
    url={https://ndltd.org/wp-content/uploads/2021/04/etd-ms-v1.1.html}, journal={NDLTD},
    publisher={Networked Digital Library of Theses and Dissertations},
    year={2010},
    month=aug,
    note={version 1.1 - the author list is the editors}
    }
@misc{digital_commons_metadata_2016,
    title={Dublin Core Elements in Digital Commons},
    url={https://bepress.com/wp-content/uploads/2016/12/Dublin-Core-Elements-in-Digital-Commons.pdf},
    year=2016,
    month=dec
}

@misc{bepressOAIPMH,
    author={bepress},
    title={Digital Commons and OAI-PMH: Outbound Harvesting of Repository Records},
    url={https://bepress.com/reference_guide_dc/digital-commons-oai-harvesting/},
    
}

@misc{Ann_Apps_2005,
    author={Ann Apps},
    title={Guidelines for Encoding Bibliographic Citation Information in Dublin Core™ Metadata},
    year=2005,
    month=jun,
    note={Date Issued:	2005-06-13},
    url={https://www.dublincore.org/specifications/dublin-core/dc-citation-guidelines/}
}

@article{ivanovic_data_2012,
	title = {A data model of theses and dissertations compatible with {CERIF}, {Dublin} {Core} and {EDT}‐{MS}},
	volume = {36},
	issn = {1468-4527},
	url = {https://www.emerald.com/insight/content/doi/10.1108/14684521211254068/full/html},
	doi = {10.1108/14684521211254068},
	abstract = {Purpose
              The aim of this research is to define a data model of theses and dissertations that enables data exchange with CERIF‐compatible CRIS systems and data exchange according to OAI‐PMH protocol in different metadata formats (Dublin Core, EDT‐MS, etc.).
            
            
              Design/methodology/approach
              Various systems that contain metadata about theses and dissertations are analyzed. There are different standards and protocols that enable the interoperability of those systems: CERIF standard, AOI‐PMH protocol, etc. A physical data model that enables interoperability with almost all of those systems is created using the PowerDesigner CASE tool.
            
            
              Findings
              A set of metadata about theses and dissertations that contain all the metadata required by CERIF data model, Dublin Core format, EDT‐MS format and all the metadata prescribed by the University of Novi Sad is defined. Defined metadata can be stored in the CERIF‐compatible data model based on the MARC21 format.
            
            
              Practical implications
              CRIS‐UNS is a CRIS which has been developed at the University of Novi Sad since 2008. The system is based on the proposed data model, which enables the system's interoperability with other CERIF‐compatible CRIS systems. Also, the system based on the proposed model can become a member of NDLTD.
            
            
              Social implications
              A system based on the proposed model increases the availability of theses and dissertations, and thus encourages the development of the knowledge‐based society.
            
            
              Originality/value
              A data model of theses and dissertations that enables interoperability with CERIF‐compatible CRIS systems is proposed. A software system based on the proposed model could become a member of NDLTD and exchange metadata with institutional repositories. The proposed model increases the availability of theses and dissertations.},
	language = {en},
	number = {4},
	urldate = {2022-12-30},
	journal = {Online Information Review},
	author = {Ivanović, Lidija and Ivanović, Dragan and Surla, Dušan},
	month = aug,
	year = {2012},
	pages = {548--567},
}
