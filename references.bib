@inproceedings{10.1007/978-3-031-35260-7_5,
  title = {The Impact of Importance-Aware Dataset Partitioning on Data-Parallel Training of Deep Neural Networks},
  booktitle = {Distributed Applications and Interoperable Systems},
  author = {Sheikholeslami, Sina and Payberah, Amir H. and Wang, Tianze and Dowling, Jim and Vlassov, Vladimir},
  editor = {{Pati{\~n}o-Mart{\'i}nez}, Marta and Paulo, Jo{\~a}o},
  year = {2023},
  pages = {74--89},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-35260-7_5},
  abstract = {Deep neural networks used for computer vision tasks are typically trained on datasets consisting of thousands of images, called examples. Recent studies have shown that examples in a dataset are not of equal importance for model training and can be categorized based on quantifiable measures reflecting a notion of ``hardness'' or ``importance''. In this work, we conduct an empirical study of the impact of importance-aware partitioning of the dataset examples across workers on the performance of data-parallel training of deep neural networks. Our experiments with CIFAR-10 and CIFAR-100 image datasets show that data-parallel training with importance-aware partitioning can perform better than vanilla data-parallel training, which is oblivious to the importance of examples. More specifically, the proper choice of the importance measure, partitioning heuristic, and the number of intervals for dataset repartitioning can improve the best accuracy of the model trained for a fixed number of epochs. We conclude that the parameters related to importance-aware data-parallel training, including the importance measure, number of warmup training epochs, and others defined in the paper, may be considered as hyperparameters of data-parallel model training.},
  isbn = {978-3-031-35260-7},
  file = {C:\Users\Sebas\Zotero\storage\Q8CB9D7L\Sheikholeslami et al. - 2023 - The impact of importance-aware dataset partitionin.pdf}
}

@article{10.1145/3591335.3591347,
  title = {{{ANIARA}} Project - Automation of Network Edge Infrastructure and Applications with Artificial Intelligence},
  author = {John, Wolfgang and Balador, Ali and Taghia, Jalil and Johnsson, Andreas and Sj{\"o}berg, Johan and Marsh, Ian and Gustafsson, Jonas and Tonini, Federico and Monti, Paolo and Sk{\"o}ldstr{\"o}m, Pontus and Dowling, Jim},
  year = {2023},
  month = apr,
  journal = {Ada Lett.},
  volume = {42},
  number = {2},
  pages = {92--95},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {1094-3641},
  doi = {10.1145/3591335.3591347},
  abstract = {Emerging use-cases like smart manufacturing and smart cities pose challenges in terms of latency, which cannot be satisfied by traditional centralized infrastructure. Edge networks, which bring computational capacity closer to the users/clients, are a promising solution for supporting these critical low latency services. Different from traditional centralized networks, the edge is distributed by nature and is usually equipped with limited compute capacity. This creates a complex network to handle, subject to failures of different natures, that requires novel solutions to work in practice. To reduce complexity, edge application technology enablers, advanced infrastructure and application orchestration techniques need to be in place where AI and ML are key players.},
  issue_date = {December 2022},
  file = {C:\Users\Sebas\Zotero\storage\ITLFTN2Q\John et al. - 2023 - ANIARA project - automation of network edge infras.pdf}
}

@inproceedings{10.1145/3626246.3653389,
  title = {The {{Hopsworks Feature Store}} for {{Machine Learning}}},
  booktitle = {Companion of the 2024 {{International Conference}} on {{Management}} of {{Data}}},
  author = {{de la R{\'u}a Mart{\'i}nez}, Javier and Buso, Fabio and Kouzoupis, Antonios and Ormenisan, Alexandru A. and Niazi, Salman and Bzhalava, Davit and Mak, Kenneth and Jouffrey, Victor and Ronstr{\"o}m, Mikael and Cunningham, Raymond and Zangis, Ralfs and Mukhedkar, Dhananjay and Khazanchi, Ayushman and Vlassov, Vladimir and Dowling, Jim},
  year = {2024},
  month = jun,
  series = {{{SIGMOD}}/{{PODS}} '24},
  pages = {135--147},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626246.3653389},
  urldate = {2025-01-22},
  abstract = {Data management is the most challenging aspect of building Machine Learning (ML) systems. ML systems can read large volumes of historical data when training models, but inference workloads are more varied, depending on whether it is a batch or online ML system. The feature store for ML has recently emerged as a single data platform for managing ML data throughout the ML lifecycle, from feature engineering to model training to inference.  In this paper, we present the Hopsworks feature store for machine learning as a highly available platform for managing feature data with API support for columnar, row-oriented, and similarity search query workloads. We introduce and address challenges solved by the feature stores related to feature reuse, how to organize data transformations, and how to ensure correct and consistent data between feature engineering, model training, and model inference. We present the engineering challenges in building high-performance query services for a feature store and show how Hopsworks outperforms existing cloud feature stores for training and online inference query workloads.},
  isbn = {9798400704222},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\FGPTG6RW\\de la Rúa Martínez et al. - 2024 - The Hopsworks Feature Store for Machine Learning.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\ITNYNIV7\\de la Rúa Martínez et al. - 2024 - The Hopsworks Feature Store for Machine Learning.pdf}
}

@inproceedings{ahmadBenchmarkingApacheArrow2022,
  title = {Benchmarking {{Apache Arrow Flight}} - {{A}} Wire-Speed Protocol for Data Transfer, Querying and Microservices},
  booktitle = {Benchmarking in the {{Data Center}}: {{Expanding}} to the {{Cloud}}},
  author = {Ahmad, Tanveer},
  year = {2022},
  month = apr,
  pages = {1--10},
  publisher = {ACM},
  address = {Seoul Republic of Korea},
  doi = {10.1145/3527199.3527264},
  urldate = {2024-03-04},
  isbn = {978-1-4503-9324-9},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\UWX6GU4R\Ahmad - 2022 - Benchmarking Apache Arrow Flight - A wire-speed pr.pdf}
}

@mastersthesis{AlSamisti1181210,
  title = {Visual Debugging of Dataflow Systems},
  author = {Al Samisti, Fanti Machmount},
  year = {2017},
  series = {{{TRITA-ICT-EX}}},
  number = {2017:152},
  pages = {62},
  abstract = {Big data processing has seen vast integration into the idea of data analysis in live streaming and batch environments. A plethora of tools have been developed to break down a problem into manageable tasks and to allocate both software and hardware resources in a distributed and fault tolerant manner. Apache Spark is one of the most well known platforms for large-scale cluster computation. In SICS Swedish ICT, Spark runs on top of an in-house developed solution. HopsWorks provides a graphical user interface to the Hops platform that aims to simplify the process of configuring a Hadoop environment and improving upon it. The user interface includes, among other capabilities, an array of tools for executing distributed applications such as Spark, TensorFlow, Flink with a variety of input and output sources, e.g. Kafka, HDFS files etc. Currently the available tools to monitor and instrument a stack that includes the aforementioned technologies come from both the corporate and open source world. The former is usually part of a bigger family of products running on proprietary code. In contrast, the latter offers a wider variety of choices with the most prominent ones lacking either the flexibility in exchange for a more generic approach or the ease of gaining meaningful insight except of the most experienced users. The contribution of this project is a visualization tool in the form of a web user interface, part of the Hops platform, for understanding, debugging and ultimately optimizing the resource allocation and performance of dataflow applications. These processes are based both on the abstraction provided by the dataflow programming paradigm and on systems concepts such as properties of data, how much variability in the data, computation, distribution, and other system wide resources.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT)},
  file = {C:\Users\Sebas\Zotero\storage\FLWZI622\Al Samisti - 2017 - Visual debugging of dataflow systems.pdf}
}

@misc{altexsoftHowDataEngineering2021,
  title = {How {{Data Engineering Works}}},
  author = {{AltexSoft}},
  year = {2021},
  month = mar,
  urldate = {2025-01-22},
  abstract = {Licenza Attribuzione di Creative Commons (riutilizzo consentito)}
}

@misc{AnnouncingDeltaLake2023,
  title = {Announcing {{Delta Lake}} 3.0 with {{New Universal Format}} and {{Liquid Clustering}}},
  year = {Wed, 06/28/2023 - 23:53},
  journal = {Databricks},
  urldate = {2025-01-22},
  abstract = {Databricks announces Delta Lake 3.0 with a new universal format and liquid clustering for improved performance and cost savings.},
  howpublished = {https://www.databricks.com/blog/announcing-delta-lake-30-new-universal-format-and-liquid-clustering},
  langid = {american},
  file = {C:\Users\Sebas\Zotero\storage\FPNJ4XNX\announcing-delta-lake-30-new-universal-format-and-liquid-clustering.html}
}

@misc{ApacheArrowRs2024,
  title = {Apache/{{Arrow-Rs}}},
  year = {2024},
  month = sep,
  publisher = {The Apache Software Foundation},
  abstract = {Official Rust implementation of Apache Arrow},
  copyright = {Apache-2.0}
}

@misc{ApacheDataFusionApache,
  title = {Apache {{DataFusion}} --- {{Apache DataFusion}} Documentation},
  urldate = {2025-01-22},
  howpublished = {https://datafusion.apache.org/},
  file = {C:\Users\Sebas\Zotero\storage\DPVSVVNX\datafusion.apache.org.html}
}

@misc{ApacheHadoop,
  title = {Apache {{Hadoop}}},
  urldate = {2025-01-22},
  howpublished = {https://hadoop.apache.org/},
  file = {C:\Users\Sebas\Zotero\storage\HGLSCF9K\hadoop.apache.org.html}
}

@misc{ApacheHudiVs,
  title = {Apache {{Hudi}} vs {{Delta Lake}} vs {{Apache Iceberg}} - {{Data Lakehouse Feature Comparison}}},
  urldate = {2025-01-22},
  howpublished = {https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison},
  file = {C:\Users\Sebas\Zotero\storage\BN4XZTH8\apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison.html}
}

@misc{ApacheIcebergApache,
  title = {Apache {{Iceberg}} - {{Apache Iceberg}}™},
  urldate = {2025-01-22},
  howpublished = {https://iceberg.apache.org/},
  file = {C:\Users\Sebas\Zotero\storage\KV7SLYL8\iceberg.apache.org.html}
}

@misc{ApacheSparkOpen,
  title = {The {{Apache Spark Open Source Project}} on {{Open Hub}}},
  urldate = {2025-01-22},
  howpublished = {https://openhub.net/p/apache-spark},
  file = {C:\Users\Sebas\Zotero\storage\I2876KEK\apache-spark.html}
}

@article{armbrustDeltaLakeHighperformance2020,
  title = {Delta Lake: High-Performance {{ACID}} Table Storage over Cloud Object Stores},
  shorttitle = {Delta Lake},
  author = {Armbrust, Michael and Das, Tathagata and Sun, Liwen and Yavuz, Burak and Zhu, Shixiong and Murthy, Mukul and Torres, Joseph and {van Hovell}, Herman and Ionescu, Adrian and {\L}uszczak, Alicja and {\'S}witakowski, Micha{\l} and Szafra{\'n}ski, Micha{\l} and Li, Xiao and Ueshin, Takuya and Mokhtar, Mostafa and Boncz, Peter and Ghodsi, Ali and Paranjpye, Sameer and Senster, Pieter and Xin, Reynold and Zaharia, Matei},
  year = {2020},
  month = aug,
  journal = {Proc. VLDB Endow.},
  volume = {13},
  number = {12},
  pages = {3411--3424},
  issn = {2150-8097},
  doi = {10.14778/3415478.3415560},
  urldate = {2025-01-24},
  abstract = {Cloud object stores such as Amazon S3 are some of the largest and most cost-effective storage systems on the planet, making them an attractive target to store large data warehouses and data lakes. Unfortunately, their implementation as key-value stores makes it difficult to achieve ACID transactions and high performance: metadata operations such as listing objects are expensive, and consistency guarantees are limited. In this paper, we present Delta Lake, an open source ACID table storage layer over cloud object stores initially developed at Databricks. Delta Lake uses a transaction log that is compacted into Apache Parquet format to provide ACID properties, time travel, and significantly faster metadata operations for large tabular datasets (e.g., the ability to quickly search billions of table partitions for those relevant to a query). It also leverages this design to provide high-level features such as automatic data layout optimization, upserts, caching, and audit logs. Delta Lake tables can be accessed from Apache Spark, Hive, Presto, Redshift and other systems. Delta Lake is deployed at thousands of Databricks customers that process exabytes of data per day, with the largest instances managing exabyte-scale datasets and billions of objects.},
  file = {C:\Users\Sebas\Zotero\storage\BEGCG6MZ\Armbrust et al. - 2020 - Delta lake high-performance ACID table storage ov.pdf}
}

@misc{ArrowrsObjectStore,
  title = {Arrow-Rs {{Object Store Repository}}},
  urldate = {2025-01-22},
  howpublished = {https://github.com/apache/arrow-rs/blob/main/object\_store/README.md},
  file = {C:\Users\Sebas\Zotero\storage\QBBBK5E9\README.html}
}

@inproceedings{azaguryObjectStore2003,
  title = {Towards an Object Store},
  booktitle = {20th {{IEEE}}/11th {{NASA Goddard Conference}} on {{Mass Storage Systems}} and {{Technologies}}, 2003. ({{MSST}} 2003). {{Proceedings}}.},
  author = {Azagury, A. and Dreizin, V. and Factor, M. and Henis, E. and Naor, D. and Rinetzky, N. and Rodeh, O. and Satran, J. and Tavory, A. and Yerushalmi, L.},
  year = {2003},
  pages = {165--176},
  publisher = {IEEE Comput. Soc},
  address = {San Diego, CA, USA},
  doi = {10.1109/MASS.2003.1194853},
  urldate = {2024-02-20},
  abstract = {Today's SAN architectures promise unmediated host access to storage (i.e., without going through a server). To achieve this promise, however, we must address several issues and opportunities raised by SANs, including security, scalability and management. Object storage, such as introduced by the NASD work [14], is a means of addressing these issues and opportunities. An object store raises the level of abstraction presented by a storage control unit from an array of 512 byte blocks to a collection of objects. The object store provides ``fine-grain,'' object-level security, improved scalability by localizing space management, and improved management by allowing end-to-end management of semantically meaningful entities.},
  isbn = {978-0-7695-1914-2},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\UKGIYZM7\Azagury et al. - 2003 - Towards an object store.pdf}
}

@article{Balasub2017,
  title = {System {{Programming}} in {{Rust}}: {{Beyond Safety}}},
  shorttitle = {System {{Programming}} in {{Rust}}},
  author = {Balasubramanian, Abhiram and Baranowski, Marek S. and Burtsev, Anton and Panda, Aurojit and Rakamari, Zvonimir and Ryzhyk, Leonid},
  year = {2017},
  month = sep,
  journal = {SIGOPS Oper. Syst. Rev.},
  volume = {51},
  number = {1},
  pages = {94--99},
  issn = {0163-5980},
  doi = {10.1145/3139645.3139660},
  urldate = {2025-01-22},
  abstract = {Rust is a new system programming language that offers a practical and safe alternative to C. Rust is unique in that it enforces safety without runtime overhead, most importantly, without the overhead of garbage collection. While zero-cost safety is remarkable on its own, we argue that the superpowers of Rust go beyond safety. In particular, Rust's linear type system enables capabilities that cannot be implemented efficiently in traditional languages, both safe and unsafe, and that dramatically improve security and reliability of system software. We show three examples of such capabilities: zero-copy software fault isolation, efficient static information flow analysis, and automatic checkpointing. While these capabilities have been in the spotlight of systems research for a long time, their practical use is hindered by high cost and complexity. We argue that with the adoption of Rust these mechanisms will become commoditized.},
  file = {C:\Users\Sebas\Zotero\storage\ITQNN6GN\Balasubramanian et al. - 2017 - System Programming in Rust Beyond Safety.pdf}
}

@inproceedings{behmPhotonFastQuery2022,
  title = {Photon: {{A Fast Query Engine}} for {{Lakehouse Systems}}},
  shorttitle = {Photon},
  booktitle = {Proceedings of the 2022 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Behm, Alexander and Palkar, Shoumik and Agarwal, Utkarsh and Armstrong, Timothy and Cashman, David and Dave, Ankur and Greenstein, Todd and Hovsepian, Shant and Johnson, Ryan and Sai Krishnan, Arvind and Leventis, Paul and Luszczak, Ala and Menon, Prashanth and Mokhtar, Mostafa and Pang, Gene and Paranjpye, Sameer and Rahn, Greg and Samwel, Bart and Van Bussel, Tom and Van Hovell, Herman and Xue, Maryann and Xin, Reynold and Zaharia, Matei},
  year = {2022},
  month = jun,
  pages = {2326--2339},
  publisher = {ACM},
  address = {Philadelphia PA USA},
  doi = {10.1145/3514221.3526054},
  urldate = {2025-01-22},
  abstract = {Many organizations are shifting to a data management paradigm called the ``Lakehouse,'' which implements the functionality of structured data warehouses on top of unstructured data lakes. This presents new challenges for query execution engines. The execution engine needs to provide good performance on the raw uncurated datasets that are ubiquitous in data lakes, and excellent performance on structured data stored in popular columnar file formats like Apache Parquet. Toward these goals, we present Photon, a vectorized query engine for Lakehouse environments that we developed at Databricks. Photon can outperform existing cloud data warehouses in SQL workloads, but implements a more general execution framework that enables efficient processing of raw data and also enables Photon to support the Apache Spark API. We discuss the design choices we made in Photon (e.g., vectorization vs. code generation) and describe its integration with our existing SQL and Apache Spark runtimes, its task model, and its memory manager. Photon has accelerated some customer workloads by over 10{\texttimes} and has recently allowed Databricks to set a new audited performance record for the official 100TB TPC-DS benchmark.},
  isbn = {978-1-4503-9249-5},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\GQ4I8A7K\Behm et al. - 2022 - Photon A Fast Query Engine for Lakehouse Systems.pdf}
}

@article{belovAnalysisBigData2021,
  title = {Analysis of {{Big Data Storage Tools}} for {{Data Lakes}} Based on {{Apache Hadoop Platform}}},
  author = {Belov, Vladimir and Nikulchev, Evgeny},
  year = {2021},
  month = aug,
  journal = {International Journal of Advanced Computer Science and Applications},
  volume = {12},
  pages = {551--557},
  doi = {10.14569/IJACSA.2021.0120864},
  abstract = {When developing large data processing systems, the question of data storage arises. One of the modern tools for solving this problem is the so-called data lakes. Many implementations of data lakes use Apache Hadoop as a basic platform. Hadoop does not have a default data storage format, which leads to the task of choosing a data format when designing a data processing system. To solve this problem, it is necessary to proceed from the results of the assessment according to several criteria. In turn, experimental evaluation does not always give a complete understanding of the possibilities for working with a particular data storage format. In this case, it is necessary to study the features of the format, its internal structure, recommendations for use, etc. The article describes the features of both widely used data storage formats and the currently gaining popularity.},
  file = {C:\Users\Sebas\Zotero\storage\9XXW6REL\Belov and Nikulchev - 2021 - Analysis of Big Data Storage Tools for Data Lakes .pdf}
}

@misc{BenchmarkResultsSpark,
  title = {{{DataFrames}} at {{Scale Comparison}}: {{TPC-H}}},
  shorttitle = {{{DataFrames}} at {{Scale Comparison}}},
  journal = {Coiled},
  urldate = {2025-01-22},
  abstract = {Hendrik Makait, Sarah Johnson, Matthew Rocklin 2024-05-14 14 min read We run benchmarks derived from the TPC-H benchmark suite on a variety of scales, hardware architectures, and dataframe projects...},
  howpublished = {https://docs.coiled.io/blog/tpch.html},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\KDCZ6UUM\tpch.html}
}

@misc{BigDataVendorIceberg,
  title = {Big Data Vendors Embrace {{Apache Iceberg}}},
  author = {Clark, Lindsay},
  urldate = {2025-01-25},
  abstract = {Market rivals settle on open table format, while Microsoft and Databricks go their own way},
  howpublished = {https://www.theregister.com/2024/10/14/apache\_iceberg\_feature\_announcements/},
  langid = {english}
}

@misc{binfordKimahrimanHdfsnative2025,
  title = {Kimahriman/Hdfs-Native},
  author = {Binford, Adam},
  year = {2025},
  month = jan,
  urldate = {2025-01-22},
  copyright = {Apache-2.0}
}

@misc{BlockVsFile,
  title = {Block vs {{File}} vs {{Object Storage}} - {{Difference Between Data Storage Services}} - {{AWS}}},
  journal = {Amazon Web Services, Inc.},
  urldate = {2025-01-22},
  abstract = {What's the difference between Block, File and Object Storage? How to Use Block, File and Object storage with AWS.},
  howpublished = {https://aws.amazon.com/compare/the-difference-between-block-file-object-storage/},
  langid = {american},
  file = {C:\Users\Sebas\Zotero\storage\FF44X3TR\the-difference-between-block-file-object-storage.html}
}

@inproceedings{bogdanovNearestReplicaCan2015,
  title = {The Nearest Replica Can Be Farther than You Think},
  booktitle = {Proceedings of the {{Sixth ACM Symposium}} on {{Cloud Computing}}},
  author = {Bogdanov, Kirill and {Pe{\'o}n-Quir{\'o}s}, Miguel and Maguire, Gerald Q. and Kosti{\'c}, Dejan},
  year = {2015},
  month = aug,
  series = {{{SoCC}} '15},
  pages = {16--29},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2806777.2806939},
  urldate = {2025-01-22},
  abstract = {Modern distributed systems are geo-distributed for reasons of increased performance, reliability, and survivability. At the heart of many such systems, e.g., the widely used Cassandra and MongoDB data stores, is an algorithm for choosing a closest set of replicas to service a client request. Suboptimal replica choices due to dynamically changing network conditions result in reduced performance as a result of increased response latency. We present GeoPerf, a tool that tries to automate the process of systematically testing the performance of replica selection algorithms for geo-distributed storage systems. Our key idea is to combine symbolic execution and lightweight modeling to generate a set of inputs that can expose weaknesses in replica selection. As part of our evaluation, we analyzed network round trip times between geographically distributed Amazon EC2 regions, and showed a significant number of daily changes in nearest-K replica orders. We tested Cassandra and MongoDB using our tool, and found bugs in each of these systems. Finally, we use our collected Amazon EC2 latency traces to quantify the time lost due to these bugs. For example due to the bug in Cassandra, the median wasted time for 10\% of all requests is above 50 ms.},
  isbn = {978-1-4503-3651-2},
  file = {C:\Users\Sebas\Zotero\storage\M26PMYH5\Bogdanov et al. - 2015 - The nearest replica can be farther than you think.pdf}
}

@incollection{bonomiImprovedConstructionCounting2006,
  title = {An {{Improved Construction}} for {{Counting Bloom Filters}}},
  booktitle = {Algorithms -- {{ESA}} 2006},
  author = {Bonomi, Flavio and Mitzenmacher, Michael and Panigrahy, Rina and Singh, Sushil and Varghese, George},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Azar, Yossi and Erlebach, Thomas},
  year = {2006},
  volume = {4168},
  pages = {684--695},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11841036_61},
  urldate = {2024-03-06},
  abstract = {A counting Bloom filter (CBF) generalizes a Bloom filter data structure so as to allow membership queries on a set that can be changing dynamically via insertions and deletions. As with a Bloom filter, a CBF obtains space savings by allowing false positives. We provide a simple hashing-based alternative based on d-left hashing called a d-left CBF (dlCBF). The dlCBF offers the same functionality as a CBF, but uses less space, generally saving a factor of two or more. We describe the construction of dlCBFs, provide an analysis, and demonstrate their effectiveness experimentally.},
  isbn = {978-3-540-38875-3 978-3-540-38876-0},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\JCSMDXLT\Bonomi et al. - 2006 - An Improved Construction for Counting Bloom Filter.pdf}
}

@article{borthakurHadoopDistributedFile2005,
  title = {The {{Hadoop Distributed File System}}: {{Architecture}} and {{Design}}},
  author = {Borthakur, Dhruba},
  year = {2007},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\EZ4RYSSH\Borthakur - 2007 - The Hadoop Distributed File System Architecture a.pdf}
}

@misc{BuildSoftwareBetter,
  title = {Build Software Better, Together},
  journal = {GitHub},
  urldate = {2025-01-22},
  abstract = {GitHub is where people build software. More than 150 million people use GitHub to discover, fork, and contribute to over 420 million projects.},
  howpublished = {https://github.com},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\H3W5K55D\github.com.html}
}

@mastersthesis{Buso1149002,
  title = {{{SQL}} on Hops},
  author = {Buso, Fabio},
  year = {2017},
  series = {{{TRITA-ICT-EX}}},
  number = {2017:146},
  pages = {50},
  abstract = {In today's world data is extremely valuable. Companies and researchers store every sort of data, from users activities to medical records. However, data is useless if one cannot extract meaning and insight from it. In 2004 Dean and Ghemawat introduced the MapReduce framework. This sparked the development of open source frameworks for big data storage (HDFS) and processing (Hadoop). Hops and Apache Hive build on top of this heritage. The former proposes a new distributed file system which achieves higher scalability and throughput by storing metadata in a database called MySQL-Cluster. The latter is an open source data warehousing solution built on top of the Hadoop ecosystems, which allows users to query big data stored on HDFS using a SQL-like query language.Apache Hive is a widely used and mature project, however it lacks of consistency between the data stored on the file system and the metadata describing it, stored on a relational database. This means that if users delete Hive's data from the file system, Hive does not delete the related metadata. This causes two issues: (1) users do not get an error if the data is missing from the filesystem (2) if users forget to delete the metadata, it will become orphaned in the database. In this thesis we exploit the fact that both HopsFS' metadata and Hive's metadata is stored in a relational database, to provide a mechanisms to automatically delete Hive's metadata if the data is delete from the file system.The second objective of this thesis is to integrate Apache Hive into the Hops ecosystem and in particular in the HopsWorks platform. HopsWorks is a multitenant, UI based service which allows users to store and process big data projects. In this thesis we develop a custom authenticator for Hive to allow HopsWorks users to authenticate with Hive and to integrate with its security model.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT)},
  file = {C:\Users\Sebas\Zotero\storage\2LR5YEK5\Buso - 2017 - SQL on hops.pdf}
}

@misc{camacho-rodriguezLSTBenchBenchmarkingLogStructured2024,
  title = {{{LST-Bench}}: {{Benchmarking Log-Structured Tables}} in the {{Cloud}}},
  shorttitle = {{{LST-Bench}}},
  author = {{Camacho-Rodr{\'i}guez}, Jes{\'u}s and Agrawal, Ashvin and Gruenheid, Anja and Gosalia, Ashit and Petculescu, Cristian and {Aguilar-Saborit}, Josep and Floratou, Avrilia and Curino, Carlo and Ramakrishnan, Raghu},
  year = {2024},
  month = jan,
  eprint = {2305.01120},
  primaryclass = {cs},
  doi = {10.1145/3639314},
  urldate = {2024-02-16},
  abstract = {Data processing engines increasingly leverage distributed file systems for scalable, cost-effective storage. While the Apache Parquet columnar format has become a popular choice for data storage and retrieval, the immutability of Parquet files renders it impractical to meet the demands of frequent updates in contemporary analytical workloads. Log-Structured Tables (LSTs), such as Delta Lake, Apache Iceberg, and Apache Hudi, offer an alternative for scenarios requiring data mutability, providing a balance between efficient updates and the benefits of columnar storage. They provide features like transactions, time-travel, and schema evolution, enhancing usability and enabling access from multiple engines. Moreover, engines like Apache Spark and Trino can be configured to leverage the optimizations and controls offered by LSTs to meet specific business needs. Conventional benchmarks and tools are inadequate for evaluating the transformative changes in the storage layer resulting from these advancements, as they do not allow us to measure the impact of design and optimization choices in this new setting. In this paper, we propose a novel benchmarking approach and metrics that build upon existing benchmarks, aiming to systematically assess LSTs. We develop a framework, LST-Bench, which facilitates effective exploration and evaluation of the collaborative functioning of LSTs and data processing engines through tailored benchmark packages. A package is a mix of use patterns reflecting a target workload; LST-Bench makes it easy to define a wide range of use patterns and combine them into a package, and we include a baseline package for completeness. Our assessment demonstrates the effectiveness of our framework and benchmark packages in extracting valuable insights across diverse environments. The code for LST-Bench is open-sourced and is available at https://github.com/microsoft/lst-bench/ .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\CWQ8CNUP\\Camacho-Rodríguez et al. - 2024 - LST-Bench Benchmarking Log-Structured Tables in t.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\QUMDKK3X\\2305.html}
}

@article{carboneApacheFlinkStream,
  title = {Apache {{Flink}}™: {{Stream}} and {{Batch Processing}} in a {{Single Engine}}},
  author = {Carbone, Paris and Katsifodimos, Asterios and Ewen, Stephan and Markl, Volker and Haridi, Seif and Tzoumas, Kostas},
  abstract = {Apache Flink1 is an open-source system for processing streaming and batch data. Flink is built on the philosophy that many classes of data processing applications, including real-time analytics, continuous data pipelines, historic data processing (batch), and iterative algorithms (machine learning, graph analysis) can be expressed and executed as pipelined fault-tolerant dataflows. In this paper, we present Flink's architecture and expand on how a (seemingly diverse) set of use cases can be unified under a single execution model.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\7IWTL3GF\Carbone et al. - Apache Flink™ Stream and Batch Processing in a Si.pdf}
}

@article{chaudhuriOverviewDataWarehousing1997,
  title = {An Overview of Data Warehousing and {{OLAP}} Technology},
  author = {Chaudhuri, Surajit and Dayal, Umeshwar},
  year = {1997},
  month = mar,
  journal = {ACM SIGMOD Record},
  volume = {26},
  number = {1},
  pages = {65--74},
  issn = {0163-5808},
  doi = {10.1145/248603.248616},
  urldate = {2025-01-22},
  abstract = {Data warehousing and on-line analytical processing (OLAP) are essential elements of decision support, which has increasingly become a focus of the database industry. Many commercial products and services are now available, and all of the principal database management system vendors now have offerings in these areas. Decision support places some rather different requirements on database technology compared to traditional on-line transaction processing applications. This paper provides an overview of data warehousing and OLAP technologies, with an emphasis on their new requirements. We describe back end tools for extracting, cleaning and loading data into a data warehouse; multidimensional data models typical of OLAP; front end client tools for querying and data analysis; server extensions for efficient query processing; and tools for metadata management and for managing the warehouse. In addition to surveying the state of the art, this paper also identifies some promising research issues, some of which are related to problems that the database research community has worked on for years, but others are only just beginning to be addressed. This overview is based on a tutorial that the authors presented at the VLDB Conference, 1996.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\TZADNMKV\Chaudhuri and Dayal - 1997 - An overview of data warehousing and OLAP technolog.pdf}
}

@mastersthesis{Chen1705419,
  title = {Data Build Tool ({{DBT}}) Jobs in Hopsworks},
  author = {Chen, Zidi},
  year = {2022},
  series = {{{TRITA-EECS-EX}}},
  number = {2022:402},
  pages = {51},
  abstract = {Feature engineering at scale is always critical and challenging in the machine learning pipeline. Modern data warehouses enable data analysts to do feature engineering by transforming, validating and aggregating data in Structured Query Language (SQL). To help data analysts do this work, Data Build Tool (DBT), an open-source tool, was proposed to build and orchestrate SQL pipelines. Hopsworks, an open-source scalable feature store, would like to add support for DBT so that data scientists can do feature engineering in Python, Spark, Flink, and SQL in a single platform. This project aims to create a concept about how to build this support and then implement it. The project checks the feasibility of the solution using a sample DBT project. According to measurements, this working solution needs around 800 MB of space in the server and it takes more time than executing DBT commands locally. However, it persistently stores the results of each execution in HopsFS, which are available to users. By adding this novel support for SQL using DBT, Hopsworks might be one of the completest platforms for feature engineering so far.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {feature engineering,funktionsteknik,Structured Query Language (SQL),strukturerat fragesprak (SQL)},
  file = {C:\Users\Sebas\Zotero\storage\SRJ8FSCZ\Chen - 2022 - Data build tool (DBT) jobs in hopsworks.pdf}
}

@mastersthesis{Chikafa1614362,
  title = {Project Based Multi-Tenant Managed {{RStudio}} on {{Kubernetes}} for {{Hopsworks}}},
  author = {Chikafa, Gibson},
  year = {2021},
  series = {{{TRITA-EECS-EX}}},
  number = {2021:727},
  pages = {67},
  abstract = {In order to fully benefit from cloud computing, services are designed following the ``multi-tenant'' architectural model which is aimed at maximizing resource sharing among users. However, multi-tenancy introduces challenges of security, performance isolation, scaling and customization. RStudio server is an open source Integrated Development Environment (IDE) accessible over a web browser for R programming language. The purpose of this thesis is to develop an open source multi-user distributed system on Hopsworks, a data intensive and AI platform, following the multi-tenant model, that provides RStudio as Software as a Service (SaaS). Our goal is to promote collaboration among users when using RStudio and the learning and teaching of R by enabling users easily have access to same computational environments and resources while eliminating installation and maintenance tasks. Hopsworks introduces project-based multi-tenancy where users within a project share project resources (e.g datasets, programs and services) for collaboration which introduces one more challenge of sharing project resources in RStudio server instances. To achieve our purpose and goal we therefore needed to solve the following problems: performance isolation, security, project resources sharing, scaling and customization. Our proposed model is on demand single user RStudio server instances per project. Our system is built around Docker and Kubernetes to solve the problems of performance isolation, security and scaling. We introduce HopsFS-mount, that allows securely mounting HopsFS via FUSE to solve the project resources (datasets and programs) sharing problem. We integrate our system with Apache Spark which can scale and handle Big Data processing workloads. Also we provide a UI where users can provide custom configuration and have full control of their own RStudio server instances. Our system was tested on a GCP cluster with four worker nodes each with 30GB of RAM allocated to them. The tests on this cluster showed that 44 RStudio servers, each with 2GB of RAM, can be run concurrently. Our system can scale out to potentially support hundreds of concurrently running RStudio servers by adding more resources (CPUs and RAM) to the cluster or system.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Azure,Cloud computing,Docker,GCP,Kubernetes,Molntjanster,Multi-tenancy,Multitenans,Performance isolation,Prestandaisolering,Sakerhet,Scaling,Security,Skalning},
  file = {C:\Users\Sebas\Zotero\storage\B6PIHFBZ\Chikafa - 2021 - Project based multi-tenant managed RStudio on Kube.pdf}
}

@misc{crociDataLakehouseHype2022,
  title = {Data {{Lakehouse}}, beyond the Hype},
  author = {Croci, Daniele},
  year = {2022},
  month = dec,
  journal = {Bitrock},
  urldate = {2025-01-22},
  abstract = {In this blogpost we explore the data lakehouse as new concept that moves data lakes closer to warehouses, to compete in the BI and analytical scenario},
  langid = {american},
  file = {C:\Users\Sebas\Zotero\storage\HQJKD8XK\data-lakehouse.html}
}

@misc{DatalakehouseCostEfficiency,
  title = {Dremio {{Report Highlights Surge}} in {{Data Lakehouse Adoption}} for {{Enhanced Cost Efficiency}} and {{Analytics}}},
  year = {2024},
  journal = {BigDATAwire},
  urldate = {2025-01-25},
  abstract = {SANTA CLARA, Calif., Nov. 28, 2023 --~Dremio today announced the release of its survey findings and full report, The State of the Data Lakehouse, 2024.},
  howpublished = {https://www.bigdatawire.com/this-just-in/new-dremio-report-highlights-surge-in-data-lakehouse-adoption-for-enhanced-cost-efficiency-and-analytics/}
}

@article{DataLakehouseSurvey2025,
  title = {Data {{Lakehouse}}: {{A}} Survey and Experimental Study},
  shorttitle = {Data {{Lakehouse}}},
  year = {2025},
  month = jan,
  journal = {Information Systems},
  volume = {127},
  pages = {102460},
  publisher = {Pergamon},
  issn = {0306-4379},
  doi = {10.1016/j.is.2024.102460},
  urldate = {2025-01-23},
  abstract = {Efficient big data management is a dire necessity to manage the exponential growth in data generated by digital information systems to produce usable {\dots}},
  langid = {american},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\WZ8Q4U5X\\2025 - Data Lakehouse A survey and experimental study.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\MUHBC4N9\\S0306437924001182.html}
}

@article{dean2004mapreduce,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  month = jan,
  journal = {Commun. ACM},
  volume = {51},
  number = {1},
  pages = {107--113},
  issn = {0001-0782},
  doi = {10.1145/1327452.1327492},
  urldate = {2025-01-22},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\KS5BF3MK\\Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\UE5DYZ7Y\\Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf}
}

@misc{DeltaioDeltarsRepository2025,
  title = {Delta-Io {{Delta-rs Repository}}},
  year = {2025},
  month = jan,
  urldate = {2025-01-22},
  abstract = {A native Rust library for Delta Lake, with bindings into Python},
  copyright = {Apache-2.0},
  howpublished = {Delta Lake},
  keywords = {databricks,delta,delta-lake,pandas,pandas-dataframe,python,rust}
}

@misc{DeltarsSilemoRepository,
  title = {Delta-Rs {{Silemo Repository}}},
  journal = {GitHub},
  urldate = {2025-01-22},
  abstract = {A native Rust library for Delta Lake, with bindings into Python - Silemo/delta-rs},
  howpublished = {https://github.com/Silemo/delta-rs/tree/main/python},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\BNFK4946\python.html}
}

@article{despa2014comparative,
  title = {Comparative Study on Software Development Methodologies},
  author = {Despa, Mihai Liviu},
  number = {3},
  abstract = {This paper focuses on the current state of knowledge in the field of software development methodologies. It aims to set the stage for the formalization of a software development methodology dedicated to innovation orientated IT projects. The paper starts by depicting specific characteristics in software development project management. Managing software development projects involves techniques and skills that are proprietary to the IT industry. Also the software development project manager handles challenges and risks that are predominantly encountered in business and research areas that involve state of the art technology. Conventional software development stages are defined and briefly described. Development stages are the building blocks of any software development methodology so it is important to properly research this aspect. Current software development methodologies are presented. Development stages are defined for every showcased methodology. For each methodology a graphic representation is illustrated in order to better individualize its structure. Software development methodologies are compared by highlighting strengths and weaknesses from the stakeholder's point of view. Conclusions are formulated and a research direction aimed at formalizing a software development methodology dedicated to innovation orientated IT projects is enunciated.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\77N7TCIX\Despa - Comparative study on software development methodol.pdf}
}

@mastersthesis{DessalegnMuruts1091136,
  title = {Multi-Tenant Apache Kafka for Hops : {{Kafka}} Topic-Based Multi-Tenancy and {{ACL-}} Based Authorization for Hops},
  author = {Dessalegn Muruts, Misganu},
  year = {2016},
  series = {{{TRITA-ICT-EX}}},
  number = {2016:120},
  pages = {52},
  abstract = {Apache Kafka is a distributed, high throughput and fault-tolerant publish/subscribe messaging system in the Hadoop ecosystem. It is used as a distributed data streaming and processing platform. Kafka topics are the units of message feeds in the Kafka cluster. Kafka producer publishes messages into these topics and a Kafka consumer subscribes to topics to pull those messages. With the increased usage of Kafka in the data infrastructure of many companies, there are many Kafka clients that publish and consume messages to/from the Kafka topics. In fact, these client operations can be malicious. To mitigate this risk, clients must authenticate themselves and their operation must be authorized before they can access to a given topic. Nowadays, Kafka ships with a pluggable Authorizer interface to implement access control list (ACL) based authorization for client operation. Kafka users can implement the interface differently to satisfy their security requirements. SimpleACLAuthorizer is the out-of-box implementation of the interface and uses a Zookeeper for ACLs storage.HopsWorks, based on Hops a next generation Hadoop distribution, provides support for project-based multi-tenancy, where projects are fully isolated at the level of the Hadoop Filesystem and YARN. In this project, we added Kafka topicbased multi-tenancy in Hops projects. Kafka topic is created from inside Hops project and persisted both at the Zookeeper and the NDBCluster. Persisting a topic into a database enabled us for topic sharing across projects. ACLs are added to Kafka topics and are persisted only into the database. Client access to Kafka topics is authorized based on these ACLs. ACLs are added, updated, listed and/or removed from the HopsWorks WebUI. HopsACLAuthorizer, a Hops implementation of the Authorizer interface, authorizes Kafka client operations using the ACLs in the database. The Apache Avro schema registry for topics enabled the producer and consumer to better integrate by transferring a preestablished message format. The result of this project is the first Hadoop distribution that supports Kafka multi-tenancy.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT)},
  keywords = {ACL Authorization,Hadoop,Hops,HopsWorks,Kafka,Kafka Topics,Messaging Systems,Multi-Tenancy,Schema Registry},
  file = {C:\Users\Sebas\Zotero\storage\NMXE2CME\Dessalegn Muruts - 2016 - Multi-tenant apache kafka for hops  Kafka topic-b.pdf}
}

@misc{DigitalCommonsOAIPMH,
  title = {Digital {{Commons}} and {{OAI-PMH}}: {{Outbound Harvesting}} of {{Repository Records}}},
  shorttitle = {Digital {{Commons}} and {{OAI-PMH}}},
  journal = {Digital Commons},
  urldate = {2025-01-22},
  abstract = {Digital Commons supports the~Open Archives Initiative~Protocol for Metadata Harvesting (OAI-PMH) for the sharing of repository records. Administrators have},
  howpublished = {https://digitalcommons.elsevier.com/integration-preservation/digital-commons-and-oai-pmh},
  langid = {american},
  file = {C:\Users\Sebas\Zotero\storage\Y9Y6922T\digital-commons-and-oai-pmh.html}
}

@misc{DockerBuild0100,
  title = {Docker {{Build}}},
  year = {13:49:53 +0100 +0100},
  journal = {Docker Documentation},
  urldate = {2025-01-22},
  abstract = {Get an overview of Docker Build to package and bundle your code and ship it anywhere},
  howpublished = {https://docs.docker.com/build/},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\RDQTWRRT\build.html}
}

@misc{DremelMadeSimple,
  title = {Dremel Made Simple with {{Parquet}}},
  urldate = {2025-01-22},
  abstract = {Dremel made simple with Parquet},
  howpublished = {https://blog.x.com/engineering/en\_us/a/2013/dremel-made-simple-with-parquet},
  langid = {american},
  file = {C:\Users\Sebas\Zotero\storage\3YKEA92D\dremel-made-simple-with-parquet.html}
}

@misc{ebergenUpdatesH2OAi2023,
  title = {Updates to the {{H2O}}.Ai Db-Benchmark! -- {{DuckDB}}},
  urldate = {2025-01-22},
  howpublished = {https://duckdb.org/2023/11/03/db-benchmark-update.html},
  file = {C:\Users\Sebas\Zotero\storage\ZXBZBHLJ\db-benchmark-update.html}
}

@misc{ederUnstructuredData802008,
  title = {The 80\% {{Blind Spot}}: {{Are You Ignoring Unstructured Organizational Data}}?},
  urldate = {2025-01-22},
  howpublished = {https://www.forbes.com/councils/forbestechcouncil/2019/01/29/the-80-blind-spot-are-you-ignoring-unstructured-organizational-data/},
  file = {C:\Users\Sebas\Zotero\storage\D8SHSS4G\the-80-blind-spot-are-you-ignoring-unstructured-organizational-data.html}
}

@misc{ETDMSV11Interoperability,
  title = {{{ETD-MS}} v1.1: An {{Interoperability Metadata Standard}} for {{Electronic Theses}} and {{Dissertations}} --- {{NDLTD}}},
  urldate = {2025-01-22},
  howpublished = {https://ndltd.org/wp-content/uploads/2021/04/etd-ms-v1.1.html},
  file = {C:\Users\Sebas\Zotero\storage\3ABUPVMB\etd-ms-v1.1.html}
}

@inproceedings{factorObjectStorageFuture2005,
  title = {Object Storage: The Future Building Block for Storage Systems},
  shorttitle = {Object Storage},
  booktitle = {2005 {{IEEE International Symposium}} on {{Mass Storage Systems}} and {{Technology}}},
  author = {Factor, M. and Meth, K. and Naor, D. and Rodeh, O. and Satran, J.},
  year = {2005},
  month = jun,
  pages = {119--123},
  doi = {10.1109/LGDI.2005.1612479},
  urldate = {2024-02-20},
  abstract = {The concept of object storage was introduced in the early 1990's by the research community. Since then it has greatly matured and is now in its early stages of adoption by the industry. Yet, object storage is still not widely accepted. Viewing object store technology as the future building block particularly for large storage systems, our team in IBM Haifa Research Lab has invested substantial efforts in this area. In this position paper we survey the latest developments in the area of object store technology, focusing on standardization, research prototypes, and technology adoption and deployment. A major step has been the approval of the TIO OSD protocol (version I) as an OSD standard in late 2004. We also report on prototyping efforts that are carried out in IBM Haifa Research Lab in building an object store. Our latest prototype is compliant with a large subset of the TIO standard. To facilitate deployment of the new technology and protocol in the community at large, our team also implemented a TIO-compliant OSD (iSCSI) initiator for Linux. The initiator is interoperable with object disks of other vendors. The initiator is available as an open source driver for Linux.},
  keywords = {Access control,File servers,Linux,Logic arrays,Paper technology,Protocols,Prototypes,Secure storage,Space technology,Storage area networks},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\FZL8I7VH\\Factor et al. - 2005 - Object storage the future building block for stor.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\4ZW8HYXL\\1612479.html}
}

@misc{fannCocrustAnalyzerRepository2025,
  title = {Coc-Rust {{Analyzer Repository}}},
  author = {Fann, Heyward},
  year = {2025},
  month = jan,
  urldate = {2025-01-22},
  abstract = {rust-analyzer extension for coc.nvim},
  copyright = {MIT},
  keywords = {coc,coc-extensions,coc-nvim,rust,rust-analyzer}
}

@mastersthesis{FilotasSiskos1301514,
  title = {Secure Data Streaming into {{HopsWorks}} from Occasionally Connected Mobile Devices},
  author = {Filotas Siskos, Stavros},
  year = {2018},
  series = {{{TRITA-EECS-EX}}},
  number = {2018:20},
  pages = {52},
  abstract = {The number of devices that are connected to the internet has recently surpassed the number of human beings living on our planet. These devices are capable of generating a tremendous amount of data. Organizations, researchers and companies are faced with unique challenges when collecting, storing and analysing the data for the benefit and progress of the human kind. To address such challenges, cutting-edge Big Data platforms are needed like the Hadoop Open Platform-as-a-Service (Hops) ecosystem which is the resulting work of many years of continuous research conducted at RISE SICS in collaboration with KTH Royal Institute of Technology. This master thesis provides a means for ingesting streams of data generated from internet enabled devices into Hops in a secure and reliable way for storage and further stream processing and data analysis. In order to accomplish data ingestion, the HopsWorks component of Hops has been extended with an API where authenticated devices can stream data to. Furthermore, as a use case scenario the hopsworks-android-client library has been built in the Android platform for facilitating the secure and reliable data collection of streams of records from any Android application. The library has also been used as a testbed for performing system reliability and system end-to-end performance experiments for the developed system extension. The results show that this approach works and since the system is based on the well defined and heavily used HTTPS protocol it enables any interested stakeholder to collect data from mobile or IoT devices into Hops in order to not only utilize all the functional features that Hops offers but also to leverage its desired system properties such as high-availability, high-performance and scalability.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)}
}

@book{framptonCompleteGuideOpen2018,
  title = {Complete {{Guide}} to {{Open Source Big Data Stack}}},
  author = {Frampton, Michael},
  year = {2018},
  publisher = {Apress},
  address = {Berkeley, CA},
  doi = {10.1007/978-1-4842-2149-5},
  urldate = {2025-01-24},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-1-4842-2148-8 978-1-4842-2149-5},
  langid = {english},
  keywords = {Apache Brooklyn,Apache cloud stack,Apache Kafka,Apache Spark,Apache Zeppelin,Big data stack,Cassandra,data structures,Data visualization,Hadoop,Open source software,RIAK},
  file = {C:\Users\Sebas\Zotero\storage\UPE55JCB\Frampton - 2018 - Complete Guide to Open Source Big Data Stack.pdf}
}

@phdthesis{Gebremeskel1224181,
  title = {Analysis and Comparison of Distributed Training Techniques for Deep Neural Networks in a Dynamic Environment},
  author = {Gebremeskel, Ermias},
  year = {2018},
  series = {{{TRITA-EECS-EX}}},
  number = {2018:375},
  pages = {72},
  abstract = {Deep learning models' prediction accuracy tends to improve with the size of the model. The implications being that the amount of computational power needed to train models is continuously increasing. Distributed deep learning training tries to address this issue by spreading the computational load onto several devices. In theory, distributing computation onto N devices should give a performance improvement of xN. Yet, in reality the performance improvement is rarely xN, due to communication and other overheads. This thesis will study the communication overhead incurred when distributing deep learning training. Hopsworks is a platform designed for data science. The purpose of this work is to explore a feasible way of deploying distributed deep learning training on a shared cluster and analyzing the performance of different distributed deep learning algorithms to be used on this platform. The findings of this study show that bandwidth-optimal communication algorithms like ring all-reduce scales better than many-to-one communication algorithms like parameter server, but were less fault tolerant. Furthermore, system usage statistics collected revealed a network bottleneck when training is distributed on multiple machines. This work also shows that it is possible to run MPI on a hadoop cluster by building a prototype that orchestrates resource allocation, deployment, and monitoring of MPI based training jobs. Even though the experiments did not cover different cluster configurations, the results are still relevant in showing what considerations need to be made when distributing deep learning training.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {data parallelism,deep learning,large scale distributed deep learning},
  file = {C:\Users\Sebas\Zotero\storage\3YZYBSYD\Gebremeskel - 2018 - Analysis and comparison of distributed training te.pdf}
}

@mastersthesis{GebretsadkanKidane1413103,
  title = {Hudi on Hops : {{Incremental}} Processing and Fast Data Ingestion for Hops},
  author = {Gebretsadkan Kidane, Netsanet},
  year = {2019},
  series = {{{TRITA-EECS-EX}}},
  number = {2019:809},
  pages = {49},
  abstract = {In the era of big data, data is flooding from numerous data sources and many companies have been utilizing different types of tools to load and process data from various sources in a data lake. The major challenges where different companies are facing these days are how to update data into an existing dataset without having to read the entire dataset and overwriting it to accommodate the changes which have a negative impact on the performance. Besides this, finding a way to capture and track changed data in a big data lake as the system gets complex with large amounts of data to maintain and query is another challenge. Web platforms such as Hopsworks are also facing these problems without having an efficient mechanism to modify an existing processed results and pull out only changed data which could be useful to meet the processing needs of an organization. The challenge of accommodating row level changes in an efficient and effective manner is solved by integrating Hudi with Hops. This takes advantage of Hudi's upsert mechanism which uses Bloom indexing to significantly speed up the ability of looking up records across partitions. Hudi indexing maps a record key into the file id without scanning over every record in the dataset. In addition, each successful data ingestion is stored in Apache Hudi format stamped with commit timeline. This commit timeline is needed for the incremental processing mainly to pull updated rows since a specified instant of time and obtain change logs from a dataset. Hence, incremental pulls are realized through the monotonically increasing commit time line. Similarly, incremental updates are realized over a time column (key expression) that allows Hudi to update rows based on this time column. HoodieDeltaStreamer utility and DataSource API are used for the integration of Hudi with Hops and Feature store. As a result, this provided a fabulous way of ingesting and extracting row level updates where its performance can further be enhanced by the configurations of the shuffle parallelism and other spark parameter configurations since Hudi is a spark based library.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Hadoop,Hops,Hudi,Kafka,Spark,SQL,Upsert},
  file = {C:\Users\Sebas\Zotero\storage\IEQCQMFC\Gebretsadkan Kidane - 2019 - Hudi on hops  Incremental processing and fast dat.pdf}
}

@article{gortonDistributionDataDeployment2015,
  title = {Distribution, {{Data}}, {{Deployment}}: {{Software Architecture Convergence}} in {{Big Data Systems}}},
  shorttitle = {Distribution, {{Data}}, {{Deployment}}},
  author = {Gorton, Ian and Klein, John},
  year = {2015},
  month = may,
  journal = {IEEE Software},
  volume = {32},
  number = {3},
  pages = {78--85},
  issn = {1937-4194},
  doi = {10.1109/MS.2014.51},
  urldate = {2025-01-22},
  abstract = {Big data applications are pushing the limits of software engineering on multiple horizons. Successful solutions span the design of the data, distribution, and deployment architectures. The body of software architecture knowledge must evolve to capture this advanced design knowledge for big data systems. This article is a first step on this path. Our research is proceeding in two complementary directions. First, we're expanding our collection of architecture tactics and encoding them in an environment that supports navigation between quality attributes and tactics, making crosscutting concerns for design choices explicit. Second, we're linking tactics to design solutions based on specific big data technologies, enabling architects to rapidly relate a particular technology's capabilities to a specific set of tactics.},
  keywords = {big data,Big data,Computer architecture,data management,Data management,Data models,Distributed databases,distributed systems,NoSQL,software architecture,Software architecture,software engineering,Software engineering},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\8ISGLSA7\\Gorton and Klein - 2015 - Distribution, Data, Deployment Software Architect.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\M2TRYRBM\\6774768.html}
}

@misc{graziotinTheoryAffectSoftware2016,
  title = {Towards a {{Theory}} of {{Affect}} and {{Software Developers}}' {{Performance}}},
  author = {Graziotin, Daniel},
  year = {2016},
  month = jan,
  number = {arXiv:1601.05330},
  eprint = {1601.05330},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1601.05330},
  urldate = {2025-01-22},
  abstract = {For more than thirty years, it has been claimed that a way to improve software developers' productivity and software quality is to focus on people. The underlying assumption seems to be that "happy and satisfied software developers perform better". More specifically, affects-emotions and moods-have an impact on cognitive activities and the working performance of individuals. Development tasks are undertaken heavily through cognitive processes, yet software engineering research (SE) lacks theory on affects and their impact on software development activities. This PhD dissertation supports the advocates of studying the human and social aspects of SE and the psychology of programming. This dissertation aims to theorize on the link between affects and software development performance. A mixed method approach was employed, which comprises studies of the literature in psychology and SE, quantitative experiments, and a qualitative study, for constructing a multifaceted theory of the link between affects and programming performance. The theory explicates the linkage between affects and analytical problem-solving performance of developers, their software development task productivity, and the process behind the linkage. The results are novel in the domains of SE and psychology, and they fill an important lack that had been raised by both previous research and by practitioners. The implications of this PhD lie in setting out the basic building blocks for researching and understanding the affect of software developers, and how it is related to software development performance. Overall, the evidence hints that happy software developers perform better in analytic problem solving, are more productive while developing software, are prone to share their feelings in order to let researchers and managers understand them, and are susceptible to interventions for enhancing their affects on the job.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Software Engineering},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\SH8WQB8U\\Graziotin - 2016 - Towards a Theory of Affect and Software Developers.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\8C9YJAUF\\1601.html}
}

@misc{GreenSoftwareFoundation,
  title = {Green {{Software Foundation}}},
  urldate = {2025-01-22},
  abstract = {The Green Software Foundation is a non-profit with the mission to create a trusted ecosystem of people, standards, tooling and best practices for building green software},
  howpublished = {https://greensoftware.foundation/},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\DNVSHSUZ\greensoftware.foundation.html}
}

@misc{GRPC,
  title = {{{gRPC}}},
  journal = {gRPC},
  urldate = {2025-01-22},
  abstract = {A high-performance, open source universal RPC framework},
  howpublished = {https://grpc.io/},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\9YWFREJB\grpc.io.html}
}

@misc{GuidelinesEncodingBibliographic,
  title = {Guidelines for {{Encoding Bibliographic Citation Information}} in {{Dublin Core}}\&\#8482; {{Metadata}}},
  journal = {DCMI},
  urldate = {2025-01-22},
  abstract = {This document provides guidelines for capturing bibliographic citation information within a Dublin Core description. It focuses on bibliographic citations for journal articles, but it also considers other genre. It deals primarily with bibliographic citations for a resource within its own metadata, but some guidelines for describing references to other resources are also indicated. Some other issues that arise when describing a bibliographic resource using Dublin Core metadata are also discussed.},
  howpublished = {https://www.dublincore.org/specifications/dublin-core/dc-citation-guidelines/},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\KIE7LUBB\dc-citation-guidelines.html}
}

@mastersthesis{Gustafson1707793,
  title = {Improving Availability of Stateful Serverless Functions in Apache Flink},
  author = {Gustafson, Christopher},
  year = {2022},
  series = {{{TRITA-EECS-EX}}},
  number = {2022:531},
  pages = {51},
  abstract = {Serverless computing and Function-as-a-Service are rising in popularity due to their ease of use, provided scalability and cost-efficient billing model. One such platform is Apache Flink Stateful Functions. It allows application developers to run serverless functions with state that is persisted using the underlying stream processing engine Apache Flink. Stateful Functions use an embedded RocksDB state backend, where state is stored locally at each worker. One downside of this architecture is that state is lost if a worker fails. To recover, a recent snapshot of the state is fetched from a persistent file system. This can be a costly operation if the size of the state is large. In this thesis, we designed and developed a new decoupled state backend for Apache Flink Stateful Functions, with the goal of increasing availability while measuring potential performance trade-offs. It extends an existing decoupled state backend for Flink, FlinkNDB, to support the operations of Stateful Functions. FlinkNDB stores state in a separate highly available database, RonDB, instead of locally at the worker nodes. This allows for fast recovery as large state does not have to be transferred between nodes. Two new recovery methods were developed, eager and lazy recovery. The results show that lazy recovery can decrease recovery time by up to 60\% compared to RocksDB when the state is large. Eager recovery did not provide any recovery time improvements. The measured performance was similar between RocksDB and FlinkNDB. Checkpointing times in FlinkNDB were however longer, which cause short periodic performance degradation. The evaluation of FlinkNDB suggests that decoupled state can be used to improve availability, but that there might be performance deficits included. The proposed solution could thus be a viable option for applications with high requirements of availability and lower performance requirements.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Apache Flink StateFun,ApacheFlink StateFun,Availability,Function-as-a-Service,RocksDB,RonDB,Stateful Serverless Functions,Tillganglighet,Tillstandsbaserade ServerlosaFunktioner},
  file = {C:\Users\Sebas\Zotero\storage\RT5MXWNH\Gustafson - 2022 - Improving availability of stateful serverless func.pdf}
}

@article{Hagos1601997,
  title = {{{ExtremeEarth}} Meets Satellite Data from Space},
  author = {Hagos, Desta Haileselassie and Kakantousis, Theofilos and Vlassov, Vladimir and Sheikholeslami, Sina and Wang, Tianze and Dowling, Jim and Paris, Claudia and Marinelli, Daniele and Weikmann, Giulio and Bruzzone, Lorenzo and Khaleghian, Salman and Kraemer, Thomas and Eltoft, Torbjorn and Marinoni, Andrea and Pantazi, Despina-Athanasia and Stamoulis, George and Bilidas, Dimitris and Papadakis, George and Mandilaras, George and Koubarakis, Manolis and Troumpoukis, Antonis and Konstantopoulos, Stasinos and Muerth, Markus and Appel, Florian and Fleming, Andrew and Cziferszky, Andreas},
  year = {2021},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume = {14},
  pages = {9038--9063},
  publisher = {VISTA Remote Sensing Geosci GmbH, D-80333 Munich, Germany.},
  doi = {10.1109/JSTARS.2021.3107982},
  abstract = {Bringing together a number of cutting-edge technologies that range from storing extremely large volumes of data all the way to developing scalable machine learning and deep learning algorithms in a distributed manner and having them operate over the same infrastructure poses unprecedented challenges. One of these challenges is the integration of European Space Agency (ESA)'s Thematic Exploitation Platforms (TEPs) and data information access service platforms with a data platform, namely Hopsworks, which enables scalable data processing, machine learning, and deep learning on Copernicus data, and development of very large training datasets for deep learning architectures targeting the classification of Sentinel images. In this article, we present the software architecture of ExtremeEarth that aims at the development of scalable deep learning and geospatial analytics techniques for processing and analyzing petabytes of Copernicus data. The ExtremeEarth software infrastructure seamlessly integrates existing and novel software platforms and tools for storing, accessing, processing, analyzing, and visualizing large amounts of Copernicus data. New techniques in the areas of remote sensing and artificial intelligence with an emphasis on deep learning are developed. These techniques and corresponding software presented in this article are to be integrated with and used in two ESA TEPs, namely Polar and Food Security TEPs. Furthermore, we present the integration of Hopsworks with the Polar and Food Security use cases and the flow of events for the products offered through the TEPs.},
  keywords = {Artificial intelligence (AI),Computer architecture,copernicus,Data models,Deep learning,earth observation (EO),extremeearth,food security,Geospatial analysis,hopsworks,linked geospatial data,Monitoring,polar regions,remote sensing,Satellites,Sea ice},
  file = {C:\Users\Sebas\Zotero\storage\AXPEKCDV\Hagos et al. - 2021 - ExtremeEarth meets satellite data from space.pdf}
}

@article{Hagos1656656,
  title = {Scalable Artificial Intelligence for Earth Observation Data Using Hopsworks},
  author = {Hagos, Desta Haileselassie and Kakantousis, Theofilos and Sheikholeslami, Sina and Wang, Tianze and Vlassov, Vladimir and Payberah, Amir Hossein and Meister, Moritz and Andersson, Robin and Dowling, Jim},
  year = {2022},
  journal = {Remote Sensing},
  volume = {14},
  number = {1889},
  publisher = {MDPI AG},
  doi = {10.3390/rs14081889},
  abstract = {This paper introduces the Hopsworks platform to the entire Earth Observation (EO) data community and the Copernicus programme. Hopsworks is a scalable data-intensive open-source Artificial Intelligence (AI) platform that was jointly developed by Logical Clocks and the KTH Royal Institute of Technology for building end-to-end Machine Learning (ML)/Deep Learning (DL) pipelines for EO data. It provides the full stack of services needed to manage the entire life cycle of data in ML. In particular, Hopsworks supports the development of horizontally scalable DL applications in notebooks and the operation of workflows to support those applications, including parallel data processing, model training, and model deployment at scale. To the best of our knowledge, this is the first work that demonstrates the services and features of the Hopsworks platform, which provide users with the means to build scalable end-to-end ML/DL pipelines for EO data, as well as support for the discovery and search for EO metadata. This paper serves as a demonstration and walkthrough of the stages of building a production-level model that includes data ingestion, data preparation, feature extraction, model training, model serving, and monitoring. To this end, we provide a practical example that demonstrates the aforementioned stages with real-world EO data and includes source code that implements the functionality of the platform. We also perform an experimental evaluation of two frameworks built on top of Hopsworks, namely Maggy and AutoAblation. We show that using Maggy for hyperparameter tuning results in roughly half the wall-clock time required to execute the same number of hyperparameter tuning trials using Spark while providing linear scalability as more workers are added. Furthermore, we demonstrate how AutoAblation facilitates the definition of ablation studies and enables the asynchronous parallel execution of ablation trials.},
  keywords = {ablation studies,artificial intelligence,big data,Copernicus,deep learning,Earth Observation,ExtremeEarth,Hopsworks,machine learning,Maggy,model serving},
  file = {C:\Users\Sebas\Zotero\storage\QNRABXL5\Hagos et al. - 2022 - Scalable artificial intelligence for earth observa.pdf}
}

@mastersthesis{Hasan1044829,
  title = {Quota Based Access-Control for {{Hops}} : {{Improving}} Cluster Utilization with {{Hops-YARN}}},
  author = {Hasan, Muhammed Rizvi},
  year = {2016},
  series = {{{TRITA-ICT-EX}}},
  number = {2016:109},
  pages = {71},
  abstract = {YARN is the resource management framework for Hadoop, and is, in many senses, the modern operating system for the data center. YARN clusters are running at organizations such as Yahoo!, Spotify, and Twitter with clusters of up to 3500 nodes being reported in the literature. To harness the power of so many nodes and manage them efficiently YARN is required to fulfill the requirements like scalability, serviceability, multitenancy, reliability, high cluster utilization, secure and auditable operation. Currently, YARN supports three different schedulers for prioritizing the allocation of resources (CPU, memory) to applications. Existing schedulers have a broken incentive model for popular frameworks like Apache Spark and Apache Flink where applications have gang-scheduling semantics, that is, they need all nodes to be available before they can start work. Users are incentivized to launch and hog their resources, as there may be a substantial delay (in Spotify, up to 1 hour) in getting 100 or more nodes allocated to your application. Users are not penalized for hogging resources. Capacity scheduler is one of the schedulers that has been used as a default scheduler in YARN which is quite good in sharing resources among tenants with a degree of guaranteed resource availability. Still there is room for improvements. In this thesis, we propose the design and implementation of a new system called Quota-based access control system that will work as a layer over capacity scheduler for Hops-YARN, a project developed on Apache YARN. Quota-based access control system involves allocating a quota of resources to projects. A project consists of a number of users who manage a number of data sets and is taken from a new frontend for Hadoop called HopsWorks, (www.hops.io). Project members can spend part of their quota to launch and run applications. In contrast to existing schedulers, our control system will incentivize users for not launching unnecessary applications or hog resources. In this work we also have analyzed the operational model of the scheduler including Quota-based access control system with different application scheduling scenarios. We also have investigated the failure scenarios which includes network partition and failure of different components of YARN and analyzed the consequence of the failure on the scheduling operation. Finally, we have proposed some future improvements for this scheduling system.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT)},
  file = {C:\Users\Sebas\Zotero\storage\THDYD5DU\Hasan - 2016 - Quota based access-control for Hops  Improving cl.pdf}
}

@phdthesis{hellmanStudyComparisonData2023,
  title = {Study and {{Comparison}} of {{Data Lakehouse Systems}}},
  author = {Hellman, Fredrik},
  year = {2023},
  address = {{\AA}bo},
  urldate = {2024-02-16},
  abstract = {This thesis presents a comprehensive study and comparative analysis of three distinct data lakehouse systems: Delta Lake, Apache Iceberg, and Apache Hudi. Data lakehouse systems are an emergent concept that combines the capabilities of data warehouses and data lakes to provide a unified platform for large-scale data management and analysis. Three experimental scenarios were conducted focusing on data ingestion, query performance, and scaling, each assessing a different aspect of the system's capabilities. The results show that each data lakehouse system possesses unique strengths and weaknesses: Apache Iceberg demonstrated the best data ingestion speed, Delta Lake exhibited consistent performance across all testing scenarios, while Apache Hudi excelled with smaller datasets. Furthermore, the study also considered the ease of implementation and use for each system. Apache Iceberg emerged as the most user-friendly, with comprehensive documentation. Delta Lake provided a slightly steeper learning curve, while Apache Hudi was the most challenging to implement. This study underscores the promising potential of data lakehouses as alternatives to traditional database architectures. However, further research is necessary to solidify the positioning of data lakehouses as the new generation of database architectures.},
  langid = {english},
  school = {Vaasa {\AA}bo Akademi University},
  file = {C:\Users\Sebas\Zotero\storage\SKPMEQ98\hellman_fredrik.pdf}
}

@misc{HopsworksRealtimeAI,
  title = {Hopsworks - {{The Real-time AI Lakehouse}}},
  urldate = {2025-01-22},
  abstract = {Hopsworks is the flexible and modular AI Lakehouse with a feature store that provides seamless integration for existing pipelines, superior performance for any SLA, and increased productivity for data and AI teams.},
  howpublished = {https://www.hopsworks.ai/},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\GRWBU6KM\www.hopsworks.ai.html}
}

@misc{HowObjectVs,
  title = {How {{Object}} vs {{Block}} vs {{File Storage}} Differ},
  journal = {Google Cloud},
  urldate = {2025-01-22},
  abstract = {Learn the differences between object, file, and block storage including what data they store and how they store it.},
  howpublished = {https://cloud.google.com/discover/object-vs-block-vs-file-storage},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\6BPCKSRK\object-vs-block-vs-file-storage.html}
}

@misc{IcebergExamples2024,
  title = {Iceberg {{Architecture Examples}}},
  shorttitle = {Iceberg {{Architecture Examples}}},
  author = {Team, Upsolver},
  year = {2024},
  month = feb,
  journal = {Upsolver},
  urldate = {2025-01-25},
  abstract = {Exploring Iceberg? Don't miss our Iceberg Table Optimization Techniques e-learning module, covering essential optimization techniques crucial for maintaining a healthy Iceberg architecture according to best practices.},
  howpublished = {https://www.upsolver.com/blog/iceberg-architecture-examples},
  langid = {american}
}

@misc{IcebergNewHadoop,
  title = {Apache {{Iceberg}}: {{The Hadoop}} of the {{Modern Data Stack}}?},
  shorttitle = {Apache {{Iceberg}}},
  author = {Dani},
  year = {2024},
  month = dec,
  journal = {Medium},
  urldate = {2025-01-25},
  abstract = {The bigger they are the harder they fall.},
  howpublished = {https://blog.det.life/apache-iceberg-the-hadoop-of-the-modern-data-stack-c83f63a4ebb9},
  langid = {english}
}

@misc{incStorageWarsDatabases2023,
  title = {Storage {{Wars}}: {{Databases Becoming Just Query Engines}} for {{Big Object Stores}}?},
  shorttitle = {Storage {{Wars}}},
  author = {Inc, Data Saint Consulting},
  year = {2023},
  month = may,
  journal = {Medium},
  urldate = {2024-02-21},
  abstract = {Object storage, also known as object-based storage, is a computer data storage architecture designed to handle large amounts of unstructured data. Unlike other architectures, it designates data as{\dots}},
  langid = {english},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\3ED27W23\\Inc - 2023 - Storage Wars Databases Becoming Just Query Engine.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\N3USAQXF\\storage-wars-databases-becoming-just-query-engines-for-big-object-stores-179c3655599c.html}
}

@inproceedings{Ismail1158011,
  title = {Hopsworks : {{Improving}} User Experience and Development on Hadoop with Scalable, Strongly Consistent Metadata},
  shorttitle = {Hopsworks},
  booktitle = {2017 {{IEEE}} 37th {{International Conference}} on {{Distributed Computing Systems}} ({{ICDCS}})},
  author = {Ismail, Mahmoud and Gebremeskel, Ermias and Kakantousis, Theofilos and Berthou, Gautier and Dowling, Jim},
  year = {2017},
  month = jun,
  series = {{{IEEE}} International Conference on Distributed Computing Systems},
  pages = {2525--2528},
  publisher = {{IEEE COMPUTER SOC / KTH, Software and Computer systems, SCS and RISE SICS, Sweden}},
  address = {Atlanta, GA, USA},
  doi = {10.1109/ICDCS.2017.41},
  abstract = {Hadoop is a popular system for storing, managing, and processing large volumes of data, but it has bare-bones internal support for metadata, as metadata is a bottleneck and less means more scalability. The result is a scalable platform with rudimentary access control that is neither user-nor developer friendly. Also, metadata services that are built on Hadoop, such as SQL-on-Hadoop, access control, data provenance, and data governance are necessarily implemented as eventually consistent services, resulting in increased development effort and more brittle software. In this paper, we present a new project-based multi-tenancy model for Hadoop, built on a new distribution of Hadoop that provides a distributed database backend for the Hadoop Distributed Filesystem's (HDFS) metadata layer. We extend Hadoop's metadata model to introduce projects, datasets, and project-users as new core concepts that enable a user-friendly, UI-driven Hadoop experience. As our metadata service is backed by a transactional database, developers can easily extend metadata by adding new tables and ensure the strong consistency of extended metadata using both transactions and foreign keys.},
  isbn = {978-1-5386-1791-5},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\QR9X7XH2\Ismail et al. - 2017 - Hopsworks Improving User Experience and Developme.pdf}
}

@phdthesis{Ismail1500242,
  title = {Distributed File System Metadata and Its Applications},
  author = {Ismail, Mahmoud},
  year = {2020},
  series = {{{TRITA-EECS-AVL}}},
  number = {2020:65},
  abstract = {Distributed hierarchical file systems typically decouple the storage and serving of the file metadata from the file contents (file system blocks) to enable the file system to scale to store more data and support higher throughput. We designed HopsFS to take the scalability of the file system one step further by also decoupling the storage and serving of the file system metadata. HopsFS is an open-source, next- generation distribution of the Apache Hadoop Distributed File System (HDFS) that replaces the main scalability bottleneck in HDFS, the single-node in-memory metadata service, with a distributed metadata service built on a NewSQL database (NDB). HopsFS stores the file system's metadata fully normalized in NDB, then it uses locking primitives and application-defined locks to ensure strongly consistent metadata.In this thesis, we leverage the consistent distributed hierarchical file system meta- data provided by HopsFS to efficiently build new classes of applications that are tightly coupled with the file system as well as to improve the internal file system operations. First, we introduce hbr, a new block reporting protocol for HopsFS that removes a scalability bottleneck that prevented HopsFS from scaling to tens of thousands of servers. Second, we introduce HopsFS-CL, a highly available cloud-native distribution of HopsFS that deploys the file system across Availability Zones in the cloud while maintaining the same file system semantics. Third, we introduce HopsFS-S3, a highly available cloud-native distribution of HopsFS that uses object stores as a backend for the block storage layer in the cloud while again maintaining the same file system semantics. Fourth, we introduce ePipe, a databus that both creates a consistent change stream for HopsFS and eventually delivers the correctly ordered stream with low latency to downstream clients. That is, ePipe extends HopsFS with a change-data-capture (CDC) API that provides not only efficient file system notifications but also enables polyglot storage for file system metadata. Polyglot storage enables us to offload metadata queries to a more appropriate engine - we use Elasticsearch to provide a free-text search of the file system namespace to demonstrate this capability. Finally, we introduce Hopsworks, a scalable, project-based multi-tenant big data platform that provides support for collaborative development and operations for teams through extended metadata.},
  isbn = {978-91-7873-702-4},
  school = {KTH, Software and Computer systems, SCS / KTH, Software and Computer systems, SCS},
  file = {C:\Users\Sebas\Zotero\storage\HG69FLEB\Ismail - 2020 - Distributed file system metadata and its applicati.pdf}
}

@inproceedings{ismailDistributedHierarchicalFile2020,
  title = {Distributed {{Hierarchical File Systems}} Strike Back in the {{Cloud}}},
  booktitle = {40th {{IEEE International Conference}} on {{Distributed Computing Systems}}, {{November}} 29 - {{December}} 1, 2020, {{Singapore}}},
  author = {Ismail, Mahmoud and Niazi, Salman and Sundell, Mauritz and Ronstr{\"o}m, Mikael and Haridi, Seif and Dowling, Jim},
  year = {2020},
  urldate = {2024-02-16},
  abstract = {Cloud service providers have aligned on availability zones as an important unit of failure and replication for storage systems. An availability zone (AZ) has independent power, networking, and cool ...},
  langid = {english},
  keywords = {Cloud computing,File systems,Metadata,Optimization,Protocols,Semantics,Throughput},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\72P5DQ9U\\Ismail et al. - 2020 - Distributed Hierarchical File Systems strike back .pdf;C\:\\Users\\Sebas\\Zotero\\storage\\C6VXY74X\\9355756.html}
}

@inproceedings{ismailHopsFSS3ExtendingObject2020,
  title = {{{HopsFS-S3}}: {{Extending Object Stores}} with {{POSIX-like Semantics}} and More (Industry Track)},
  shorttitle = {{{HopsFS-S3}}},
  booktitle = {Proceedings of the 1st {{International Middleware Conference Industrial Track}}},
  author = {Ismail, Mahmoud and Niazi, Salman and Berthou, Gautier and Ronstr{\"o}m, Mikael and Haridi, Seif and Dowling, Jim},
  year = {2020},
  month = dec,
  pages = {23--30},
  publisher = {ACM},
  address = {Delft Netherlands},
  doi = {10.1145/3429357.3430521},
  urldate = {2024-02-14},
  isbn = {978-1-4503-8201-4},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\H7UL6E8M\Ismail et al. - 2020 - HopsFS-S3 Extending Object Stores with POSIX-like.pdf}
}

@inproceedings{ismailScalingHDFSMore2017,
  title = {Scaling {{HDFS}} to {{More Than}} 1 {{Million Operations Per Second}} with {{HopsFS}}},
  booktitle = {2017 17th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}} ({{CCGRID}})},
  author = {Ismail, Mahmoud and Niazi, Salman and Ronstrom, Mikael and Haridi, Seif and Dowling, Jim},
  year = {2017},
  month = may,
  pages = {683--688},
  publisher = {IEEE},
  address = {Madrid, Spain},
  doi = {10.1109/CCGRID.2017.117},
  urldate = {2024-02-16},
  abstract = {HopsFS is an open-source, next generation distribution of the Apache Hadoop Distributed File System (HDFS) that replaces the main scalability bottleneck in HDFS, single node in-memory metadata service, with a no-shared state distributed system built on a NewSQL database. By removing the metadata bottleneck in Apache HDFS, HopsFS enables significantly larger cluster sizes, more than an order of magnitude higher throughput, and significantly lower client latencies for large clusters.},
  isbn = {978-1-5090-6611-7},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\4IWXCNGL\Ismail et al. - 2017 - Scaling HDFS to More Than 1 Million Operations Per.pdf}
}

@article{ivanovicDataModelTheses2012,
  title = {A Data Model of Theses and Dissertations Compatible with {{CERIF}}, {{Dublin Core}} and {{EDT}}-{{MS}}},
  author = {Ivanovi{\'c}, Lidija and Ivanovi{\'c}, Dragan and Surla, Du{\v s}an},
  year = {2012},
  month = jan,
  journal = {Online Information Review},
  volume = {36},
  number = {4},
  pages = {548--567},
  publisher = {Emerald Group Publishing Limited},
  issn = {1468-4527},
  doi = {10.1108/14684521211254068},
  urldate = {2025-01-22},
  abstract = {Purpose -- The aim of this research is to define a data model of theses and dissertations that enables data exchange with CERIF-compatible CRIS systems and data exchange according to OAI-PMH protocol in different metadata formats (Dublin Core, EDT-MS, etc.). Design/methodology/approach -- Various systems that contain metadata about theses and dissertations are analyzed. There are different standards and protocols that enable the interoperability of those systems: CERIF standard, AOI-PMH protocol, etc. A physical data model that enables interoperability with almost all of those systems is created using the PowerDesigner CASE tool. Findings -- A set of metadata about theses and dissertations that contain all the metadata required by CERIF data model, Dublin Core format, EDT-MS format and all the metadata prescribed by the University of Novi Sad is defined. Defined metadata can be stored in the CERIF-compatible data model based on the MARC21 format. Practical implications -- CRIS-UNS is a CRIS which has been developed at the University of Novi Sad since 2008. The system is based on the proposed data model, which enables the system's interoperability with other CERIF-compatible CRIS systems. Also, the system based on the proposed model can become a member of NDLTD. Social implications -- A system based on the proposed model increases the availability of theses and dissertations, and thus encourages the development of the knowledge-based society. Originality/value -- A data model of theses and dissertations that enables interoperability with CERIF-compatible CRIS systems is proposed. A software system based on the proposed model could become a member of NDLTD and exchange metadata with institutional repositories. The proposed model increases the availability of theses and dissertations.},
  keywords = {CERIF,CRIS-UNS,Data management,ETD-MS,Institutional repositories,MARC21,NDLTD,OAI-PMH,Open systems},
  file = {C:\Users\Sebas\Zotero\storage\TSYBG937\Ivanović et al. - 2012 - A data model of theses and dissertations compatibl.pdf}
}

@article{jainAnalyzingComparingLakehouse2023,
  title = {Analyzing and {{Comparing Lakehouse Storage Systems}}},
  author = {Jain, Paras and Kraft, Peter and Power, Conor and Das, Tathagata and Stoica, Ion and Zaharia, Matei},
  year = {2023},
  abstract = {Lakehouse storage systems that implement ACID transactions and other management features over data lake storage, such as Delta Lake, Apache Hudi and Apache Iceberg, have rapidly grown in popularity, replacing traditional data lakes at many organizations. These open storage systems with rich management features promise to simplify management of large datasets, accelerate SQL workloads, and offer fast, direct file access for other workloads, such as machine learning. However, the research community has not explored the tradeoffs in designing lakehouse systems in detail. In this paper, we analyze the designs of the three most popular lakehouse storage systems---Delta Lake, Hudi and Iceberg---and compare their performance and features among varying axes based on these designs. We also release a simple benchmark, LHBench, that researchers can use to compare other designs. LHBench is available at https://github.com/lhbench/lhbench.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\GKDLB4NW\Jain et al. - 2023 - Analyzing and Comparing Lakehouse Storage Systems.pdf}
}

@mastersthesis{Kashyap1484512,
  title = {Project-Based Multi-Tenant Container Registry for Hopsworks},
  author = {Kashyap, Pradyumna Krishna},
  year = {2020},
  series = {{{TRITA-EECS-EX}}},
  number = {2020:725},
  pages = {63},
  abstract = {There has been a substantial growth in the usage of data in the past decade, cloud technologies and big data platforms have gained popularity as they help in processing such data on a large scale. Hopsworks is such a managed plat- form for scale out data science. It is an open-source platform for the develop- ment and operation of Machine Learning models, available on-premise and as a managed platform in the cloud. As most of these platforms provide data sci- ence environments to collate the required libraries to work with, Hopsworks provides users with Anaconda environments.Hopsworks provides multi-tenancy, ensuring a secure model to manage sen- sitive data in the shared platform. Most of the Hopsworks features are built around projects, each project includes an Anaconda environment that provides users with a number of libraries capable of processing data. Each project cre- ation triggers a creation of a base Anaconda environment and each added li- brary updates this environment. For an on-premise application, as data science teams are diverse and work towards building repeatable and scalable models, it becomes increasingly important to manage these environments in a central location locally.The purpose of the thesis is to provide a secure storage for these Anaconda en- vironments. As Hopsworks uses a Kubernetes cluster to serve models, these environments can be containerized and stored on a secure container registry on the Kubernetes Cluster. The provided solution also aims to extend the multi- tenancy feature of Hopsworks onto the hosted local storage. The implemen- tation comprises of two parts; First one, is to host a compatible open source container registry to store the container images on a local Kubernetes cluster with fault tolerance and by avoiding a single point of failure. Second one, is to leverage the multi-tenancy feature in Hopsworks by storing the images on the self sufficient secure registry with project level isolation.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Big Data,Cloud,Container,Data Science,Hopsworks,Kubernetes.,Multitenancy,On-premise,Registry},
  file = {C:\Users\Sebas\Zotero\storage\MCR7MBRF\Kashyap - 2020 - Project-based multi-tenant container registry for .pdf}
}

@article{kerstenEverythingYouAlways2018,
  title = {Everything You Always Wanted to Know about Compiled and Vectorized Queries but Were Afraid to Ask},
  author = {Kersten, Timo and Leis, Viktor and Kemper, Alfons and Neumann, Thomas and Pavlo, Andrew and Boncz, Peter},
  year = {2018},
  month = sep,
  journal = {Proceedings of the VLDB Endowment},
  volume = {11},
  number = {13},
  pages = {2209--2222},
  issn = {21508097},
  doi = {10.14778/3275366.3275370},
  urldate = {2024-02-26},
  abstract = {The query engines of most modern database systems are either based on vectorization or data-centric code generation. These two state-of-the-art query processing paradigms are fundamentally different in terms of system structure and query execution code. Both paradigms were used to build fast systems. However, until today it is not clear which paradigm yields faster query execution, as many implementation-specific choices obstruct a direct comparison of architectures. In this paper, we experimentally compare the two models by implementing both within the same test system. This allows us to use for both models the same query processing algorithms, the same data structures, and the same parallelization framework to ultimately create an apples-to-apples comparison. We find that both are efficient, but have different strengths and weaknesses. Vectorization is better at hiding cache miss latency, whereas data-centric compilation requires fewer CPU instructions, which benefits cacheresident workloads. Besides raw, single-threaded performance, we also investigate SIMD as well as multi-core parallelization and different hardware architectures. Finally, we analyze qualitative differences as a guide for system architects.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\PWU2885E\Kersten et al. - 2018 - Everything you always wanted to know about compile.pdf}
}

@mastersthesis{Khazanchi1801362,
  title = {Faster Reading with {{DuckDB}} and Arrow Flight on Hopsworks : {{Benchmark}} and Performance Evaluation of Offline Feature Stores},
  author = {Khazanchi, Ayushman},
  year = {2023},
  series = {{{TRITA-EECS-EX}}},
  number = {2023:661},
  pages = {69},
  abstract = {Over the last few years, Machine Learning has become a huge field with ``Big Tech'' companies sharing their experiences building machine learning infrastructure. Feature Stores, used as centralized data repositories for machine learning features, are seen as a central component to operational and scalable machine learning. With the growth in machine learning, there is, naturally, a tremendous growth in data used for training. Most of this data tends to sit in Parquet files in cloud object stores or data lakes and is used either directly from files or in-memory where it is used in exploratory data analysis and small batches of training. A majority of the data science involved in machine learning is done in Python, but the infrastructure surrounding it is not always directly compatible with Python. Often, query processing engines and feature stores end up having their own Domain Specific Language or require data scientists to write SQL code, thus leading to some level of `transpilation' overhead across the system. This overhead can not only introduce errors but can also add up to significant time and productivity cost down the line. In this thesis, we conduct a systems research on the performance of offline feature stores and identify ways that allow us to pull out data from feature stores in a fast and efficient way. We conduct a model evaluation based on benchmark tests that address common exploratory data analysis and training use cases. We find that in the Hopsworks feature store, with the use of state-of-the-art, storage-optimized, format-aware, and vector execution-based query processing engine as well as using Arrow protocol from start to finish, we are able to see significant improvements in both creating batch training data (feature value reads) and creating Point-In-Time Correct training data. For batch training data created in-memory, Hopsworks shows an average speedup of 27x over Databricks (5M and 10M scale factors), 18x over Vertex, and 8x over Sagemaker across all scale factors. For batch training data as parquet files, Hopsworks shows a speedup of 5x over Databricks (5M, 10M, and 20M scale factors), 13x over Vertex, and 6x over Sagemaker across all scale factors. For creating in-memory Point-In-Time Correct training data, Hopsworks shows an average speedup of 8x over Databricks, 6x over Vertex, and 3x over Sagemaker across all scale factors. Similary for PIT-Correct training data created as file, Hopsworks shows an average speedup of 9x over Databricks, 8x over Vertex, and 6x over Sagemaker across all scale factors. Through the analysis of these experimental results and the underlying infrastructure, we identify the reasons for this performance gap and examine the strengths and limitations of the design.},
  school = {KTH Royal Institute of Technology / KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Distributed Systems,Feature Store,Machine Learning,MLOps},
  file = {C:\Users\Sebas\Zotero\storage\ZMXCBWPZ\Khazanchi - 2023 - Faster reading with DuckDB and arrow flight on hop.pdf}
}

@article{krepsKafkaDistributedMessaging2011,
  title = {Kafka: A {{Distributed Messaging System}} for {{Log Processing}}},
  author = {Kreps, Jay and Narkhede, Neha and Rao, Jun},
  abstract = {Log processing has become a critical component of the data pipeline for consumer internet companies. We introduce Kafka, a distributed messaging system that we developed for collecting and delivering high volumes of log data with low latency. Our system incorporates ideas from existing log aggregators and messaging systems, and is suitable for both offline and online message consumption. We made quite a few unconventional yet practical design choices in Kafka to make our system efficient and scalable. Our experimental results show that Kafka has superior performance when compared to two popular messaging systems. We have been using Kafka in production for some time and it is processing hundreds of gigabytes of new data each day.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\VDP5CMZW\Kreps et al. - Kafka a Distributed Messaging System for Log Proc.pdf}
}

@inproceedings{kuiperTheseRowsAre2023,
  title = {These {{Rows Are Made}} for {{Sorting}} and {{That}}'s {{Just What We}}'ll {{Do}}},
  booktitle = {2023 {{IEEE}} 39th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Kuiper, Laurens and M{\"u}hleisen, Hannes},
  year = {2023},
  month = apr,
  pages = {2050--2062},
  publisher = {IEEE},
  address = {Anaheim, CA, USA},
  doi = {10.1109/ICDE55515.2023.00159},
  urldate = {2024-02-26},
  abstract = {Sorting is one of the most well-studied problems in computer science and a vital operation for relational database systems. Despite this, little research has been published on implementing an efficient relational sorting operator. In this work, we aim to fill this gap. We use micro-benchmarks to explore how to sort relational data efficiently for analytical database systems, taking into account different query execution engines as well as row and columnar data formats. We show that, regardless of architectural differences between query engines, sorting rows is almost always more efficient than sorting columnar data, even if this requires converting the data from columns to rows and back. Sorting rows efficiently is challenging for systems with an interpreted execution engine, as interpreting rows at runtime causes overhead. We show that this overhead can be overcome with several existing techniques. Based on our findings, we implement a highly optimized row-based sorting approach in the DuckDB open-source in-process analytical database management system, which has a vectorized interpreted query engine. We compare DuckDB with four analytical database systems and find that DuckDB's sort implementation outperforms query engines that sort using a columnar data format, and matches or outperforms compiled query engines that sort using a row data format.},
  isbn = {9798350322279},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\QTFF5JY4\Kuiper and Mühleisen - 2023 - These Rows Are Made for Sorting and That’s Just Wh.pdf}
}

@article{lakehouse2021,
  title = {Lakehouse: {{A New Generation}} of {{Open Platforms}} That {{Unify Data Warehousing}} and {{Advanced Analytics}}},
  author = {Armbrust, Michael and Ghodsi, Ali and Xin, Reynold and Zaharia, Matei},
  year = {2021},
  abstract = {This paper argues that the data warehouse architecture as we know it today will wither in the coming years and be replaced by a new architectural pattern, the Lakehouse, which will (i) be based on open direct-access data formats, such as Apache Parquet, (ii) have firstclass support for machine learning and data science, and (iii) offer state-of-the-art performance. Lakehouses can help address several major challenges with data warehouses, including data staleness, reliability, total cost of ownership, data lock-in, and limited use-case support. We discuss how the industry is already moving toward Lakehouses and how this shift may affect work in data management. We also report results from a Lakehouse system using Parquet that is competitive with popular cloud data warehouses on TPC-DS.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\V7H69UZK\Armbrust et al. - 2021 - Lakehouse A New Generation of Open Platforms that.pdf}
}

@phdthesis{manfredigiovanniReducingReadWrite2024,
  title = {Reducing Read and Write Latency in a {{Delta Lake-backed}} Offline Feature Store},
  author = {Manfredi, Giovanni},
  year = {2024},
  month = nov,
  address = {Stockholm},
  abstract = {The need to build Machine Learning (ML) models based on large amounts of data brought new challenges to data management systems. Feature stores have emerged as a centralized data platform enabling feature reuse while ensuring consistency between feature engineer ing, model training, and inference. Recent publications demonstrate that the Hopsworks feature store outperforms existing cloud-based alternatives in training and online infer ence query workloads. In its offline feature store, the Hopsworks feature store stores batch or historical data, collecting it into feature groups organized in Apache Hudi tables and stored on HopsFS, Hopsworks HDFS distribution. However, even in this system, the latency to perform a write operation is at least one or more minutes, even for small quantities of data (1 GB or less). The hypothesis of this work is that the limitation is caused by Spark, which the system uses to write data on Apache Hudi tables. A promising approach to avoid using Spark appears to be adopting Delta Lake instead of Apache Hudi and access data using a Rust library called delta-rs. This thesis investigates the possibility of reducing the read and write latency in the offline feature store by expanding the delta rs library to support HDFS and HopsFS and comparatively evaluating the performance of the legacy and newly implemented system. Two major iterations of storage support in delta-rs for HopsFS were developed to meet the strict production-ready requirements. The system was then evaluated by performing and measuring read and write operations increasing the number of CPU cores up to eight. Results confirmed the superior perfor mance of the delta-rs library over the Spark system in all write operations with a latency reduction from ten up to forty times. Delta-rs also surpassed the Spark alternative in read operations with a latency reduction of forty-seven percent, up to forty times. These findings encourage future research investigating Spark alternative when optimizing performance in small-scale (1 GB - 100 GB) data management systems. The system developed will find application in the Hopsworks feature store production environment},
  langid = {english},
  school = {KTH Royal Institute of Technology / KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  file = {C:\Users\Sebas\Zotero\storage\3DPHEREI\Manfredi, Giovanni - 2024 - Reducing read and write latency in a Delta Lake-ba.pdf}
}

@misc{MaximizingDeveloperEffectiveness,
  title = {Maximizing {{Developer Effectiveness}}},
  journal = {martinfowler.com},
  urldate = {2025-01-22},
  abstract = {To be successful at digital transformation you need to optimize the key feedback loops in your software engineering organization},
  howpublished = {https://martinfowler.com/articles/developer-effectiveness.html},
  file = {C:\Users\Sebas\Zotero\storage\V7G697SX\developer-effectiveness.html}
}

@misc{mazumdarDataLakehouseData2023,
  title = {The {{Data Lakehouse}}: {{Data Warehousing}} and {{More}}},
  shorttitle = {The {{Data Lakehouse}}},
  author = {Mazumdar, Dipankar and Hughes, Jason and Onofre, J. B.},
  year = {2023},
  month = oct,
  number = {arXiv:2310.08697},
  eprint = {2310.08697},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-16},
  abstract = {Relational Database Management Systems designed for Online Analytical Processing (RDBMS-OLAP) have been foundational to democratizing data and enabling analytical use cases such as business intelligence and reporting for many years. However, RDBMS-OLAP systems present some well-known challenges. They are primarily optimized only for relational workloads, lead to proliferation of data copies which can become unmanageable, and since the data is stored in proprietary formats, it can lead to vendor lock-in, restricting access to engines, tools, and capabilities beyond what the vendor offers. As the demand for data-driven decision making surges, the need for a more robust data architecture to address these challenges becomes ever more critical. Cloud data lakes have addressed some of the shortcomings of RDBMS-OLAP systems, but they present their own set of challenges. More recently, organizations have often followed a two-tier architectural approach to take advantage of both these platforms, leveraging both cloud data lakes and RDBMS-OLAP systems. However, this approach brings additional challenges, complexities, and overhead. This paper discusses how a data lakehouse, a new architectural approach, achieves the same benefits of an RDBMS-OLAP and cloud data lake combined, while also providing additional advantages. We take today's data warehousing and break it down into implementation independent components, capabilities, and practices. We then take these aspects and show how a lakehouse architecture satisfies them. Then, we go a step further and discuss what additional capabilities and benefits a lakehouse architecture provides over an RDBMS-OLAP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\45VLCJGC\\Mazumdar et al. - 2023 - The Data Lakehouse Data Warehousing and More.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\QWFTHUZG\\2310.html}
}

@misc{MeetMichelangeloUbers2017,
  title = {Meet {{Michelangelo}}: {{Uber}}'s {{Machine Learning Platform}}},
  shorttitle = {Meet {{Michelangelo}}},
  year = {2017},
  month = sep,
  journal = {Uber Blog},
  urldate = {2025-01-22},
  abstract = {Uber Engineering introduces Michelangelo, our machine learning-as-a-service system that enables teams to easily build, deploy, and operate ML solutions at scale.},
  howpublished = {https://www.uber.com/en-IN/blog/michelangelo-machine-learning-platform/},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\F8U2HZSZ\michelangelo-machine-learning-platform.html}
}

@phdthesis{More862135,
  title = {{{HopsWorks}} : {{A}} Project-Based Access Control Model for {{Hadoop}}},
  author = {Mor{\'e}, Andr{\'e} and Gebremeskel, Ermias},
  year = {2015},
  series = {{{TRITA-ICT-EX}}},
  number = {2015:70},
  abstract = {The growth in the global data gathering capacity is producing a vast amount of data which is getting vaster at an increasingly faster rate. This data properly analyzed can represent great opportunity for businesses, but processing it is a resource-intensive task. Sharing can increase efficiency due to reusability but there are legal and ethical questions that arise when data is shared. The purpose of this thesis is to gain an in depth understanding of the different access control methods that can be used to facilitate sharing, and choose one to implement on a platform that lets user analyze, share, and collaborate on, datasets. The resulting platform uses a project based access control on the API level and a fine-grained role based access control on the file system to give full control over the shared data to the data owner.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT) and KTH, School of Information and Communication Technology (ICT)},
  keywords = {Big Data,DataSets,Distributed Computing,Hadoop,Hops,HopsWorks},
  file = {C:\Users\Sebas\Zotero\storage\97KBTXXE\Moré and Gebremeskel - 2015 - HopsWorks  A project-based access control model f.pdf}
}

@inproceedings{nagpalPythonDataAnalytics2019,
  title = {Python for {{Data Analytics}}, {{Scientific}} and {{Technical Applications}}},
  booktitle = {2019 {{Amity International Conference}} on {{Artificial Intelligence}} ({{AICAI}})},
  author = {Nagpal, Abhinav and Gabrani, Goldie},
  year = {2019},
  month = feb,
  pages = {140--145},
  doi = {10.1109/AICAI.2019.8701341},
  urldate = {2025-01-22},
  abstract = {Since the invention of computers or machines, their capability to perform various tasks has experienced an exponential growth. In the current times, data science and analytics, a branch of computer science, has revived due to the major increase in computer power, presence of huge amounts of data, and better understanding in techniques in the area of Data Analytics, Artificial Intelligence, Machine Learning, Deep Learning etc. Hence, they have become an essential part of the technology industry, and are being used to solve many challenging problems. In the search for a good programming language on which many data science applications can be developed, python has emerged as a complete programming solution. Due to the low learning curve, and flexibility of Python, it has become one of the fastest growing languages. Python's ever-evolving libraries make it a good choice for Data analytics. The paper talks about the features and characteristics of Python programming language and later discusses reasons behind python being credited as one of the fastest growing programming language and why it is at the forefront of data science applications, research and development.},
  keywords = {Artificial intelligence,Artificial Intelligence,Computer Languages,Data analysis,Data Analytics,Deep Learning,Frameworks,Libraries,Machine Learning,Matlab,Natural Language Processing,Python,Scientific Computations,Tools},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\USWRVSBY\\Nagpal and Gabrani - 2019 - Python for Data Analytics, Scientific and Technica.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\3CCNNM48\\8701341.html}
}

@misc{NeoclideCocnvimNodejs,
  title = {Neoclide/Coc.Nvim: {{Nodejs}} Extension Host for Vim \& Neovim, Load Extensions like {{VSCode}} and Host Language Servers.},
  urldate = {2025-01-22},
  howpublished = {https://github.com/neoclide/coc.nvim},
  file = {C:\Users\Sebas\Zotero\storage\48GASDXU\coc.html}
}

@misc{NewAutomatedWay,
  title = {A {{New Automated Way}} to {{Measure Polyethylene Wear}} in {{THA Using}} a {{High Resolution CT Scanner}}: {{Method}} and {{Analysis}} - {{Maguire}} - 2014 - {{The Scientific World Journal}} - {{Wiley Online Library}}},
  urldate = {2025-01-22},
  howpublished = {https://onlinelibrary.wiley.com/doi/10.1155/2014/528407},
  file = {C:\Users\Sebas\Zotero\storage\ZANNT7CC\528407.html}
}

@article{niAutomaticExtractionAuthor,
  title = {Automatic {{Extraction}} of Author Self Contributed Metadata for Electonic Theses and Disserations},
  author = {Ni, Mao},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\WDMGZC7Q\Ni - AUTOMATIC EXTRACTION OF AUTHOR SELF CONTRIBUTED ME.pdf}
}

@article{niaziHopsFSScalingHierarchical2017,
  title = {{{HopsFS}}: {{Scaling Hierarchical File System Metadata Using NewSQL Databases}}},
  author = {Niazi, Salman and Ismail, Mahmoud and Haridi, Seif and Dowling, Jim and Grohsschmiedt, Steffen and Ronstr{\"o}m, Mikael},
  abstract = {Recent improvements in both the performance and scalability of shared-nothing, transactional, in-memory NewSQL databases have reopened the research question of whether distributed metadata for hierarchical file systems can be managed using commodity databases. In this paper, we introduce HopsFS, a next generation distribution of the Hadoop Distributed File System (HDFS) that replaces HDFS' single node in-memory metadata service, with a distributed metadata service built on a NewSQL database. By removing the metadata bottleneck, HopsFS enables an order of magnitude larger and higher throughput clusters compared to HDFS. Metadata capacity has been increased to at least 37 times HDFS' capacity, and in experiments based on a workload trace from Spotify, we show that HopsFS supports 16 to 37 times the throughput of Apache HDFS. HopsFS also has lower latency for many concurrent clients, and no downtime during failover. Finally, as metadata is now stored in a commodity database, it can be safely extended and easily exported to external systems for online analysis and free-text search.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\IGGVPFPW\Niazi et al. - HopsFS Scaling Hierarchical File System Metadata .pdf}
}

@inproceedings{niaziSizeMattersImproving2018,
  title = {Size {{Matters}}: {{Improving}} the {{Performance}} of {{Small Files}} in {{Hadoop}}},
  shorttitle = {Size {{Matters}}},
  booktitle = {Proceedings of the 19th {{International Middleware Conference}}},
  author = {Niazi, Salman and Ronstr{\"o}m, Mikael and Haridi, Seif and Dowling, Jim},
  year = {2018},
  month = nov,
  pages = {26--39},
  publisher = {ACM},
  address = {Rennes France},
  doi = {10.1145/3274808.3274811},
  urldate = {2024-02-14},
  isbn = {978-1-4503-5702-9},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\CXG9APSL\Niazi et al. - 2018 - Size Matters Improving the Performance of Small F.pdf}
}

@misc{Object_storeRust,
  title = {Object\_store - {{Rust}}},
  urldate = {2025-01-22},
  howpublished = {https://docs.rs/object\_store/latest/object\_store/},
  file = {C:\Users\Sebas\Zotero\storage\VF6DN3IZ\object_store.html}
}

@misc{ObjectVsFile2021,
  title = {Object vs. {{File}} vs. {{Block Storage}}: {{What}}'s the {{Difference}}? {\textbar} {{IBM}}},
  shorttitle = {Object vs. {{File}} vs. {{Block Storage}}},
  year = {2024},
  month = aug,
  urldate = {2025-01-22},
  abstract = {A helpful look into file, object and block storage, their key differences and what type best meets your needs.},
  howpublished = {https://www.ibm.com/think/topics/object-vs-file-vs-block-storage},
  langid = {english}
}

@misc{OngoingEvolutionTableFormat,
  title = {The ({{Ongoing}}) {{Evolution Of Table Formats}}},
  year = {2024},
  month = sep,
  urldate = {2025-01-25},
  abstract = {Today's table formats are so much more than collections of rows and columns. Learn how table formats have evolved and why data observability is as crucial as ever in the modern data stack.},
  howpublished = {https://www.montecarlodata.com/blog-the-evolution-of-table-formats/},
  langid = {american}
}

@inproceedings{ormenisanTimeTravelProvenance2020,
  title = {Time {{Travel}} and {{Provenance}} for {{Machine Learning Pipelines}}},
  author = {Ormenisan, Alexandru A. and Buso, Fabio and Andersson, Robin and Haridi, Seif and Dowling, Jim and Meister, Moritz},
  year = {2020},
  publisher = {2020 Unesix Conference on Operational Machine Learning},
  abstract = {Machine learning pipelines have become the defacto paradigm for productionizing machine learning applications as they clearly abstract the processing steps involved in transforming raw data into engineered features that are then used to train models. In this paper, we use a bottom-up method for capturing provenance information regarding the processing steps and artifacts produced in ML pipelines. Our approach is based on replacing traditional intrusive hooks in application code (to capture ML pipeline events) with standardized change-data-capture support in the systems involved in ML pipelines: the distributed file system, feature store, resource manager, and applications themselves. In particular, we leverage data versioning and time-travel capabilities in our feature store to show how provenance can enable model reproducibility and debugging.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\BCIZ32ND\Ormenisan et al. - 2020 - Time Travel and Provenance for Machine Learning Pi.pdf}
}

@misc{Parquet,
  title = {Parquet},
  journal = {Apache Parquet},
  urldate = {2025-01-22},
  abstract = {The Apache Parquet Website},
  howpublished = {https://parquet.apache.org/},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\CJ3ZWPLW\parquet.apache.org.html}
}

@misc{pattersonCarbonEmissionsLarge2021,
  title = {Carbon {{Emissions}} and {{Large Neural Network Training}}},
  author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  year = {2021},
  month = apr,
  number = {arXiv:2104.10350},
  eprint = {2104.10350},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.10350},
  urldate = {2025-01-22},
  abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {$<$}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\2CCXUC55\\Patterson et al. - 2021 - Carbon Emissions and Large Neural Network Training.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\3M2ERB8C\\2104.html}
}

@article{pattersonCarbonFootprintMachine2022,
  title = {The {{Carbon Footprint}} of {{Machine Learning Training Will Plateau}}, {{Then Shrink}}},
  author = {Patterson, David and Gonzalez, Joseph and H{\"o}lzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},
  year = {2022},
  month = jul,
  journal = {Computer},
  volume = {55},
  number = {7},
  pages = {18--28},
  issn = {1558-0814},
  doi = {10.1109/MC.2022.3148714},
  urldate = {2025-01-22},
  abstract = {Machine learning (ML) workloads have rapidly grown, raising concerns about their carbon footprint. We show four best practices to reduce ML training energy and carbon dioxide emissions. If the whole ML field adopts best practices, we predict that by 2030, total carbon emissions from training will decline.},
  keywords = {Best practices,Carbon dioxide,Carbon footprint,Emissions,Machine learning,Training data},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\7G6XCJTE\\Patterson et al. - 2022 - The Carbon Footprint of Machine Learning Training .pdf;C\:\\Users\\Sebas\\Zotero\\storage\\Y7V9JQFU\\9810097.html}
}

@article{PDFBigData2024,
  title = {({{PDF}}) {{Big Data Processing Stacks}}},
  year = {2024},
  month = oct,
  journal = {ResearchGate},
  doi = {10.1109/MITP.2017.6},
  urldate = {2025-01-22},
  abstract = {PDF {\textbar} After roughly a decade of dominance by the Hadoop framework in the big data processing world, we are witnessing the emergence of various stacks... {\textbar} Find, read and cite all the research you need on ResearchGate},
  langid = {english},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\QXZCXD7W\\2024 - (PDF) Big Data Processing Stacks.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\ZRCBXRP7\\313461573_Big_Data_Processing_Stacks.html}
}

@article{penceWhatBigData2014,
  title = {What Is {{Big Data}} and {{Why}} Is It {{Important}}?},
  author = {Pence, Harry E.},
  year = {2014},
  month = dec,
  journal = {Journal of Educational Technology Systems},
  volume = {43},
  number = {2},
  pages = {159--171},
  publisher = {SAGE Publications Inc},
  issn = {0047-2395},
  doi = {10.2190/ET.43.2.d},
  urldate = {2025-01-22},
  abstract = {Big Data Analytics is a topic fraught with both positive and negative potential. Big Data is defined not just by the amount of information involved but also its variety and complexity, as well as the speed with which it must be analyzed or delivered. The amount of data being produced is already incredibly great, and current developments suggest that this rate will only increase in the near future. Improved service should result as companies better understand their customers, but it is also possible that this data will create privacy problems. Thus, Big Data is important not only to students who hope to gain employment using these techniques and those who plan to use it for legitimate research, but also for everyone who will be living and working in the 21st Century.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\2YWY363I\Pence - 2014 - What is Big Data and Why is it Important.pdf}
}

@mastersthesis{Pettersson1695672,
  title = {Resource-Efficient and Fast {{Point-in-Time}} Joins for {{Apache Spark}} : {{Optimization}} of Time Travel Operations for the Creation of Machine Learning Training Datasets},
  author = {Pettersson, Axel},
  year = {2022},
  series = {{{TRITA-EECS-EX}}},
  number = {2022:193},
  pages = {63},
  abstract = {A scenario in which modern machine learning models are trained is to make use of past data to be able to make predictions about the future. When working with multiple structured and time-labeled datasets, it has become a more common practice to make use of a join operator called the Point-in-Time join, or PIT join, to construct these datasets. The PIT join matches entries from the left dataset with entries of the right dataset where the matched entry is the row whose recorded event time is the closest to the left row's timestamp, out of all the right entries whose event time occurred before or at the same time of the left event time. This feature has long only been a part of time series data processing tools but has recently received a new wave of attention due to the rise of the popularity of feature stores. To be able to perform such an operation when dealing with a large amount of data, data engineers commonly turn to large-scale data processing tools, such as Apache Spark. However, Spark does not have a native implementation when performing these joins and there has not been a clear consensus by the community on how this should be achieved. This, along with previous implementations of the PIT join, raises the question: ''How to perform fast and resource efficient Pointin- Time joins in Apache Spark?''. To answer this question, three different algorithms have been developed and compared for performing a PIT join in Spark in terms of resource consumption and execution time. These algorithms were benchmarked using generated datasets using varying physical partitions and sorting structures. Furthermore, the scalability of the algorithms was tested by running the algorithms on Apache Spark clusters of varying sizes. The results received from the benchmarks showed that the best measurements were achieved by performing the join using Early Stop Sort-Merge Join, a modified version of the regular Sort-Merge Join native to Spark. The best performing datasets were the datasets that were sorted by timestamp and primary key, ascending or descending, using a suitable number of physical partitions. Using this new information gathered by this project, data engineers have been provided with general guidelines to optimize their data processing pipelines to be able to perform more resource-efficient and faster PIT joins.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Apache Spark,ASOF,Join,Optimeringar,Optimizations,Point-in-Time,Tidsresning,Time travel},
  file = {C:\Users\Sebas\Zotero\storage\JE2P3YPT\Pettersson - 2022 - Resource-efficient and fast Point-in-Time joins fo.pdf}
}

@misc{PyIceberg,
  title = {{{PyIceberg}}},
  urldate = {2025-01-22},
  howpublished = {https://py.iceberg.apache.org/},
  file = {C:\Users\Sebas\Zotero\storage\VZ8RNG9U\py.iceberg.apache.org.html}
}

@article{Python_CS-R9526,
  title = {Python Tutorial},
  author = {{van Rossum}, Guido},
  year = {1995},
  month = jan,
  number = {R 9526},
  urldate = {2025-01-22},
  abstract = {Python is a simple, yet powerful programming language that bridges the gap between C and shell programming, and is thus ideally suited for ``throw-away programming'' and rapid prototyping. Its syntax is put together from constructs borrowed from a variety of other languages; most prominent are influences from ABC, C, Modula-3 and Icon. The Python interpreter is easily extended with new functions and data types implemented in C. Python is also suitable as an extension language for highly customizable C applications such as editors or window managers. Python is available for various operating systems, amongst which several flavors of UNIX, Amoeba, the Apple Macintosh O.S., and MS-DOS. This tutorial introduces the reader informally to the basic concepts and features of the Python language and system. It helps to have a Python interpreter handy for hands-on experience, but as the examples are self-contained, the tutorial can be read off-line as well. For a description of standard objects and modules, see the Python Library Reference manual. The Python Reference Manual gives a more formal definition of the language.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\6NJBRPLR\van Rossum - 1995 - Python tutorial.pdf}
}

@misc{python-machine-learning,
  title = {Python {{Machine Learning}} {\textbar} {{Data}} {\textbar} {{Print}}},
  journal = {Packt},
  urldate = {2025-01-22},
  abstract = {Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2. 41 customer reviews. Top rated Data products.},
  howpublished = {https://www.packtpub.com/en-us/product/python-machine-learning-9781789955750?type=print},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\FM6B6YXR\python-machine-learning-9781789955750.html}
}

@inproceedings{raasveldtDuckDBEmbeddableAnalytical2019,
  title = {{{DuckDB}}: An {{Embeddable Analytical Database}}},
  shorttitle = {{{DuckDB}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Raasveldt, Mark and M{\"u}hleisen, Hannes},
  year = {2019},
  month = jun,
  series = {{{SIGMOD}} '19},
  pages = {1981--1984},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3299869.3320212},
  urldate = {2025-01-22},
  abstract = {The immense popularity of SQLite shows that there is a need for unobtrusive in-process data management solutions. However, there is no such system yet geared towards analytical workloads. We demonstrate DuckDB, a novel data management system designed to execute analytical SQL queries while embedded in another process. In our demonstration, we pit DuckDB against other data management solutions to showcase its performance in the embedded analytics scenario. DuckDB is available as Open Source software under a permissive license.},
  isbn = {978-1-4503-5643-5},
  file = {C:\Users\Sebas\Zotero\storage\RFUMYHKM\Raasveldt and Mühleisen - 2019 - DuckDB an Embeddable Analytical Database.pdf}
}

@inproceedings{raasveldtFairBenchmarkingConsidered2018,
  title = {Fair {{Benchmarking Considered Difficult}}: {{Common Pitfalls In Database Performance Testing}}},
  shorttitle = {Fair {{Benchmarking Considered Difficult}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Testing Database Systems}}},
  author = {Raasveldt, Mark and Holanda, Pedro and Gubner, Tim and M{\"u}hleisen, Hannes},
  year = {2018},
  month = jun,
  pages = {1--6},
  publisher = {ACM},
  address = {Houston TX USA},
  doi = {10.1145/3209950.3209955},
  urldate = {2024-04-18},
  abstract = {Performance benchmarking is one of the most commonly used methods for comparing different systems or algorithms, both in scientific literature and in industrial publications. While performance measurements might seem objective on the surface, there are many different ways to influence benchmark results to favor one system over the other, either by accident or on purpose. In this paper, we perform a study of the common pitfalls in DBMS performance comparisons, and give advice on how they can be spotted and avoided so a fair performance comparison between systems can be made. We illustrate the common pitfalls with a series of mock benchmarks, which show large differences in performance where none should be present.},
  isbn = {978-1-4503-5826-2},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\Q52SD8NU\Raasveldt et al. - 2018 - Fair Benchmarking Considered Difficult Common Pit.pdf}
}

@misc{rajaperumalUberEngineeringIncremental2017,
  title = {Uber {{Engineering}}'s {{Incremental Processing Framework}} on {{Hadoop}}},
  author = {Rajaperumal, Prasanna},
  year = {2017},
  month = mar,
  journal = {Uber Blog},
  urldate = {2025-01-22},
  abstract = {Uber Engineering recently built and open sourced Hoodie, an incremental processing framework powering Uber's business critical pipelines on Hadoop.},
  howpublished = {https://www.uber.com/en-EC/blog/hoodie/},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\2LWNLWII\hoodie.html}
}

@misc{RonDBWorldsFastest,
  title = {{{RonDB}} - {{World}}'s Fastest {{Key-Value Store}}},
  urldate = {2025-01-22},
  abstract = {The most advanced distribution system of NDB Cluster, by Mikael Ronstr{\"o}m the inventor of NDB~Cluster; made to work for the most advanced ML applications.},
  howpublished = {https://www.rondb.com/},
  file = {C:\Users\Sebas\Zotero\storage\CAP2IHIP\www.rondb.com.html}
}

@inproceedings{roozbehResourceMonitoringNetwork2013,
  title = {Resource {{Monitoring}} in a {{Network Embedded Cloud}}: {{An Extension}} to {{OSPF-TE}}},
  shorttitle = {Resource {{Monitoring}} in a {{Network Embedded Cloud}}},
  booktitle = {2013 {{IEEE}}/{{ACM}} 6th {{International Conference}} on {{Utility}} and {{Cloud Computing}}},
  author = {Roozbeh, Amir and Sefidcon, Azimeh and Maguire, Gerald Q.},
  year = {2013},
  month = dec,
  pages = {139--146},
  doi = {10.1109/UCC.2013.36},
  urldate = {2025-01-22},
  abstract = {A "network embedded cloud", also known as a "carrier cloud", is a distributed cloud architecture where the computing resources are embedded in, and distributed across the carrier's network. This idea is based on a transformation of carrier-grade networks toward a platform for cloud services and the emergence of cloud computing in a telecommunication environment. In this model the network's resources are simply another set of cloud resources. This changes the resource management problem as compared to resource management inside a data center. In order to meet applications' requirements regarding available bandwidth and computing resources, one needs to consider link conditions and traffic engineering when allocating resources in a network embedded cloud. This paper presents a resource monitoring solution for a network embedded cloud by proposing an extension to the Open Shortest Path First-Traffic Engineering (OSPF-TE) protocols. Modeling, emulation, and analysis show the proposed solution can provide the required data to a cloud management system by sending information about virtual resources in the form of a new opaque link-state advertisement, named cloud LSA. Each embedded data center injects less than 40 bytes per second of additional traffic into the network when sending cloud LSAs at most every 5 seconds.},
  keywords = {carrier cloud,Cloud computing,Cloud LSA,Data models,Distributed databases,link-state update policy,network embedded cloud,opaque LSA,OSPF-TE,Resource management,Routing protocols},
  file = {C:\Users\Sebas\Zotero\storage\QVATYNKI\Roozbeh et al. - 2013 - Resource Monitoring in a Network Embedded Cloud A.pdf}
}

@misc{RustlsTokiorustls2025,
  title = {Rustls/Tokio-Rustls},
  year = {2025},
  month = jan,
  urldate = {2025-01-22},
  abstract = {Async TLS for the Tokio runtime},
  copyright = {Apache-2.0},
  howpublished = {rustls}
}

@misc{RustProgrammingLanguage,
  title = {Rust {{Programming Language}}},
  urldate = {2025-01-22},
  abstract = {A language empowering everyone to build reliable and efficient software.},
  howpublished = {https://www.rust-lang.org/},
  langid = {american}
}

@article{sakrBigDataProcessing2017,
  title = {Big {{Data Processing Stacks}}},
  author = {Sakr, Sherif},
  year = {2017},
  month = jan,
  journal = {IT Professional},
  volume = {19},
  number = {1},
  pages = {34--41},
  issn = {1941-045X},
  doi = {10.1109/MITP.2017.6},
  urldate = {2025-01-24},
  abstract = {After roughly a decade of dominance by the Hadoop framework in the Big Data processing world, we are witnessing the emergence of various stacks that have been enhanced with domain-specific, optimized, and vertically focused Big Data processing features. The author analyzes in detail the capabilities of various Big Data processing stacks and provides insights and guidelines about the latest ongoing developments in this domain.},
  keywords = {big data,Big data,Biological system modeling,Computational modeling,data analysis,Distributed databases,Flink,Hadoop,Programming,Spark,Sparks,Structured Query Language}
}

@inproceedings{samundiswaryObjectStorageArchitecture2017,
  title = {Object Storage Architecture in Cloud for Unstructured Data},
  booktitle = {2017 {{International Conference}} on {{Inventive Systems}} and {{Control}} ({{ICISC}})},
  author = {Samundiswary, S. and Dongre, Nilma M},
  year = {2017},
  month = jan,
  pages = {1--6},
  publisher = {IEEE},
  address = {Coimbatore, India},
  doi = {10.1109/ICISC.2017.8068716},
  urldate = {2024-02-20},
  isbn = {978-1-5090-4715-4},
  file = {C:\Users\Sebas\Zotero\storage\VAEY2RCA\Samundiswary and Dongre - 2017 - Object storage architecture in cloud for unstructu.pdf}
}

@inproceedings{schneiderAssessingLakehouseAnalysis2023,
  title = {Assessing the {{Lakehouse}}: {{Analysis}}, {{Requirements}} and {{Definition}}:},
  shorttitle = {Assessing the {{Lakehouse}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Enterprise Information Systems}}},
  author = {Schneider, Jan and Gr{\"o}ger, Christoph and Lutsch, Arnold and Schwarz, Holger and Mitschang, Bernhard},
  year = {2023},
  pages = {44--56},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  address = {Prague, Czech Republic},
  doi = {10.5220/0011840500003467},
  urldate = {2024-03-05},
  isbn = {978-989-758-648-4},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\MQEKQQZF\Schneider et al. - 2023 - Assessing the Lakehouse Analysis, Requirements an.pdf}
}

@article{shahImproveYourOLAP,
  title = {Improve {{Your OLAP Environment}} with {{Microsoft}} and {{Teradata}}},
  author = {Shah, Rupal and Tkachuk, Richard},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\L384UGE6\Shah and Tkachuk - Improve Your OLAP Environment with Microsoft and T.pdf}
}

@mastersthesis{Sheikholeslami1349978,
  title = {Ablation Programming for Machine Learning},
  author = {Sheikholeslami, Sina},
  year = {2019},
  series = {{{TRITA-EECS-EX}}},
  number = {2019:557},
  pages = {52},
  abstract = {As machine learning systems are being used in an increasing number of applications from analysis of satellite sensory data and health-care analytics to smart virtual assistants and self-driving cars they are also becoming more and more complex. This means that more time and computing resources are needed in order to train the models and the number of design choices and hyperparameters will increase as well. Due to this complexity, it is usually hard to explain the effect of each design choice or component of the machine learning system on its performance.A simple approach for addressing this problem is to perform an ablation study, a scientific examination of a machine learning system in order to gain insight on the effects of its building blocks on its overall performance. However, ablation studies are currently not part of the standard machine learning practice. One of the key reasons for this is the fact that currently, performing an ablation study requires major modifications in the code as well as extra compute and time resources.On the other hand, experimentation with a machine learning system is an iterative process that consists of several trials. A popular approach for execution is to run these trials in parallel, on an Apache Spark cluster. Since Apache Spark follows the Bulk Synchronous Parallel model, parallel execution of trials includes several stages, between which there will be barriers. This means that in order to execute a new set of trials, all trials from the previous stage must be finished. As a result, we usually end up wasting a lot of time and computing resources on unpromising trials that could have been stopped soon after their start.We have attempted to address these challenges by introducing MAGGY, an open-source framework for asynchronous and parallel hyperparameter optimization and ablation studies with Apache Spark and TensorFlow. This framework allows for better resource utilization as well as ablation studies and hyperparameter optimization in a unified and extendable API.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Ablation Studies,Apache Spark,Distributed Machine Learning,Distributed Systems,Hopsworks,Keras},
  file = {C:\Users\Sebas\Zotero\storage\RJ695NYZ\Sheikholeslami - 2019 - Ablation programming for machine learning.pdf}
}

@book{shiranApacheIcebergDefinitive2024,
  title = {Apache {{Iceberg}}: {{The Definitive Guide}}},
  shorttitle = {Apache {{Iceberg}}},
  author = {Shiran, Tomer and Hughes, Jason and Merced, Alex},
  year = {2024},
  month = mar,
  publisher = {O'Reilly},
  urldate = {2024-02-19},
  abstract = {Traditional data architecture patterns are severely limited. To use these patterns, you have to ETL data into each tool---a cost-prohibitive process for making warehouse features available to all of...},
  isbn = {978-1-09-814861-4},
  langid = {english},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\RLMCPD3E\\Apache Iceberg, The Definitive Guide.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\DPS442GX\\9781098148614.html}
}

@misc{StackOverflowDeveloper,
  title = {Stack {{Overflow Developer Survey}} 2023},
  journal = {Stack Overflow},
  urldate = {2025-01-22},
  abstract = {In May 2023 over 90,000 developers responded to our annual survey about how they learn and level up, which tools they're using, and which ones they want.},
  howpublished = {https://survey.stackoverflow.co/2023/?utm\_source=social-share\&utm\_medium=social\&utm\_campaign=dev-survey-2023},
  langid = {english}
}

@misc{StateDataLakehouse2024,
  title = {State of the {{Data Lakehouse}} 2024},
  author = {, Dremio},
  year = {2024},
  urldate = {2025-01-22},
  howpublished = {https://www.dremio.com/newsroom/state-of-the-data-lakehouse-2024-dremio/},
  file = {C:\Users\Sebas\Zotero\storage\3FTVHHYD\state-of-the-data-lakehouse-2024-dremio.html}
}

@misc{StateDataLakehouse2025,
  title = {State of the {{Data Lakehouse}} 2025},
  author = {Jackson, Scott A.},
  journal = {Dremio},
  urldate = {2025-01-25},
  abstract = {Simplify BI and achieve fast time-to-insights with a fully-managed, cloud-native high-performance lakehousing service from Dremio.},
  howpublished = {https://www.dremio.com/},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\I5CUED5W\Dremio-2025-State-of-the-Data-Lakehouse-in-the-AI-Era.pdf}
}

@misc{SurgeAI2024,
  title = {The State of {{AI}} in Early 2024 {\textbar}},
  author = {, McKinsey},
  year = {2024},
  month = may,
  urldate = {2025-01-25},
  howpublished = {https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai},
  langid = {english}
}

@misc{SustainableDevelopment,
  title = {Sustainable {{Development}}},
  urldate = {2025-01-22},
  howpublished = {https://sdgs.un.org/}
}

@phdthesis{SvedlundNordstrom1088359,
  title = {A Global Ecosystem for Datasets on Hadoop},
  author = {Svedlund Nordstr{\"o}m, Johan},
  year = {2016},
  series = {{{TRITA-ICT-EX}}},
  number = {2016:131},
  pages = {47},
  abstract = {The immense growth of the web has led to the age of Big Data. Companies like Google, Yahoo and Facebook generates massive amounts of data everyday. In order to gain value from this data, it needs to be effectively stored and processed. Hadoop, a Big Data framework, can store and process Big Data in a scalable and performant fashion. Both Yahoo and Facebook, two major IT companies, deploy Hadoop as their solution to the Big Data problem. Many application areas for Big Data would benefit from the ability to share datasets across cluster boundaries. However, Hadoop does not support searching for datasets either local to a single Hadoop cluster or across many Hadoop clusters. Similarly, there is only limited support for copying datasets between Hadoop clusters (using Distcp). This project presents a solution to this weakness using the Hadoop distribution, Hops, and its frontend Hopsworks. Clusters advertise their peer-to-peer and search endpoints to a central server called Hops-Site. The advertised endpoints builds a global hadoop ecosystem and gives clusters the ability to participate in publicsearch or peer-to-peer sharing of datasets. HopsWorks users are given a choice to write data into Kafka as it's being downloaded. This opens up new possibilities for data scientists who can interactively analyse remote datasets without having to download everything in advance. By writing data into Kafka as its being downloaded, it can be consumed by entities like Spark-streaming or Flink.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT)},
  file = {C:\Users\Sebas\Zotero\storage\4SNLLXST\Svedlund Nordström - 2016 - A global ecosystem for datasets on hadoop.pdf}
}

@misc{TIOBEIndex,
  title = {{{TIOBE Index}}},
  journal = {TIOBE},
  urldate = {2025-01-22},
  howpublished = {https://www.tiobe.com/tiobe-index/},
  langid = {american},
  file = {C:\Users\Sebas\Zotero\storage\BFKUIJT4\tiobe-index.html}
}

@article{TPC_benchmarks_2000,
  title = {New {{TPC}} Benchmarks for Decision Support and Web Commerce},
  author = {Poess, Meikel and Floyd, Chris},
  year = {2000},
  month = dec,
  journal = {SIGMOD Rec.},
  volume = {29},
  number = {4},
  pages = {64--71},
  issn = {0163-5808},
  doi = {10.1145/369275.369291},
  urldate = {2025-01-22},
  abstract = {For as long as there have been DBMS's and applications that use them, there has been interest in the performance characteristics that these systems exhibit. This month's column describes some of the recent work that has taken place in TPC, the Transaction Processing Performance Council.TPC-A and TPC-B are obsolete benchmarks that you might have heard about in the past. TPC-C V3.5 is the current benchmark for OLTP systems. Introduced in 1992, it has been run on many hardware platforms and DBMS's. Indeed, the TPC web site currently lists 202 TPC-C benchmark results. Due to its maturity, TPC-C will not be discussed in this article.We've asked two very knowledgeable individuals to write this article. Meikel Poess is the chair of the TPC H and TPC-R Subcommittees and Chris Floyd is the chair of the TPC-W Subcommittee. We greatly appreciate their efforts.A wealth of information can be found at the TPC web site [ 1 ]. This information includes the benchmark specifications themselves, TPC membership information, and benchmark results.},
  file = {C:\Users\Sebas\Zotero\storage\A5IBM36X\Poess and Floyd - 2000 - New TPC benchmarks for decision support and web co.pdf}
}

@misc{TPCHHomepage,
  title = {{{TPC-H Homepage}}},
  urldate = {2025-01-22},
  howpublished = {https://www.tpc.org/tpch/},
  file = {C:\Users\Sebas\Zotero\storage\4LQQA7YI\tpch.html}
}

@misc{transactionprocessingperformancecounciltpcTPCH_v301pdf1993,
  title = {{{TPC-H Benchmark}}: {{Decision}} Support, Standard Verification, Revision v3.0.1},
  year = {2022},
  urldate = {2025-01-22},
  howpublished = {https://www.tpc.org/tpc\_documents\_current\_versions/current\_specifications5.asp},
  file = {C\:\\Users\\Sebas\\Zotero\\storage\\IGKWRY22\\TPC-H_v3.0.1.pdf;C\:\\Users\\Sebas\\Zotero\\storage\\Y4LQR47T\\current_specifications5.html}
}

@phdthesis{vilenMountingExternalStorage2018,
  title = {Mounting {{External Storage}} in {{HopsFS}}},
  author = {Vilen, Gabriel},
  year = {2018},
  address = {Delft},
  abstract = {A desirable feature of distributed file systems is to enable clients to mount external namespaces, allowing them to access physically remote data as if it was transparently stored in the local cluster. This feature is desirable in a variety of scenarios, such as in enterprise hybrid-cloud deployments, where some data may be stored remote on the cloud while other in the local file system. This feature introduces questions as how to design, implement, and handle various file system semantics, such as different consistency levels and operational performance between the remote and local storage. For instance, a POXIS-complient block based file system is strongly consistent and does not pose the same semantics as a more relaxed eventual consistent object-based cloud storage. Further, it is interesting to benchmark how such an integrated remote storage solution performs and what potential speedups that might be possible. To examine such questions and provide the remote storage feature in HopsFS, a scale-out metadata HDFS distribution, this thesis presents an Proof-of-Concept (PoC) implementation that enables the local file system to be transparently integrated with an external remote storage. The PoC is storage provider agnostic and enables a HopsFS client to on-demand pull in and access remote data from an external storage into the local file system. Currently, the solution works by mounting a snapshot of a remote storage into HopsFS, a read-only approach. For write and synchronization work, frameworks that layers stronger consistency models on top of S3 may be used, such as s3mper. Performance of the implemented feature has been benchmarked against remote Amazon S3 buckets, showing results of fast throughput for scanning and persisting metadata, but slower (4 Mb/s) for pulling data into the cluster through the Amazon cloud connector S3A. To improve performance, caching by replication of the remote files onto local disk storage has shown to drastically increase the read speed (up to 100x). The work done in this thesis shows a promising PoC that successfully serves remote blocks into a local file system, with future work including supporting writes, improved caching mechanisms, and reflecting off-band changes between the remote and local storage.},
  langid = {english},
  school = {Delft University of Technology},
  file = {C:\Users\Sebas\Zotero\storage\23SAU4KL\Vilen - Mounting External Storage in HopsFS.pdf}
}

@misc{vinkWroteOneFastest2021,
  title = {I Wrote One of the Fastest {{DataFrame}} Libraries - {{Ritchie Vink}}},
  author = {Vink, Ritchie},
  urldate = {2025-01-22},
  howpublished = {https://www.ritchievink.com/blog/2021/02/28/i-wrote-one-of-the-fastest-dataframe-libraries/},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\JTHUH2KD\i-wrote-one-of-the-fastest-dataframe-libraries.html}
}

@misc{WelcomeHomeVim,
  title = {Welcome Home : Vim Online},
  urldate = {2025-01-22},
  howpublished = {https://www.vim.org/},
  file = {C:\Users\Sebas\Zotero\storage\FP54AVQL\www.vim.org.html}
}

@misc{wesmIntroducingApacheArrow2019,
  title = {Introducing {{Apache Arrow Flight}}: {{A Framework}} for {{Fast Data Transport}}},
  shorttitle = {Introducing {{Apache Arrow Flight}}},
  author = {{wesm}},
  year = {2019},
  month = oct,
  journal = {Apache Arrow},
  urldate = {2025-01-22},
  abstract = {This post introduces Arrow Flight, a framework for building high performance data services. We have been building Flight over the last 18 months and are looking for developers and users to get involved.},
  howpublished = {https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/},
  langid = {american},
  file = {C:\Users\Sebas\Zotero\storage\FTGF4TK2\introducing-arrow-flight.html}
}

@misc{WhatGreenSoftware2021,
  title = {What Is {{Green Software}}?},
  year = {2021},
  month = oct,
  journal = {Green Software Foundation},
  urldate = {2025-01-22},
  abstract = {Creating a trusted ecosystem of people, standards, tooling, and best practices for building green software.},
  howpublished = {https://greensoftware.foundation/articles/what-is-green-software},
  file = {C:\Users\Sebas\Zotero\storage\VGYF5IY7\what-is-green-software.html}
}

@misc{WhatLakehouse2020,
  title = {What {{Is}} a {{Lakehouse}}?},
  urldate = {2025-01-22},
  howpublished = {https://www.databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html},
  file = {C:\Users\Sebas\Zotero\storage\8ML4E7K3\what-is-a-data-lakehouse.html}
}

@book{white2009hadoopMapReduce,
  title = {Hadoop: The Definitive Guide},
  shorttitle = {Hadoop},
  author = {White, Tom},
  year = {2015},
  edition = {Fourth edition},
  publisher = {O'Reilly},
  address = {Beijing Cambridge Farnham K{\"o}ln Sebastopol Tokyo},
  isbn = {978-1-4919-0163-2 978-1-4919-0171-7},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\V7I6VPET\White - 2015 - Hadoop the definitive guide.pdf}
}

@mastersthesis{Yedurupak1181024,
  title = {Multitenant {{PrestoDB}} as a Service},
  author = {Yedurupak, Aruna Kumari},
  year = {2017},
  series = {{{TRITA-ICT-EX}}},
  number = {2017:181},
  pages = {55},
  abstract = {In recent years, there has been tremendous growth in both the volumes of data that is produced, stored, and queried by organizations. Organizations spend more money to investigate and obtain useful information or knowledge against terabytes and even petabytes of data. Large-scale data analysis is the key functionality provided by Big Data platforms. Previously, data platforms would get the information from unstructured data in the form of files, text, and videos. In recent times, the Hadoop stack has played a vital role in Big Data, becoming the defector open source software used to process and analyze Big Data. Hops is a Hadoop distribution developed by KTH and RISE SICS. Hops modifies the Hadoop stack by moving the meta-data for YARN and HDFS to NDB, an open-source in-memory distributed database. HopsWorks is the User Interface for Hops and provides support for multi-tenant users, as well as self-service, graphical access to frameworks such as Hadoop, Flink, Spark, Kafka, and Kibana. HopsWorks currently does not provide a SQL-on-Hadoop service, although work is ongoing for supporting Hive. Presto is one of the main SQL-on-Hadoop platform, but, currently, Presto does not provide multi-tenancy support for users. This thesis investigates providing multitenancy support to Presto with the help of HopsWorks, including both the security problem and the self-service UI requirements of HopsWorks. Presto is a distributed SQL query Engine which can run SQL queries against up to petabytes of data. As HopsWorks provides UI access to services, we decided to build our UI for Presto on an existing open-source UI for Presto, called Airpal, developed by Airbnb. This provided solution of the thesis divided into two functionalities. First one, maintain two separate Applications (HopsWorks and Airpal Applications) run by the help of two JVMs and maintain ProxyServlet to control traffic between them. Second one HopsWorks-Presto-service leverages HopsWorks access-control (Data owner and Data-scientist) and self-service security model. The evaluation of the thesis used qualitative approach by comparing HopsWorks-PrestoService with standalone PrestoDB and comparing HopsWorks-PrestoService with HopsWorks without Presto-Service.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT)},
  keywords = {Airpal,Hadoop,Hops,HopsWorks,multi-hyresratt,Multi-tenancy,Presto,Proxy servlet,SQL},
  file = {C:\Users\Sebas\Zotero\storage\I5SM648U\Yedurupak - 2017 - Multitenant PrestoDB as a service.pdf}
}

@article{Zaharia:EECS-2011-82,
  title = {Resilient {{Distributed Datasets}}: {{A Fault-Tolerant Abstraction}} for {{In-Memory Cluster Computing}}},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauley, Murphy and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  abstract = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarsegrained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\SACMQCJE\Zaharia et al. - Resilient Distributed Datasets A Fault-Tolerant A.pdf}
}

@article{zaharia2010spark,
  title = {Spark: {{Cluster Computing}} with {{Working Sets}}},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.},
  langid = {english},
  file = {C:\Users\Sebas\Zotero\storage\RSW5EW5G\Zaharia et al. - Spark Cluster Computing with Working Sets.pdf}
}

@article{zahariaApacheSparkUnified2016,
  title = {Apache {{Spark}}: A Unified Engine for Big Data Processing},
  shorttitle = {Apache {{Spark}}},
  author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
  year = {2016},
  month = oct,
  journal = {Commun. ACM},
  volume = {59},
  number = {11},
  pages = {56--65},
  issn = {0001-0782},
  doi = {10.1145/2934664},
  urldate = {2025-01-22},
  abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.},
  file = {C:\Users\Sebas\Zotero\storage\C2K9DLVK\Zaharia et al. - 2016 - Apache Spark a unified engine for big data proces.pdf}
}

@mastersthesis{Zangis1696783,
  title = {Scaling Apache Hudi by Boosting Query Performance with {{RonDB}} as a Global Index : {{Adopting}} a {{LATS}} Data Store for Indexing},
  author = {Zangis, Ralfs},
  year = {2022},
  series = {{{TRITA-EECS-EX}}},
  number = {2022:194},
  pages = {70},
  abstract = {The storage and use of voluminous data are perplexing issues, the resolution of which has become more pressing with the exponential growth of information. Lakehouses are relatively new approaches that try to accomplish this while hiding the complexity from the user. They provide similar capabilities to a standard database while operating on top of low-cost storage and open file formats. An example of such a system is Hudi, which internally uses indexing to improve the performance of data management in tabular format. This study investigates if the execution times could be decreased by introducing a new engine option for indexing in Hudi. Therefore, the thesis proposes the usage of RonDB as a global index, which is expanded upon by further investigating the viability of different connectors that are available for communication. The research was conducted using both practical experiments and the study of relevant literature. The analysis involved observations made over multiple workloads to document how adequately the solutions can adapt to changes in requirements and types of actions. This thesis recorded the results and visualized them for the convenience of the reader, as well as made them available in a public repository. The conclusions did not coincide with the author's hypothesis that RonDB would provide the fastest indexing solution for all scenarios. Nonetheless, it was observed to be the most consistent approach, potentially making it the best general-purpose solution. As an example, it was noted, that RonDB is capable of dealing with read and write heavy workloads, whilst consistently providing low query latency independent from the file count.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Apache Hudi,Index,Key-value store,Lakehouse,Nyckel-varde butik,Performance,Prestanda,RonDB},
  file = {C:\Users\Sebas\Zotero\storage\53J7EPG8\Zangis - 2022 - Scaling apache hudi by boosting query performance .pdf}
}
