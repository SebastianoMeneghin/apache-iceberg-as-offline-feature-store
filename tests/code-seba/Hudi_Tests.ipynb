{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9123df17",
   "metadata": {},
   "source": [
    "# Read/Write Test of Current Hopsworks Implementation (Hudi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c2d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==2.2.0\n",
    "!pip install timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19604b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import hopsworks\n",
    "import numpy as np\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88ca37",
   "metadata": {},
   "source": [
    "## Console ðŸ•¹ï¸\n",
    "**Modify something here if** you want to:\n",
    "- Change the test repetitions\n",
    "- Define the number of used cores\n",
    "- Change the file names\n",
    "- Save subreports, and define how often\n",
    "- Change the source from where to collect the data\n",
    "- Change the path where to save the data\n",
    "- Change the file type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate how many times you should repeat each test, and how many cores are you using\n",
    "reps = dict()\n",
    "reps[\"sup_1\"]   = 50\n",
    "reps[\"sup_10\"]  = 50\n",
    "reps[\"sup_100\"] = 50\n",
    "reps[\"line_1\"]  = 50\n",
    "reps[\"line_10\"] = 50\n",
    "number_of_cores = 1\n",
    "\n",
    "# Define the file names, according to the name of the test (sup_1, line_10, etc...)\n",
    "file_names = dict()\n",
    "file_names[\"sup_1\"]   = 'supplier_tpch_sf1'\n",
    "file_names[\"sup_10\"]  = 'supplier_tpch_sf10'\n",
    "file_names[\"sup_100\"] = 'supplier_tpch_sf100'\n",
    "file_names[\"line_1\"]  = 'lineitem_tpch_sf1'\n",
    "file_names[\"line_10\"] = 'lineitem_tpch_sf10'\n",
    "\n",
    "# @optional\n",
    "# Set those in order to save the subreports,\n",
    "save_subreports = False\n",
    "save_how_often  = 0\n",
    "\n",
    "# Define where the files should be generally saved, and where they can be retrieved\n",
    "LOCAL_PATH      = '/home/yarnapp/hopsfs/Resources/'\n",
    "REMOTE_PATH     = 'https://repo.hops.works/dev/gio-hopsworks/'\n",
    "FILE_TYPE       = '.parquet'\n",
    "FILENAME_PREFIX = \"hudi_benchmark_results_\" + str(number_of_cores) + \"core\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_paths(file_names):\n",
    "    local_paths = dict()\n",
    "    \n",
    "    for file in file_names:\n",
    "        # The local path of each file should look like: \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf1.parquet\"\n",
    "        local_paths[file] = LOCAL_PATH + file_names[file] + FILE_TYPE\n",
    "    \n",
    "    return local_paths\n",
    "\n",
    "\n",
    "def get_remote_paths(file_names):\n",
    "    remote_paths = dict()\n",
    "    \n",
    "    for file in file_names:\n",
    "        # The remote path of each file should look like: \"https://repo.hops.works/dev/gio-hopsworks/supplier_tpch_sf1.parquet\"\n",
    "        remote_paths[file] = REMOTE_PATH + file_names[file] + FILE_TYPE\n",
    "    \n",
    "    return remote_paths\n",
    "\n",
    "\n",
    "def column_renamer(df):\n",
    "    '''\n",
    "    Given a dataframe, renames all the column to small lower case, in order to make it possible to save the dataframe on Hopsworks\n",
    "    via the usage of a Feature Group.\n",
    "    '''\n",
    "    \n",
    "    for name in df.columns:\n",
    "        df.rename(columns={name : name.lower()}, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_time(text):\n",
    "    pattern = r'(\\d+m\\s*)?\\d+(\\.\\d+)?s'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b57148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all the files and creates the local path and remote path dictionaries\n",
    "local_paths  = get_local_paths(file_names)\n",
    "remote_paths = get_remote_paths(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6537e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all the datasets from the remote_paths to the local_paths\n",
    "for file in file_names:\n",
    "    !curl {remote_paths[file]} -o {local_paths[file]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b597566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_setup_code_write(dataset):\n",
    "    spec_path = str(local_paths[dataset])\n",
    "        \n",
    "    pre_code = '''import hopsworks\n",
    "import pandas as pd\n",
    "\n",
    "def column_renamer(df):\n",
    "    for name in df.columns:\n",
    "        df.rename(columns={name : name.lower()}, inplace=True)\n",
    "\n",
    "    return df\n",
    "'''\n",
    "\n",
    "    spec_code = '''\n",
    "LOCAL_PATH = \"''' + spec_path + '''\"\n",
    "'''    \n",
    "    \n",
    "    post_code = '''\n",
    "df = pd.read_parquet(LOCAL_PATH)\n",
    "df = column_renamer(df)\n",
    "'''\n",
    "    \n",
    "    # Put the codes together and return the wanted code\n",
    "    setup_code = pre_code + spec_code + post_code\n",
    "    return setup_code\n",
    "    \n",
    "\n",
    "def get_setup_code(mode, dataset):\n",
    "    setup_code = []\n",
    "    \n",
    "    if (mode == 'read'):\n",
    "        setup_code = '''import hopsworks\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n",
    "fg = fs.get_feature_group(\"hudi_test\")\n",
    "'''\n",
    "    elif (mode == 'write'):\n",
    "        setup_code = get_setup_code_write(dataset)\n",
    "        \n",
    "    return setup_code\n",
    "\n",
    "\n",
    "def get_test_code(mode, dataset):\n",
    "    test_code = []\n",
    "    \n",
    "    if (mode == 'read'):\n",
    "        test_code = '''\n",
    "fg.read()\n",
    "'''\n",
    "    elif (mode == 'write'):\n",
    "        test_code = '''\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n",
    "fg = fs.get_or_create_feature_group(\n",
    "    name=\"hudi_test\",\n",
    "    version=1,\n",
    "    primary_key=df.columns,\n",
    "    description='Upload of Datasets for testing reasons')\n",
    "fg.insert(df)\n",
    "'''\n",
    "        \n",
    "    return test_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes this could be useful\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n",
    "fg = fs.get_feature_group(\"hudi_test\")\n",
    "fg.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ebd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the test with the above indicated iterations\n",
    "for dataset in reps:\n",
    "    if (reps[dataset] > 0):\n",
    "        print('\\n\\n\\n\\n\\n *** STARTED THE TEST OF ' + str(dataset) + ' FOR ' + str(reps[dataset]) + ' TIME(S) *** \\n\\n\\n\\n\\n')\n",
    "\n",
    "        # Initialise results list of rows\n",
    "        results = []\n",
    "\n",
    "\n",
    "        for rep in range(reps[dataset]):\n",
    "\n",
    "            ############ CREATE THE FEATURE GROUP, PUBLISH THE INFORMATION ####################\n",
    "            # Login to the project and insert/upload the new dataset in a new feature group, while keeping track of the time required by each operation.\n",
    "            # The login time and the creation time can be included in the total time, since they are significantly small.\n",
    "            SETUP_CODE = get_setup_code('write', dataset)\n",
    "            TEST_CODE =  get_test_code('write',  dataset)\n",
    "\n",
    "            upload_time = timeit.timeit(setup = SETUP_CODE, stmt = TEST_CODE, number = 1)\n",
    "            print('**Create featuregrop on Hopsworks and publish messages on Kafka, the time taken is {0} seconds'.format(upload_time))\n",
    "\n",
    "\n",
    "\n",
    "            ############# MATERIALIZE THE DATA ON HOPSWORKS THROUGH SPARK JOBS #######################\n",
    "            # Check the repeatedly the status of the materialization_job. When it is FINISHED,\n",
    "            # get the current time and calculate the time required by the materialization.\n",
    "            before_materialize = time.time()\n",
    "\n",
    "            project = hopsworks.login()\n",
    "            fs = project.get_feature_store()\n",
    "            fg = fs.get_feature_group(\"hudi_test\")\n",
    "\n",
    "            while(fg.materialization_job.get_state() != 'FINISHED'):\n",
    "                time.sleep(2)\n",
    "\n",
    "            after_materialize = time.time()\n",
    "\n",
    "            # Check final state\n",
    "            final_state = fg.materialization_job.get_final_state()\n",
    "            if (final_state != 'SUCCEEDED'):\n",
    "                print(\"\\nWARNING: The final state is: \" + str(final_state))\n",
    "\n",
    "            materialize_time = after_materialize - before_materialize\n",
    "            print(\"\\n**Time needed to materialize: \" + str(materialize_time))\n",
    "\n",
    "\n",
    "\n",
    "            ############### READ FROM THE MATERIALIZED FEATURE GROUP ################################\n",
    "            # Read the data passing the feature group name, after having connected to the Hopsworks' project\n",
    "            SETUP_CODE = get_setup_code('read', dataset)\n",
    "            TEST_CODE =  get_test_code('read',  dataset)\n",
    "            \n",
    "            read_time = timeit.timeit(setup = SETUP_CODE, stmt = TEST_CODE, number = 1)\n",
    "            print('**Read the dataset from Hopsworks, the time taken is {0} seconds'.format(read_time))\n",
    "\n",
    "\n",
    "\n",
    "            ################ END OF MEASUREMENTS ##################################################\n",
    "            # Remove the featuregroup from Hopsworks\n",
    "            project = hopsworks.login()\n",
    "            fs = project.get_feature_store()\n",
    "            fg = fs.get_feature_group(\"hudi_test\")\n",
    "            fg.delete()\n",
    "\n",
    "\n",
    "            ## Save the results of the current test\n",
    "            results_curr = [upload_time, materialize_time, read_time, final_state]\n",
    "            results.append(results_curr)\n",
    "\n",
    "            # Wait in order to be sure that the featuregroup has been removed\n",
    "            time.sleep(5)\n",
    "            \n",
    "            if(save_subreports and ((rep + 1) % save_how_often == 0) and ((rep + 1) < reps[dataset])):\n",
    "                ## Save the test's results in a .csv file with specified name\n",
    "                df = pd.DataFrame(results, columns =  [\"Upload\", \"Materialize\", \"Read\", \"Success_of_materialization\"])\n",
    "                path_csv = str(LOCAL_PATH) + str(FILENAME_PREFIX) + '_' + 'SUB' + str(rep + 1) + '_' + str(file_names[dataset]) + \".csv\"\n",
    "                df.to_csv(path_csv)\n",
    "\n",
    "\n",
    "        ## Save the test's results in a .csv file with specified name\n",
    "        df = pd.DataFrame(results, columns =  [\"Upload\", \"Materialize\", \"Read\", \"Success_of_materialization\"])\n",
    "        path_csv = str(LOCAL_PATH) + str(FILENAME_PREFIX) + '_' + str(file_names[dataset]) + \".csv\"\n",
    "        df.to_csv(path_csv)\n",
    "        \n",
    "        print('\\n\\n\\n\\n\\n *** ENDED TESTING OF ' + str(dataset) + ' FOR ' + str(reps[dataset]) + ' TIME(S) *** \\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the end, remove the kafka topics, in order to free up memory\n",
    "# (By default, Kafka keeps the messages in a topic for 7 days)\n",
    "project = hopsworks.login()\n",
    "kafka_api = project.get_kafka_api()\n",
    "kafka_topics = kafka_api.get_topics()\n",
    "\n",
    "for topic in kafka_topics:\n",
    "    print('\\n *** Deleting the topic named: ' + str(topic.name) + ' ***\\n')\n",
    "    topic.delete()\n",
    "    \n",
    "print('\\n *** Now the dirty work should be done! ***\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9170159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
