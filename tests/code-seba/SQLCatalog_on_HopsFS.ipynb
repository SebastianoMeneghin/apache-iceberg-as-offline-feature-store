{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the PyIceberg library, then fix the dependency problem with SQLAlchemy library\n",
    "!pip install pyiceberg[pyarrow,duckdb,sql-sqlite] --upgrade\n",
    "!pip install sqlalchemy --upgrade\n",
    "#!pip install sqlalchemy==2.0.28 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad3d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import os\n",
    "import importlib\n",
    "from urllib.parse import urlparse\n",
    "from typing import Dict, List\n",
    "from pyarrow.fs import HadoopFileSystem\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import math\n",
    "from pyiceberg.exceptions import CommitFailedException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec211dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ªðŸ§ª ONLY ONCE! ðŸ§ªðŸ§ª\n",
    "# Get the NYC Taxi dataset from the network\n",
    "!curl https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet -o \"/home/yarnapp/hopsfs/Resources/nyc_taxiparquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11723a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder where to save on HopsFS\n",
    "!mkdir /home/yarnapp/hopsfs/Resources/test_dir/\n",
    "!mkdir /tmp/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0399f381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default (<class 'pyiceberg.catalog.sql.SqlCatalog'>)\n"
     ]
    }
   ],
   "source": [
    "#catalog_file_path = \"/home/hdfs/iceberg/catalog/pyiceberg_catalog.db\"\n",
    "catalog_file_path = \"/home/yarnapp/hopsfs/Resources/test_dir/pyiceberg_catalog.db\"\n",
    "hdfs_path = \"/tmp/test_data\"\n",
    "#hdfs_path_uri = \"hdfs://namenode.service.consul:8020/tmp/test_data\"\n",
    "\n",
    "# Create a catalog\n",
    "test_catalog = SqlCatalog(\n",
    "    \"default\",\n",
    "    **{\n",
    "        \"uri\": f\"sqlite:///{catalog_file_path}\",\n",
    "        \"warehouse\": f\"{hdfs_path}\",\n",
    "        \"hdfs.host\": 'namenode.service.consul',\n",
    "    },\n",
    ")\n",
    "\n",
    "# Print the object catalog, to show the catalog type\n",
    "print(test_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a72f9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data previously downloaded into a Parquet DataFrame (df)\n",
    "nyc_data_path = \"/home/yarnapp/hopsfs/Resources/nyc_taxiparquet\"\n",
    "df = pq.read_table(nyc_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a2af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new namespace\n",
    "test_catalog.create_namespace(\"test_ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b149f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-01 14:19:38,808 INFO: Defaulting to PyArrow FileIO\n",
      "2024-07-01 14:19:40,385 INFO: Defaulting to PyArrow FileIO\n",
      "2024-07-01 14:19:40,392 INFO: Defaulting to PyArrow FileIO\n"
     ]
    }
   ],
   "source": [
    "# Create a new table \"test_table\", specifying the schema according to the df's schema\n",
    "test_table = test_catalog.create_table(\n",
    "    \"test_ns.nyc_taxi\",\n",
    "    schema=df.schema,\n",
    "    # ðŸ§ªðŸ§ª TESTING ðŸ§ªðŸ§ª\n",
    "    # The location now should be added, since in the creation of the catalog we are both specifying the host and the warehouse!\n",
    "    location=\"/tmp/test_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c98fa",
   "metadata": {},
   "source": [
    "#### Test the data insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13647b8",
   "metadata": {},
   "source": [
    "Insert the full NYC taxi dataframe in the empy table created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019da283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start APPEND\n",
      "2024-07-01 14:19:43,512 INFO: Defaulting to PyArrow FileIO\n",
      "2024-07-01 14:19:43,517 INFO: Defaulting to PyArrow FileIO\n",
      "End APPEND\n",
      "Before the append operation, there were 0rows in the table\n",
      "After  the append operation, there were 3066766rows in the table\n"
     ]
    }
   ],
   "source": [
    "# Append the dataframe to the test_table, showing the difference between before and after the operation\n",
    "print(\"Start APPEND\")\n",
    "before_len = len(test_table.scan().to_arrow())\n",
    "\n",
    "test_table.append(df)\n",
    "\n",
    "print('End APPEND')\n",
    "after_len  = len(test_table.scan().to_arrow())\n",
    "print(\"Before the append operation, there were \" + str(before_len) + \"rows in the table\")\n",
    "print(\"After  the append operation, there were \" + str(after_len)  + \"rows in the table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae94953a",
   "metadata": {},
   "source": [
    "#### Test multiple APPEND operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058a277",
   "metadata": {},
   "source": [
    "In order to test several consecutive APPEND operations, the Arrow Dataframe containing the NYC Taxi data is transformed in Pandas Dataframe, then divided in small part of 1000 rows each.\n",
    "âš ï¸ Depending on \"how_many\" APPEND operations you want to perform, change the former parameters in the following cell.\n",
    "\n",
    "Several errors might arise, but those should not be related to the functioning of the PyIceberg library: the problem should instead reside in the underlying infrastructure (Jupyter, Hopsworks UI, VM, File access permissions ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765af65",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog(\"default\",**{\"uri\":\"sqlite:////home/yarnapp/hopsfs/Resources/test_dir/pyiceberg_catalog.db\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data previously downloaded into a Parquet DataFrame (df)\n",
    "nyc_data_path = \"/home/yarnapp/hopsfs/Resources/nyc_taxiparquet\"\n",
    "arrow_df      = pq.read_table(nyc_data_path)\n",
    "\n",
    "# Create a set for randomizing the insertion\n",
    "insert_set = set()\n",
    "for i in range(1, math.floor(arrow_df.shape[0]/1000), 2):\n",
    "    insert_set.add(i)\n",
    "    \n",
    "# Transform the arrow dataframe into a pandas DataFrame\n",
    "df_append = pd.DataFrame()\n",
    "df_append = arrow_df.to_pandas()\n",
    "\n",
    "# Set how many times you want to repeat the APPEND operation\n",
    "how_many = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the table where to append the new data\n",
    "table_append = catalog.load_table(\"test_ns.nyc_taxi\")\n",
    "\n",
    "for i in range(how_many):\n",
    "    elem = insert_set.pop()\n",
    "    partial_df = df_append[elem*1000:1000*(elem + 1)]\n",
    "    partial_table = pyarrow.Table.from_pandas(partial_df)\n",
    "\n",
    "    # Append the dataframe to the test_table, showing the difference between before and after the operation\n",
    "    print(\"Start APPEND\")\n",
    "    before_len = len(table_append.scan().to_arrow())\n",
    "    \n",
    "    table_append.append(partial_table)\n",
    "    \n",
    "    print('End APPEND')\n",
    "    after_len  = len(table_append.scan().to_arrow())\n",
    "    print(\"Before the append operation, there were \" + str(before_len) + \"rows in the table\")\n",
    "    print(\"After  the append operation, there were \" + str(after_len)  + \"rows in the table\")\n",
    "    \n",
    "    if i == how_many - 1:\n",
    "        print('\\n\\n ** All the APPEND operations have been completed **')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c94050f",
   "metadata": {},
   "source": [
    "#### Test the schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b5e766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe, equal to df but with a new column\n",
    "updated_df = df.append_column(\"tip_per_mile\", pc.divide(df[\"tip_amount\"], df[\"trip_distance\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dfae0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-01 14:31:18,557 INFO: Defaulting to PyArrow FileIO\n",
      "2024-07-01 14:31:18,564 INFO: Defaulting to PyArrow FileIO\n"
     ]
    }
   ],
   "source": [
    "catalog = load_catalog(\"default\",**{\"uri\":\"sqlite:////home/yarnapp/hopsfs/Resources/test_dir/pyiceberg_catalog.db\"})\n",
    "table   = catalog.load_table(\"test_ns.nyc_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9c6f20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-01 14:20:48,511 INFO: Defaulting to PyArrow FileIO\n",
      "2024-07-01 14:20:48,515 INFO: Defaulting to PyArrow FileIO\n"
     ]
    }
   ],
   "source": [
    "# Extract then the new schema information and save them in a new file\n",
    "with table.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(updated_df.schema)\n",
    "    \n",
    "# Overwrite the previous table, replacing the old dataframe with a new one\n",
    "table.overwrite(updated_df)\n",
    "print(table.scan().to_arrow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbe2e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ªðŸ§ª TESTING ðŸ§ªðŸ§ª\n",
    "\n",
    "# Get the table into a pandas DataFrame, in order to verift its length and integrity.\n",
    "prova = table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7d727",
   "metadata": {},
   "source": [
    "#### Test the table scan and file retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "416643cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2371784"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = table.scan(row_filter=\"tip_per_mile > 0\").to_arrow()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d96e814",
   "metadata": {},
   "source": [
    "---\n",
    "#### @FINAL Delete all the data and files created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499679e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just call it if you are at the end of your own test\n",
    "! rm -r /home/yarnapp/hopsfs/Resources/test_dir\n",
    "! rm -r /tmp/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18acdcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ªðŸ§ª TESTING ðŸ§ªðŸ§ª\n",
    "\n",
    "# Sometimes a file \"default\" is created here. This could cause problem, so run this cell to remove it.\n",
    "! rm -r /home/yarnapp/hopsfs/Experiments/default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8887d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ªðŸ§ª TESTING ðŸ§ªðŸ§ª\n",
    "\n",
    "!echo '.databases'|sqlite3 default\n",
    "#!echo '.databases'|sqlite3 default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
