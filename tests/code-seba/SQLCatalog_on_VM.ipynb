{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a54ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the PyIceberg library, then fix the dependency problem with SQLAlchemy library\n",
    "!pip install pyiceberg[pyarrow]\n",
    "!pip install sqlalchemy==2.0.28 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d820a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder where to save the VM\n",
    "!mkdir /tmp/test_dir\n",
    "!mkdir /tmp/test_dir/test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb9988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ab2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the right test directories\n",
    "test_dir_path  = \"/tmp/test_dir\"\n",
    "test_data_path = test_dir_path + \"/\" + \"test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf66a349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default (<class 'pyiceberg.catalog.sql.SqlCatalog'>)\n"
     ]
    }
   ],
   "source": [
    "# Create a catalog (ONLY THE FIRST TIME)\n",
    "test_catalog = SqlCatalog(\n",
    "    \"default\",\n",
    "    **{\n",
    "        \"uri\": f\"sqlite:///{test_data_path}/pyiceberg_test_catalog.db\",\n",
    "        \"warehouse\": f\"file://{test_data_path}\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Print the object catalog, to show the catalog type\n",
    "print(test_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdedd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the NYC Taxi dataset from the network\n",
    "nyc_data_path = \"/tmp/test_dir/nyc_taxi_data.parquet\"\n",
    "!curl https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet -o \"/tmp/test_dir/nyc_taxi_data.parquet\"\n",
    "    \n",
    "# Then, load the data into a Parquet DataFrame (df)\n",
    "df = pq.read_table(nyc_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d488b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new namespace, and inside create a new table \"test_table\", specifying the schema according to the df's schema\n",
    "test_catalog.create_namespace(\"test_ns\")\n",
    "test_table = test_catalog.create_table(\n",
    "    \"test_ns.nyc_taxi\",\n",
    "    schema=df.schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c2cc65",
   "metadata": {},
   "source": [
    "#### Test the data insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f05da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the dataframe to the test_table, showing the difference between before and after the operation\n",
    "print(\"Before the append operation, there are \" + str(len(test_table.scan().to_arrow())) + \"rows in the table\")\n",
    "test_table.append(df)\n",
    "print(\"After the append operation, there are \" + str(len(test_table.scan().to_arrow())) + \"rows in the table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3c8a5",
   "metadata": {},
   "source": [
    "#### Test the schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e098613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe, equal to df but with a new column\n",
    "updated_df = df.append_column(\"tip_per_mile\", pc.divide(df[\"tip_amount\"], df[\"trip_distance\"]))\n",
    "\n",
    "# Extract then the new schema information and save them in a new file\n",
    "with test_table.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(updated_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbbe38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the previous table, adding the new dataframe\n",
    "test_table.overwrite(updated_df)\n",
    "print(test_table.scan().to_arrow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e403a",
   "metadata": {},
   "source": [
    "#### Test the table scan and file retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48207c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_table.scan(row_filter=\"tip_per_mile > 0\").to_arrow()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find /tmp/test_dir/test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a52231a",
   "metadata": {},
   "source": [
    "---\n",
    "#### @FINAL Delete all the data and files created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just call it if you are at the end of your own test\n",
    "! rm -r /tmp/test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174130c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12541f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
