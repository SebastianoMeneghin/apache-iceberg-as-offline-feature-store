{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee15e66a",
   "metadata": {},
   "source": [
    "# Delta-rs library Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da8c5c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad427d1f",
   "metadata": {},
   "source": [
    "### Downloading the data\n",
    "\n",
    "Only once per project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f434bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose an option:\n",
    "# supplier_tpch_sf1.parquet\n",
    "# supplier_tpch_sf10.parquet\n",
    "# supplier_tpch_sf100.parquet\n",
    "# lineitem_tpch_sf1.parquet\n",
    "# lineitem_tpch_sf10.parquet\n",
    "## Copy the option in the *** field\n",
    "\n",
    "#!curl \"https://repo.hops.works/dev/gio-hopsworks/***\" -o \"/home/yarnapp/hopsfs/Resources/***\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d3696f",
   "metadata": {},
   "source": [
    "### Installing and importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the custom delta-rs library is installed\n",
    "%pip install deltalake\n",
    "# Verify that all other libraries are installed\n",
    "%pip install pyarrow\n",
    "%pip install pandas\n",
    "%pip install timeit\n",
    "\n",
    "## For debugging\n",
    "#%pip install ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake import DeltaTable, write_deltalake\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "## For debugging\n",
    "# from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7909c6",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494ccf02",
   "metadata": {},
   "source": [
    "### Supplier table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88ff3d",
   "metadata": {},
   "source": [
    "#### SF 1 - 10000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d763a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise results list of rows\n",
    "results = []\n",
    "\n",
    "## Iterate benchmark for - Choose a number\n",
    "number_of_iterations = 50\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    ###################### (1) WRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/benchmarks_offline_FS/Experiments/supplier\" \n",
    "LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf1.parquet\"\n",
    "pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "write_deltalake(HDFS_DATA_PATH, pa_table)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    write_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                 stmt   = TEST_CODE,\n",
    "                                 number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('{0} iteration - Write DeltaLake Table - Time taken: {1} s'.format(i + 1 , write_result))\n",
    "\n",
    "    ###################### (2) READ BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/benchmarks_offline_FS/Experiments/supplier\"\n",
    "DeltaTable(HDFS_DATA_PATH)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                stmt   = TEST_CODE,\n",
    "                                number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('{0} iteration - Read  DeltaLake Table - Time taken: {1} s'.format(i + 1, read_result))\n",
    "    \n",
    "    ############################## (END) ############################\n",
    "    \n",
    "    ## Save row of results in results\n",
    "    results_row = [write_result, read_result]\n",
    "    results.append(results_row)\n",
    "    \n",
    "    ## Erase data from created folder\n",
    "    !rm -r /home/yarnapp/hopsfs/Experiments/supplier\n",
    "\n",
    "## Create and then save a dataframe with the results in .csv\n",
    "df = pd.DataFrame(results, columns =  [\"Write\", \"Read\"])\n",
    "df.to_csv(\"/home/yarnapp/hopsfs/Resources/benchmark_results_supplier_tpch_sf1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d1c6e",
   "metadata": {},
   "source": [
    "#### SF 10 - 100000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise results list of rows\n",
    "results = []\n",
    "\n",
    "## Iterate benchmark for - Choose a number\n",
    "number_of_iterations = 50\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    ###################### (1) WRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/benchmarks_offline_FS/Experiments/supplier\" \n",
    "LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf10.parquet\"\n",
    "pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "write_deltalake(HDFS_DATA_PATH, pa_table)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    write_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                 stmt   = TEST_CODE,\n",
    "                                 number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('{0} iteration - Write DeltaLake Table - Time taken: {1} s'.format(i + 1 , write_result))\n",
    "\n",
    "    ###################### (2) READ BENCHMARK ######################\n",
    "\n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/benchmarks_offline_FS/Experiments/supplier\"\n",
    "DeltaTable(HDFS_DATA_PATH)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                stmt   = TEST_CODE,\n",
    "                                number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('{0} iteration - Read  DeltaLake Table - Time taken: {1} s'.format(i + 1, read_result))\n",
    "\n",
    "    ############################## (END) ############################\n",
    "    \n",
    "    ## Save row of results in results\n",
    "    results_row = [write_result, read_result]\n",
    "    results.append(results_row)\n",
    "    \n",
    "    ## Erase data from created folder\n",
    "    !rm -r \"/home/yarnapp/hopsfs/Experiments/supplier\"\n",
    "\n",
    "## Create and then save a dataframe with the results in .csv\n",
    "df = pd.DataFrame(results, columns =  [\"Write\", \"Read\"])\n",
    "df.to_csv(\"/home/yarnapp/hopsfs/Resources/benchmark_results_supplier_tpch_sf10.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c55867b",
   "metadata": {},
   "source": [
    "#### SF 100 - 1000000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise results list of rows\n",
    "results = []\n",
    "\n",
    "## Iterate benchmark for - Choose a number\n",
    "number_of_iterations = 50\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    ###################### (1) WRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/benchmarks_offline_FS/Experiments/supplier\" \n",
    "LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf100.parquet\"\n",
    "pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "write_deltalake(HDFS_DATA_PATH, pa_table)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    write_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                 stmt   = TEST_CODE,\n",
    "                                 number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('{0} iteration - Write DeltaLake Table - Time taken: {1} s'.format(i + 1 , write_result))\n",
    "\n",
    "    ###################### (2) READ BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/benchmarks_offline_FS/Experiments/supplier\"\n",
    "DeltaTable(HDFS_DATA_PATH)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                stmt   = TEST_CODE,\n",
    "                                number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('{0} iteration - Read  DeltaLake Table - Time taken: {1} s'.format(i + 1, read_result))\n",
    "\n",
    "    ############################## (END) ############################\n",
    "    \n",
    "    ## Save row of results in results\n",
    "    results_row = [write_result, read_result]\n",
    "    results.append(results_row)\n",
    "    \n",
    "    ## Erase data from created folder\n",
    "    !rm -r \"/home/yarnapp/hopsfs/Experiments/supplier\"\n",
    "\n",
    "## Create and then save a dataframe with the results in .csv\n",
    "df = pd.DataFrame(results, columns =  [\"Write\", \"Read\"])\n",
    "df.to_csv(\"/home/yarnapp/hopsfs/Resources/benchmark_results_supplier_tpch_sf100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ceb42a",
   "metadata": {},
   "source": [
    "### Lineitem table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d049caf8",
   "metadata": {},
   "source": [
    "#### SF 1 - 6000000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise results list of rows\n",
    "results = []\n",
    "\n",
    "## Iterate benchmark for - Choose a number\n",
    "number_of_iterations = 50\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    ###################### (1) WRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/benchmarks_offline_FS/Experiments/lineitem\" \n",
    "LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/lineitem_tpch_sf1.parquet\"\n",
    "pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "write_deltalake(HDFS_DATA_PATH, pa_table)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    write_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                 stmt   = TEST_CODE,\n",
    "                                 number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('{0} iteration - Write DeltaLake Table - Time taken: {1} s'.format(i + 1 , write_result))\n",
    "\n",
    "    ###################### (2) READ BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/benchmarks_offline_FS/Experiments/lineitem\"\n",
    "DeltaTable(HDFS_DATA_PATH)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                stmt   = TEST_CODE,\n",
    "                                number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('{0} iteration - Read  DeltaLake Table - Time taken: {1} s'.format(i + 1, read_result))\n",
    "\n",
    "    ############################## (END) ############################\n",
    "    \n",
    "    ## Save row of results in results\n",
    "    results_row = [write_result, read_result]\n",
    "    results.append(results_row)\n",
    "    \n",
    "    ## Erase data from created folder\n",
    "    !rm -r \"/home/yarnapp/hopsfs/Experiments/lineitem\"\n",
    "\n",
    "## Create and then save a dataframe with the results in .csv\n",
    "df = pd.DataFrame(results, columns =  [\"Write\", \"Read\"])\n",
    "df.to_csv(\"/home/yarnapp/hopsfs/Resources/benchmark_results_lineitem_tpch_sf1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658316b",
   "metadata": {},
   "source": [
    "#### SF 10 - 60000000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a941a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise results list of rows\n",
    "results = []\n",
    "\n",
    "## Iterate benchmark for - Choose a number\n",
    "number_of_iterations = 50\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    ###################### (1) WRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/benchmarks_offline_FS/Experiments/lineitem\" \n",
    "LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/lineitem_tpch_sf10.parquet\"\n",
    "pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "write_deltalake(HDFS_DATA_PATH, pa_table)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    write_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                 stmt   = TEST_CODE,\n",
    "                                 number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('{0} iteration - Write DeltaLake Table - Time taken: {1} s'.format(i + 1 , write_result))\n",
    "\n",
    "    ###################### (2) READ BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/benchmarks_offline_FS/Experiments/lineitem\"\n",
    "DeltaTable(HDFS_DATA_PATH)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                stmt   = TEST_CODE,\n",
    "                                number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('{0} iteration - Read  DeltaLake Table - Time taken: {1} s'.format(i + 1, read_result))\n",
    "\n",
    "    ############################## (END) ############################\n",
    "    \n",
    "    ## Save row of results in results\n",
    "    results_row = [write_result, read_result]\n",
    "    results.append(results_row)\n",
    "    \n",
    "    ## Erase data from created folder\n",
    "    !rm -r \"/home/yarnapp/hopsfs/Experiments/lineitem\"\n",
    "\n",
    "## Create and then save a dataframe with the results in .csv\n",
    "df = pd.DataFrame(results, columns =  [\"Write\", \"Read\"])\n",
    "df.to_csv(\"/home/yarnapp/hopsfs/Resources/benchmark_results_lineitem_tpch_sf10.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
