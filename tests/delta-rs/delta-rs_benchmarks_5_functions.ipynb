{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "921df59f",
   "metadata": {},
   "source": [
    "# Delta-rs library Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67863d4b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16cf9fb",
   "metadata": {},
   "source": [
    "### Downloading the data\n",
    "\n",
    "Only once per project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a844a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-ef7995ca8a24>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ef7995ca8a24>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    LOCAL_PATH =\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Choose an option:\n",
    "# supplier_tpch_sf1.parquet\n",
    "# supplier_tpch_sf10.parquet\n",
    "# supplier_tpch_sf100.parquet\n",
    "# lineitem_tpch_sf1.parquet\n",
    "# lineitem_tpch_sf10.parquet\n",
    "## Copy the option in the *** field\n",
    "\n",
    "## Set the local path\n",
    "LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/***\"\n",
    "\n",
    "## Set the remote URL\n",
    "REMOTE_URL = \"https://repo.hops.works/dev/gio-hopsworks/***\"\n",
    "\n",
    "!curl REMOTE_URL -o LOCAL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6bb414",
   "metadata": {},
   "source": [
    "### Installing and importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afa2990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deltalake in /srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages (0.18.3)\n",
      "Requirement already satisfied: pyarrow-hotfix in /srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages (from deltalake) (0.6)\n",
      "Requirement already satisfied: pyarrow>=8 in /srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages (from deltalake) (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages (from pyarrow>=8->deltalake) (1.26.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in /srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages (from pyarrow) (1.26.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Verify that the custom delta-rs library is installed\n",
    "%pip install deltalake\n",
    "# Verify that all other libraries are installed\n",
    "%pip install pyarrow\n",
    "%pip install pandas\n",
    "%pip install timeit\n",
    "\n",
    "## For debugging\n",
    "#%pip install ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69906c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake import DeltaTable, write_deltalake\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "## For debugging\n",
    "# from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed916d",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb672b",
   "metadata": {},
   "source": [
    "### Supplier table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70837c84",
   "metadata": {},
   "source": [
    "#### SF 1 - 10000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd47528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write delta lake table, the time taken is 1.782279137056321 seconds\n",
      "Read delta lake table, the time taken is 0.1018887641839683 seconds\n",
      "Append on delta lake table, the time taken is 1.565036104992032 seconds\n",
      "Overwrite delta lake table, the time taken is 1.3607688727788627 seconds\n",
      "Read old delta lake table, the time taken is 0.06789629790000618 seconds\n",
      "Write delta lake table, the time taken is 1.8549835828598589 seconds\n",
      "Read delta lake table, the time taken is 0.13238394213840365 seconds\n",
      "Append on delta lake table, the time taken is 1.8693466200493276 seconds\n",
      "Overwrite delta lake table, the time taken is 1.8909420589916408 seconds\n",
      "Read old delta lake table, the time taken is 0.09576750383712351 seconds\n",
      "Write delta lake table, the time taken is 2.06822645990178 seconds\n",
      "Read delta lake table, the time taken is 0.22875092714093626 seconds\n",
      "Append on delta lake table, the time taken is 1.8702663420699537 seconds\n",
      "Overwrite delta lake table, the time taken is 1.82991316402331 seconds\n",
      "Read old delta lake table, the time taken is 0.1437171339057386 seconds\n",
      "Write delta lake table, the time taken is 1.6310816728509963 seconds\n",
      "Read delta lake table, the time taken is 0.11171772913075984 seconds\n",
      "Append on delta lake table, the time taken is 1.6110331411473453 seconds\n",
      "Overwrite delta lake table, the time taken is 1.7546384949237108 seconds\n",
      "Read old delta lake table, the time taken is 0.08502985816448927 seconds\n",
      "Write delta lake table, the time taken is 1.9017473189160228 seconds\n",
      "Read delta lake table, the time taken is 0.12080788589082658 seconds\n",
      "Append on delta lake table, the time taken is 1.5833223210647702 seconds\n",
      "Overwrite delta lake table, the time taken is 1.9045306688640267 seconds\n",
      "Read old delta lake table, the time taken is 0.0854097860865295 seconds\n"
     ]
    }
   ],
   "source": [
    "## Initialise results list of rows\n",
    "results = []\n",
    "\n",
    "## Iterate benchmark for - Choose a number\n",
    "number_of_iterations = 5\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    ###################### (1) WRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\" \n",
    "LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf1.parquet\"\n",
    "pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "write_deltalake(HDFS_DATA_PATH, pa_table)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    write_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                           stmt   = TEST_CODE,\n",
    "                           number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Write delta lake table, the time taken is {0} seconds'.format(write_result))\n",
    "\n",
    "    ###################### (2) READ BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\"\n",
    "DeltaTable(HDFS_DATA_PATH)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                stmt   = TEST_CODE,\n",
    "                                number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Read delta lake table, the time taken is {0} seconds'.format(read_result))\n",
    "\n",
    "    ###################### (3) APPEND BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\"\n",
    "LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf1.parquet\"\n",
    "pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "df = pa_table.to_pandas()\n",
    "half_number_rows = df.shape[0]//2\n",
    "df_first_half = df.head(half_number_rows)\n",
    "write_deltalake(HDFS_DATA_PATH, df_first_half, mode=\"append\")'''\n",
    "\n",
    "    # benchmark the task\n",
    "    append_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                  stmt   = TEST_CODE,\n",
    "                                  number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Append on delta lake table, the time taken is {0} seconds'.format(append_result))\n",
    "\n",
    "    ###################### (4) OVERWRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\"\n",
    "LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf1.parquet\"\n",
    "pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "df = pa_table.to_pandas()\n",
    "write_deltalake(HDFS_DATA_PATH, df, mode=\"overwrite\")'''\n",
    "\n",
    "    # benchmark the task\n",
    "    overwrite_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                     stmt   = TEST_CODE,\n",
    "                                     number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Overwrite delta lake table, the time taken is {0} seconds'.format(overwrite_result))\n",
    "    \n",
    "    ###################### (5) READ OLD TABLE #####################\n",
    "    \n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\"\n",
    "# This code retrieves the first table version, i.e. Table A\n",
    "dt_old = DeltaTable(HDFS_DATA_PATH, version=0)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_old_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                    stmt   = TEST_CODE,\n",
    "                                    number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Read old delta lake table, the time taken is {0} seconds'.format(read_old_result))\n",
    "    \n",
    "    ## Save row of results in results\n",
    "    results_row = [write_result, read_result, append_result, overwrite_result, read_old_result]\n",
    "    results.append(results_row)\n",
    "    \n",
    "    ## Erase data from created folder\n",
    "    !rm -r /home/yarnapp/hopsfs/Experiments/supplier\n",
    "\n",
    "## Create and then save a dataframe with the results in .csv\n",
    "df = pd.DataFrame(results, columns =  [\"Write\", \"Read\", \"Append\", \"Overwrite\", \"ReadOld\"])\n",
    "df.to_csv(\"/home/yarnapp/hopsfs/Resources/benchmark_results_supplier_tpch_sf1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236aabe6",
   "metadata": {},
   "source": [
    "#### SF 10 - 100000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fa9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise results list of rows\n",
    "results = []\n",
    "\n",
    "## Iterate benchmark for - Choose a number\n",
    "number_of_iterations = 5\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    ###################### (1) WRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "    from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\" \n",
    "    LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf10.parquet\"\n",
    "    pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "    write_deltalake(HDFS_DATA_PATH, pa_table)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    write_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                           stmt   = TEST_CODE,\n",
    "                           number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Write delta lake table, the time taken is {0} seconds'.format(write_result))\n",
    "\n",
    "    ###################### (2) READ BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\"\n",
    "    DeltaTable(HDFS_DATA_PATH)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                stmt   = TEST_CODE,\n",
    "                                number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Read delta lake table, the time taken is {0} seconds'.format(read_result))\n",
    "\n",
    "    ###################### (3) APPEND BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "    from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\"\n",
    "    LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf10.parquet\"\n",
    "    pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "    df = pa_table.to_pandas()\n",
    "    half_number_rows = df.shape[0]//2\n",
    "    df_first_half = df.head(half_number_rows)\n",
    "    write_deltalake(HDFS_DATA_PATH, df_first_half, mode=\"append\")'''\n",
    "\n",
    "    # benchmark the task\n",
    "    append_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                  stmt   = TEST_CODE,\n",
    "                                  number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Append on delta lake table, the time taken is {0} seconds'.format(append_result))\n",
    "\n",
    "    ###################### (4) OVERWRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "    from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\"\n",
    "    LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf10.parquet\"\n",
    "    pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "    df = pa_table.to_pandas()\n",
    "    write_deltalake(HDFS_DATA_PATH, df, mode=\"overwrite\")'''\n",
    "\n",
    "    # benchmark the task\n",
    "    overwrite_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                     stmt   = TEST_CODE,\n",
    "                                     number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Overwrite delta lake table, the time taken is {0} seconds'.format(overwrite_result))\n",
    "    \n",
    "    ###################### (5) READ OLD TABLE #####################\n",
    "    \n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\"\n",
    "    # This code retrieves the first table version, i.e. Table A\n",
    "    dt_old = DeltaTable(HDFS_DATA_PATH, version=0)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_old_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                    stmt   = TEST_CODE,\n",
    "                                    number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Read old delta lake table, the time taken is {0} seconds'.format(read_old_result))\n",
    "    \n",
    "    ## Save row of results in results\n",
    "    results_row = [write_result, read_result, append_result, overwrite_result, read_old_result]\n",
    "    results.append(results_row)\n",
    "    \n",
    "    ## Erase data from created folder\n",
    "    !rm -r \"/home/yarnapp/hopsfs/Experiments/supplier\"\n",
    "\n",
    "## Create and then save a dataframe with the results in .csv\n",
    "df = pd.DataFrame(results, columns =  [\"Write\", \"Read\", \"Append\", \"Overwrite\", \"ReadOld\"])\n",
    "df.to_csv(\"/home/yarnapp/hopsfs/Resources/benchmark_results_supplier_tpch_sf10.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c8906b",
   "metadata": {},
   "source": [
    "#### SF 100 - 1000000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise results list of rows\n",
    "results = []\n",
    "\n",
    "## Iterate benchmark for - Choose a number\n",
    "number_of_iterations = 5\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    ###################### (1) WRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "    from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\" \n",
    "    LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf100.parquet\"\n",
    "    pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "    write_deltalake(HDFS_DATA_PATH, pa_table)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    write_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                           stmt   = TEST_CODE,\n",
    "                           number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Write delta lake table, the time taken is {0} seconds'.format(write_result))\n",
    "\n",
    "    ###################### (2) READ BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\"\n",
    "    DeltaTable(HDFS_DATA_PATH)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                stmt   = TEST_CODE,\n",
    "                                number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Read delta lake table, the time taken is {0} seconds'.format(read_result))\n",
    "\n",
    "    ###################### (3) APPEND BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "    from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\"\n",
    "    LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf100.parquet\"\n",
    "    pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "    df = pa_table.to_pandas()\n",
    "    half_number_rows = df.shape[0]//2\n",
    "    df_first_half = df.head(half_number_rows)\n",
    "    write_deltalake(HDFS_DATA_PATH, df_first_half, mode=\"append\")'''\n",
    "\n",
    "    # benchmark the task\n",
    "    append_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                  stmt   = TEST_CODE,\n",
    "                                  number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Append on delta lake table, the time taken is {0} seconds'.format(append_result))\n",
    "\n",
    "    ###################### (4) OVERWRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "    from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\"\n",
    "    LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/supplier_tpch_sf100.parquet\"\n",
    "    pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "    df = pa_table.to_pandas()\n",
    "    write_deltalake(HDFS_DATA_PATH, df, mode=\"overwrite\")'''\n",
    "\n",
    "    # benchmark the task\n",
    "    overwrite_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                     stmt   = TEST_CODE,\n",
    "                                     number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Overwrite delta lake table, the time taken is {0} seconds'.format(overwrite_result))\n",
    "    \n",
    "    ###################### (5) READ OLD TABLE #####################\n",
    "    \n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/supplier\"\n",
    "    # This code retrieves the first table version, i.e. Table A\n",
    "    dt_old = DeltaTable(HDFS_DATA_PATH, version=0)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_old_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                    stmt   = TEST_CODE,\n",
    "                                    number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Read old delta lake table, the time taken is {0} seconds'.format(read_old_result))\n",
    "    \n",
    "    ## Save row of results in results\n",
    "    results_row = [write_result, read_result, append_result, overwrite_result, read_old_result]\n",
    "    results.append(results_row)\n",
    "    \n",
    "    ## Erase data from created folder\n",
    "    !rm -r \"/home/yarnapp/hopsfs/Experiments/supplier\"\n",
    "\n",
    "## Create and then save a dataframe with the results in .csv\n",
    "df = pd.DataFrame(results, columns =  [\"Write\", \"Read\", \"Append\", \"Overwrite\", \"ReadOld\"])\n",
    "df.to_csv(\"/home/yarnapp/hopsfs/Resources/benchmark_results_supplier_tpch_sf100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f051cd6e",
   "metadata": {},
   "source": [
    "### Lineitem table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19187de7",
   "metadata": {},
   "source": [
    "#### SF 1 - 6000000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise results list of rows\n",
    "results = []\n",
    "\n",
    "## Iterate benchmark for - Choose a number\n",
    "number_of_iterations = 5\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    ###################### (1) WRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "    from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/lineitem\" \n",
    "    LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/lineitem_tpch_sf1.parquet\"\n",
    "    pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "    write_deltalake(HDFS_DATA_PATH, pa_table)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    write_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                           stmt   = TEST_CODE,\n",
    "                           number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Write delta lake table, the time taken is {0} seconds'.format(write_result))\n",
    "\n",
    "    ###################### (2) READ BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/lineitem\"\n",
    "    DeltaTable(HDFS_DATA_PATH)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                stmt   = TEST_CODE,\n",
    "                                number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Read delta lake table, the time taken is {0} seconds'.format(read_result))\n",
    "\n",
    "    ###################### (3) APPEND BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "    from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/lineitem\"\n",
    "    LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/lineitem_tpch_sf1.parquet\"\n",
    "    pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "    df = pa_table.to_pandas()\n",
    "    half_number_rows = df.shape[0]//2\n",
    "    df_first_half = df.head(half_number_rows)\n",
    "    write_deltalake(HDFS_DATA_PATH, df_first_half, mode=\"append\")'''\n",
    "\n",
    "    # benchmark the task\n",
    "    append_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                  stmt   = TEST_CODE,\n",
    "                                  number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Append on delta lake table, the time taken is {0} seconds'.format(append_result))\n",
    "\n",
    "    ###################### (4) OVERWRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "    from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/lineitem\"\n",
    "    LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/lineitem_tpch_sf1.parquet\"\n",
    "    pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "    df = pa_table.to_pandas()\n",
    "    write_deltalake(HDFS_DATA_PATH, df, mode=\"overwrite\")'''\n",
    "\n",
    "    # benchmark the task\n",
    "    overwrite_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                     stmt   = TEST_CODE,\n",
    "                                     number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Overwrite delta lake table, the time taken is {0} seconds'.format(overwrite_result))\n",
    "    \n",
    "    ###################### (5) READ OLD TABLE #####################\n",
    "    \n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/lineitem\"\n",
    "    # This code retrieves the first table version, i.e. Table A\n",
    "    dt_old = DeltaTable(HDFS_DATA_PATH, version=0)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_old_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                    stmt   = TEST_CODE,\n",
    "                                    number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Read old delta lake table, the time taken is {0} seconds'.format(read_old_result))\n",
    "    \n",
    "    ## Save row of results in results\n",
    "    results_row = [write_result, read_result, append_result, overwrite_result, read_old_result]\n",
    "    results.append(results_row)\n",
    "    \n",
    "    ## Erase data from created folder\n",
    "    !rm -r \"/home/yarnapp/hopsfs/Experiments/lineitem\"\n",
    "\n",
    "## Create and then save a dataframe with the results in .csv\n",
    "df = pd.DataFrame(results, columns =  [\"Write\", \"Read\", \"Append\", \"Overwrite\", \"ReadOld\"])\n",
    "df.to_csv(\"/home/yarnapp/hopsfs/Resources/benchmark_results_lineitem_tpch_sf1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384986b8",
   "metadata": {},
   "source": [
    "#### SF 10 - 60000000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cdbd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise results list of rows\n",
    "results = []\n",
    "\n",
    "## Iterate benchmark for - Choose a number\n",
    "number_of_iterations = 5\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    ###################### (1) WRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "    from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/lineitem\" \n",
    "    LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/lineitem_tpch_sf10.parquet\"\n",
    "    pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "    write_deltalake(HDFS_DATA_PATH, pa_table)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    write_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                           stmt   = TEST_CODE,\n",
    "                           number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Write delta lake table, the time taken is {0} seconds'.format(write_result))\n",
    "\n",
    "    ###################### (2) READ BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/lineitem\"\n",
    "    DeltaTable(HDFS_DATA_PATH)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                stmt   = TEST_CODE,\n",
    "                                number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Read delta lake table, the time taken is {0} seconds'.format(read_result))\n",
    "\n",
    "    ###################### (3) APPEND BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "    from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/lineitem\"\n",
    "    LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/lineitem_tpch_sf10.parquet\"\n",
    "    pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "    df = pa_table.to_pandas()\n",
    "    half_number_rows = df.shape[0]//2\n",
    "    df_first_half = df.head(half_number_rows)\n",
    "    write_deltalake(HDFS_DATA_PATH, df_first_half, mode=\"append\")'''\n",
    "\n",
    "    # benchmark the task\n",
    "    append_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                  stmt   = TEST_CODE,\n",
    "                                  number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Append on delta lake table, the time taken is {0} seconds'.format(append_result))\n",
    "\n",
    "    ###################### (4) OVERWRITE BENCHMARK #####################\n",
    "\n",
    "    SETUP_CODE='''import pyarrow as pa\n",
    "    from deltalake import write_deltalake'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/lineitem\"\n",
    "    LOCAL_PATH = \"/home/yarnapp/hopsfs/Resources/lineitem_tpch_sf10.parquet\"\n",
    "    pa_table = pa.parquet.read_table(LOCAL_PATH)\n",
    "    df = pa_table.to_pandas()\n",
    "    write_deltalake(HDFS_DATA_PATH, df, mode=\"overwrite\")'''\n",
    "\n",
    "    # benchmark the task\n",
    "    overwrite_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                     stmt   = TEST_CODE,\n",
    "                                     number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Overwrite delta lake table, the time taken is {0} seconds'.format(overwrite_result))\n",
    "    \n",
    "    ###################### (5) READ OLD TABLE #####################\n",
    "    \n",
    "    SETUP_CODE='''from deltalake import DeltaTable'''\n",
    "\n",
    "    TEST_CODE='''\n",
    "    HDFS_DATA_PATH = \"hdfs://rpc.namenode.service.consul:8020/Projects/test/Experiments/lineitem\"\n",
    "    # This code retrieves the first table version, i.e. Table A\n",
    "    dt_old = DeltaTable(HDFS_DATA_PATH, version=0)'''\n",
    "\n",
    "    # benchmark the task\n",
    "    read_old_result = timeit.timeit(setup  = SETUP_CODE,\n",
    "                                    stmt   = TEST_CODE,\n",
    "                                    number = 1          )\n",
    "\n",
    "    # report the result\n",
    "    # printing exec. time\n",
    "    print('Read old delta lake table, the time taken is {0} seconds'.format(read_old_result))\n",
    "    \n",
    "    ## Save row of results in results\n",
    "    results_row = [write_result, read_result, append_result, overwrite_result, read_old_result]\n",
    "    results.append(results_row)\n",
    "    \n",
    "    ## Erase data from created folder\n",
    "    !rm -r \"/home/yarnapp/hopsfs/Experiments/lineitem\"\n",
    "\n",
    "## Create and then save a dataframe with the results in .csv\n",
    "df = pd.DataFrame(results, columns =  [\"Write\", \"Read\", \"Append\", \"Overwrite\", \"ReadOld\"])\n",
    "df.to_csv(\"/home/yarnapp/hopsfs/Resources/benchmark_results_lineitem_tpch_sf10.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
