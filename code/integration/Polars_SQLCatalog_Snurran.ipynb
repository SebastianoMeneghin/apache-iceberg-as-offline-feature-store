{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d54d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™üß™ ONLY ONCE! üß™üß™\n",
    "# Get the NYC Taxi dataset from the network\n",
    "!curl https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet -o \"/home/yarnapp/hopsfs/Resources/nyc_taxiparquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fdefba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the PyIceberg library, then fix the dependency problem with SQLAlchemy library\n",
    "!pip install pyiceberg[pyarrow,duckdb,sql-sqlite] --upgrade\n",
    "!pip install sqlalchemy --upgrade\n",
    "#!pip install sqlalchemy==2.0.28 --upgrade\n",
    "!pip install pandas --upgrade\n",
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import os\n",
    "import importlib\n",
    "from urllib.parse import urlparse\n",
    "from typing import Dict, List\n",
    "from pyarrow.fs import HadoopFileSystem\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from pyiceberg.exceptions import CommitFailedException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34865b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder where to save on HopsFS\n",
    "!mkdir /home/yarnapp/hopsfs/Resources/test_dir/\n",
    "!mkdir /tmp/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ef766",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_file_path = \"/home/yarnapp/hopsfs/Resources/test_dir/pyiceberg_catalog.db\"\n",
    "hdfs_path = \"/tmp/test_data\"\n",
    "\n",
    "# Create a catalog\n",
    "test_catalog = SqlCatalog(\n",
    "    \"default\",\n",
    "    **{\n",
    "        \"uri\": f\"sqlite:///{catalog_file_path}\",\n",
    "        \"warehouse\": f\"{hdfs_path}\",\n",
    "        \"hdfs.host\": 'namenode.service.consul',\n",
    "    },\n",
    ")\n",
    "\n",
    "# Print the object catalog, to show the catalog type\n",
    "print(test_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data previously downloaded into a Parquet DataFrame (df)\n",
    "nyc_data_path = \"/home/yarnapp/hopsfs/Resources/nyc_taxiparquet\"\n",
    "df = pq.read_table(nyc_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e86bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new namespace\n",
    "test_catalog.create_namespace(\"test_ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new table \"test_table\", specifying the schema according to the df's schema\n",
    "test_table = test_catalog.create_table(\n",
    "    \"test_ns.nyc_taxi\",\n",
    "    schema=df.schema,\n",
    "    location=\"/tmp/test_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1dc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the Arrow dataframe to a Polars Dataframe\n",
    "polars_df = pl.from_arrow(df)\n",
    "\n",
    "# Gets the name of the numerical columns of the dataframe\n",
    "numerical_columns = [col for col, dtype in polars_df.schema.items() if dtype in [pl.Int64, pl.Float64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f2c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of num_col casual words of lenghts N, for new column naming\n",
    "# Every additional 1GB requires to create 40 new columns.\n",
    "N = 7\n",
    "num_col = 4000\n",
    "names = set()\n",
    "for i in range(num_col):\n",
    "    names.add(''.join(random.choices(string.ascii_uppercase +\n",
    "                                 string.digits, k=N)))\n",
    "    \n",
    "# Add num_col columns to the previous datafrane\n",
    "for name in names:\n",
    "    random_column = random.choice(numerical_columns)\n",
    "    random_multiplier = random.uniform(1.5, 4.9)\n",
    "    polars_df = polars_df.with_columns((pl.col(random_column) * random_multiplier).alias(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fda36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Arrow Table from a Pandas DataFrame, while tracking down the time needed for the operation.\n",
    "start_time = time.time()\n",
    "big_table  = polars_df.to_arrow()\n",
    "end_time   = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a0045",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_time   = end_time - start_time\n",
    "print(\"Time required: \" + str(req_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(int(sys.getsizeof(big_table))/(1024*1024*1024)) + \" GBs occupied by Arrow's Table!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617db568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Iceberg Table's schema\n",
    "table_append = test_catalog.load_table(\"test_ns.nyc_taxi\")\n",
    "with table_append.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(big_table.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b632b05",
   "metadata": {},
   "source": [
    "#### Test the data insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827fd152",
   "metadata": {},
   "source": [
    "Insert the full NYC taxi dataframe in the empy table created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36210ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the dataframe to the Iceberg Table test_table, showing the difference between before and after the operation\n",
    "# (Remove the commented lines below in order to check how many (new) rows are added to the Iceberg table during your operation(s))\n",
    "table_append = test_catalog.load_table(\"test_ns.nyc_taxi\")\n",
    "\n",
    "print(\"Start APPEND\")\n",
    "#before_len = len(table_append.scan().to_arrow())\n",
    "\n",
    "start_time = time.time()\n",
    "table_append.append(big_table)\n",
    "end_time   = time.time()\n",
    "req_time   = end_time - start_time\n",
    "\n",
    "print('End APPEND')\n",
    "#after_len  = len(table_append.scan().to_arrow())\n",
    "#print(\"Before the append operation, there were \" + str(before_len) + \" rows in the table\")\n",
    "#print(\"After  the append operation, there were \" + str(after_len)  + \" rows in the table\")\n",
    "print(\"Time required: \" + str(req_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec697ea",
   "metadata": {},
   "source": [
    "#### Test multiple APPEND operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f8909",
   "metadata": {},
   "source": [
    "In order to test several consecutive APPEND operations, the Arrow Dataframe containing the NYC Taxi data is transformed in Pandas Dataframe, then divided in small part of 1000 rows each.\n",
    "‚ö†Ô∏è Depending on \"how_many\" APPEND operations you want to perform, change the former parameters in the following cell.\n",
    "\n",
    "Several errors might arise, but those should not be related to the functioning of the PyIceberg library: the problem should instead reside in the underlying infrastructure (Jupyter, Hopsworks UI, VM, File access permissions ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog(\"default\",**{\"uri\":\"sqlite:////home/yarnapp/hopsfs/Resources/test_dir/pyiceberg_catalog.db\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80766355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data previously downloaded into a Parquet DataFrame (df)\n",
    "nyc_data_path = \"/home/yarnapp/hopsfs/Resources/nyc_taxiparquet\"\n",
    "arrow_df      = pq.read_table(nyc_data_path)\n",
    "\n",
    "# Create a set for randomizing the insertion\n",
    "insert_set = set()\n",
    "for i in range(1, math.floor(arrow_df.shape[0]/1000), 2):\n",
    "    insert_set.add(i)\n",
    "    \n",
    "# Transform the arrow dataframe into a pandas DataFrame\n",
    "df_append = pd.DataFrame()\n",
    "df_append = arrow_df.to_pandas()\n",
    "\n",
    "# Set how many times you want to repeat the APPEND operation\n",
    "how_many = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d25486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the table where to append the new data\n",
    "table_append = catalog.load_table(\"test_ns.nyc_taxi\")\n",
    "\n",
    "for i in range(how_many):\n",
    "    elem = insert_set.pop()\n",
    "    partial_df = df_append[elem*1000:1000*(elem + 1)]\n",
    "    partial_table = pyarrow.Table.from_pandas(partial_df)\n",
    "\n",
    "    # Append the dataframe to the test_table, showing the difference between before and after the operation\n",
    "    print(\"Start APPEND\")\n",
    "    before_len = len(table_append.scan().to_arrow())\n",
    "    \n",
    "    table_append.append(partial_table)\n",
    "    \n",
    "    print('End APPEND')\n",
    "    after_len  = len(table_append.scan().to_arrow())\n",
    "    print(\"Before the append operation, there were \" + str(before_len) + \"rows in the table\")\n",
    "    print(\"After  the append operation, there were \" + str(after_len)  + \"rows in the table\")\n",
    "    \n",
    "    if i == how_many - 1:\n",
    "        print('\\n\\n ** All the APPEND operations have been completed **')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dcae90",
   "metadata": {},
   "source": [
    "#### Test the schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e770d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe, equal to df but with a new column\n",
    "updated_df = df.append_column(\"tip_per_mile\", pc.divide(df[\"tip_amount\"], df[\"trip_distance\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cffb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog(\"default\",**{\"uri\":\"sqlite:////home/yarnapp/hopsfs/Resources/test_dir/pyiceberg_catalog.db\"})\n",
    "table   = catalog.load_table(\"test_ns.nyc_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract then the new schema information and save them in a new file\n",
    "with table.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(updated_df.schema)\n",
    "    \n",
    "# Overwrite the previous table, replacing the old dataframe with a new one\n",
    "table.overwrite(updated_df)\n",
    "print(table.scan().to_arrow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86604ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the table into a pandas DataFrame, in order to verift its length and integrity.\n",
    "prova = table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cbedf",
   "metadata": {},
   "source": [
    "#### Test the table read operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec244a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \"reading\" (scan) operation on PyIceberg, and track the time needed\n",
    "read_table = test_catalog.load_table(\"test_ns.nyc_taxi\")\n",
    "before_read = time.time()\n",
    "read_df = read_table.scan().to_arrow()\n",
    "after_read  = time.time()\n",
    "total_time = after_read - before_read\n",
    "\n",
    "print(total_time)\n",
    "print(\"\\n**Time needed to read: \" + time_printer(total_time) + \"**\")\n",
    "print(sys.getsizeof(read_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9cea8d",
   "metadata": {},
   "source": [
    "#### Test the table scan and file retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbc04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table.scan(row_filter=\"tip_per_mile > 0\").to_arrow()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65ab96e",
   "metadata": {},
   "source": [
    "---\n",
    "#### @FINAL Delete all the data and files created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just call it if you are at the end of your own test\n",
    "! rm -r /home/yarnapp/hopsfs/Resources/test_dir\n",
    "! rm -r /tmp/test_data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
