{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ªðŸ§ª ONLY ONCE! ðŸ§ªðŸ§ª\n",
    "# Get the NYC Taxi dataset from the network\n",
    "!curl https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet -o \"/home/yarnapp/hopsfs/Resources/nyc_taxiparquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the PyIceberg library, then fix the dependency problem with SQLAlchemy library\n",
    "!pip install pyiceberg[pyarrow,duckdb,sql-sqlite] --upgrade\n",
    "!pip install sqlalchemy --upgrade\n",
    "#!pip install sqlalchemy==2.0.28 --upgrade\n",
    "!pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7233c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import os\n",
    "import importlib\n",
    "from urllib.parse import urlparse\n",
    "from typing import Dict, List\n",
    "from pyarrow.fs import HadoopFileSystem\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from pyiceberg.exceptions import CommitFailedException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e32308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_printer(total_time):\n",
    "    minutes = math.floor(total_time / 60)\n",
    "    seconds = math.ceil( total_time % 60)\n",
    "    \n",
    "    to_print = \"\"\n",
    "    if (minutes > 0):\n",
    "        to_print = str(minutes) + \"m \"\n",
    "        \n",
    "    to_print = to_print + str(seconds) +\"s \"\n",
    "    return to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder where to save on HopsFS\n",
    "!mkdir /home/yarnapp/hopsfs/Resources/test_dir/\n",
    "!mkdir /tmp/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea949706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#catalog_file_path = \"/home/hdfs/iceberg/catalog/pyiceberg_catalog.db\"\n",
    "catalog_file_path = \"/home/yarnapp/hopsfs/Resources/test_dir/pyiceberg_catalog.db\"\n",
    "hdfs_path = \"/tmp/test_data\"\n",
    "#hdfs_path_uri = \"hdfs://namenode.service.consul:8020/tmp/test_data\"\n",
    "\n",
    "# Create a catalog\n",
    "test_catalog = SqlCatalog(\n",
    "    \"default\",\n",
    "    **{\n",
    "        \"uri\": f\"sqlite:///{catalog_file_path}\",\n",
    "        \"warehouse\": f\"{hdfs_path}\",\n",
    "        \"hdfs.host\": 'namenode.service.consul',\n",
    "    },\n",
    ")\n",
    "\n",
    "# Print the object catalog, to show the catalog type\n",
    "print(test_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56beb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data previously downloaded into a Parquet DataFrame (df)\n",
    "nyc_data_path = \"/home/yarnapp/hopsfs/Resources/nyc_taxiparquet\"\n",
    "df = pq.read_table(nyc_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907caa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new namespace\n",
    "test_catalog.create_namespace(\"test_ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc7679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new table \"test_table\", specifying the schema according to the df's schema\n",
    "test_table = test_catalog.create_table(\n",
    "    \"test_ns.nyc_taxi\",\n",
    "    schema=df.schema,\n",
    "    # ðŸ§ªðŸ§ª TESTING ðŸ§ªðŸ§ª\n",
    "    # The location now should be added, since in the creation of the catalog we are both specifying the host and the warehouse!\n",
    "    location=\"/tmp/test_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae72294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the arrow dataframe to a Pandas Dataframe\n",
    "dataframe = pd.DataFrame()\n",
    "dataframe = df.to_pandas()\n",
    "\n",
    "# Gets the name of the numerical columns of the dataframe\n",
    "name_df = dataframe.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "starting_columns = name_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of num_col casual words of lenghts N, for new column naming\n",
    "# Every additional 1GB requires to create 40 new columns. (4400 for 100GB)\n",
    "N = 7\n",
    "num_col = 500\n",
    "names = set()\n",
    "for i in range(num_col):\n",
    "    names.add(''.join(random.choices(string.ascii_uppercase +\n",
    "                                 string.digits, k=N)))\n",
    "    \n",
    "# Add num_col columns to the previous datafrane\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for name in names:\n",
    "        dataframe[name] = dataframe[random.choice(starting_columns)] * random.uniform(1.5, 4.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed539e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(int(sys.getsizeof(dataframe))/(1024*1024*1024)) + \" GBs occupied by Pandas' DF!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf48298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Arrow Table from a Pandas DataFrame, while tracking down the time needed for the operation.\n",
    "start_time = time.time()\n",
    "big_table  = pyarrow.Table.from_pandas(dataframe)\n",
    "end_time   = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0789f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_time   = end_time - start_time\n",
    "print(\"Time required: \" + str(req_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(int(sys.getsizeof(big_table))/(1024*1024*1024)) + \" GBs occupied by Arrow's Table!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64dc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Iceberg Table's schema\n",
    "table_append = test_catalog.load_table(\"test_ns.nyc_taxi\")\n",
    "with table_append.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(big_table.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5796cda",
   "metadata": {},
   "source": [
    "#### Test the data insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf21d4",
   "metadata": {},
   "source": [
    "Insert the full NYC taxi dataframe in the empy table created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the dataframe to the Iceberg Table test_table, showing the difference between before and after the operation\n",
    "# (Remove the commented lines below in order to check how many (new) rows are added to the Iceberg table during your operation(s))\n",
    "table_append = test_catalog.load_table(\"test_ns.nyc_taxi\")\n",
    "\n",
    "print(\"Start APPEND\")\n",
    "#before_len = len(table_append.scan().to_arrow())\n",
    "\n",
    "start_time = time.time()\n",
    "table_append.append(big_table)\n",
    "end_time   = time.time()\n",
    "req_time   = end_time - start_time\n",
    "\n",
    "print('End APPEND')\n",
    "#after_len  = len(table_append.scan().to_arrow())\n",
    "#print(\"Before the append operation, there were \" + str(before_len) + \" rows in the table\")\n",
    "#print(\"After  the append operation, there were \" + str(after_len)  + \" rows in the table\")\n",
    "print(\"Time required: \" + str(req_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b4b99",
   "metadata": {},
   "source": [
    "#### Test multiple APPEND operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5ebfd",
   "metadata": {},
   "source": [
    "In order to test several consecutive APPEND operations, the Arrow Dataframe containing the NYC Taxi data is transformed in Pandas Dataframe, then divided in small part of 1000 rows each.\n",
    "âš ï¸ Depending on \"how_many\" APPEND operations you want to perform, change the former parameters in the following cell.\n",
    "\n",
    "Several errors might arise, but those should not be related to the functioning of the PyIceberg library: the problem should instead reside in the underlying infrastructure (Jupyter, Hopsworks UI, VM, File access permissions ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f0affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog(\"default\",**{\"uri\":\"sqlite:////home/yarnapp/hopsfs/Resources/test_dir/pyiceberg_catalog.db\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b44d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data previously downloaded into a Parquet DataFrame (df)\n",
    "nyc_data_path = \"/home/yarnapp/hopsfs/Resources/nyc_taxiparquet\"\n",
    "arrow_df      = pq.read_table(nyc_data_path)\n",
    "\n",
    "# Create a set for randomizing the insertion\n",
    "insert_set = set()\n",
    "for i in range(1, math.floor(arrow_df.shape[0]/1000), 2):\n",
    "    insert_set.add(i)\n",
    "    \n",
    "# Transform the arrow dataframe into a pandas DataFrame\n",
    "df_append = pd.DataFrame()\n",
    "df_append = arrow_df.to_pandas()\n",
    "\n",
    "# Set how many times you want to repeat the APPEND operation\n",
    "how_many = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57712f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the table where to append the new data\n",
    "table_append = catalog.load_table(\"test_ns.nyc_taxi\")\n",
    "\n",
    "for i in range(how_many):\n",
    "    elem = insert_set.pop()\n",
    "    partial_df = df_append[elem*1000:1000*(elem + 1)]\n",
    "    partial_table = pyarrow.Table.from_pandas(partial_df)\n",
    "\n",
    "    # Append the dataframe to the test_table, showing the difference between before and after the operation\n",
    "    print(\"Start APPEND\")\n",
    "    before_len = len(table_append.scan().to_arrow())\n",
    "    \n",
    "    table_append.append(partial_table)\n",
    "    \n",
    "    print('End APPEND')\n",
    "    after_len  = len(table_append.scan().to_arrow())\n",
    "    print(\"Before the append operation, there were \" + str(before_len) + \"rows in the table\")\n",
    "    print(\"After  the append operation, there were \" + str(after_len)  + \"rows in the table\")\n",
    "    \n",
    "    if i == how_many - 1:\n",
    "        print('\\n\\n ** All the APPEND operations have been completed **')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93370b12",
   "metadata": {},
   "source": [
    "#### Test the schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663845f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe, equal to df but with a new column\n",
    "updated_df = df.append_column(\"tip_per_mile\", pc.divide(df[\"tip_amount\"], df[\"trip_distance\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog(\"default\",**{\"uri\":\"sqlite:////home/yarnapp/hopsfs/Resources/test_dir/pyiceberg_catalog.db\"})\n",
    "table   = catalog.load_table(\"test_ns.nyc_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ba246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract then the new schema information and save them in a new file\n",
    "with table.update_schema() as update_schema:\n",
    "    update_schema.union_by_name(updated_df.schema)\n",
    "    \n",
    "# Overwrite the previous table, replacing the old dataframe with a new one\n",
    "table.overwrite(updated_df)\n",
    "print(table.scan().to_arrow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the table into a pandas DataFrame, in order to verift its length and integrity.\n",
    "prova = table.scan().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5bbb15",
   "metadata": {},
   "source": [
    "#### Test the table read operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a5c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \"reading\" (scan) operation on PyIceberg, and track the time needed\n",
    "read_table = test_catalog.load_table(\"test_ns.nyc_taxi\")\n",
    "before_read = time.time()\n",
    "read_df = read_table.scan().to_arrow()\n",
    "after_read  = time.time()\n",
    "total_time = after_read - before_read\n",
    "\n",
    "print(total_time)\n",
    "print(\"\\n**Time needed to read: \" + time_printer(total_time) + \"**\")\n",
    "print(sys.getsizeof(read_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579433fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(read_table.scan())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7bab2b",
   "metadata": {},
   "source": [
    "#### Test the table scan and file retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table.scan(row_filter=\"tip_per_mile > 0\").to_arrow()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7816fc",
   "metadata": {},
   "source": [
    "---\n",
    "#### @FINAL Delete all the data and files created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709820f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just call it if you are at the end of your own test\n",
    "! rm -r /home/yarnapp/hopsfs/Resources/test_dir\n",
    "! rm -r /tmp/test_data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
