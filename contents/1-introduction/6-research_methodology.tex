This thesis is built from a few \glspl{IN}, provided by Hopsworks AB, and a few \glspl{PA}, validated through a literature study. \\ The Hopsworks's \glspl{IN} are:
\begin{itemize}
    \item[IN1 :] The Hopsworks offline feature store, supported by the legacy pipeline, exhibits high latency (over one minute) and low throughput in write operations for small-scale data (from 1 GB to 100 GB). These performances indicate the potential of employing Spark alternatives in a small-scale data scenario.
    \item[IN2 :] Hopsworks actively looks to customer needs and software's integration capabilities. Improving the read and write operations performance on their feature store and/or integrate their software with additional table formats, like Iceberg, is benefitted by all Hopsworks feature store users.
\end{itemize}
The \glspl{PA} will be validated in Chapter \ref{ch:background_and_related_work}. Those \glspl{PA} are: 
\begin{itemize}
    \item[PA1 :] Python is the most popular programming language and the most used in data science workflows. \gls{ML} and \gls{AI} developers prefer Python tools to work. Thus high-performance Python libraries will typically be preferred over \gls{JVM} or other alternatives.
    \item[PA2 :] Spark has been proved to perform worse than other options when processing data in small-scale scenarios (from 1 GB to 100 GB). Alternatives to the Spark-based system could strongly improve reading and writing operations on the Hopsworks feature store, in different ways.
\end{itemize}

This thesis work fulfill the \glspl{IN} following a system implementation and evaluation guided by its \glspl{G}. Initally, an integration between PyIceberg and \glspl{HopsFS} will be implemented \cite{niaziHopsFSScalingHierarchical2017}, followed by test to validate the approach. Then, an evaluation structure will be designed and used to compare the performances of the current legacy system, the newly integrated PyIceberg pipeline and the delta-rs pipeline developed in the related work. These experiments will involve datasets of varying sizes and evaluate critical performance metrics such as read and write latency (in seconds), and throughput (in rows/second). Those two metrics were chosen as they most affect the computational time of accessing \glspl{OTF}, thus being fair comparators between pipelines.