This thesis is built from a few \glspl{IN}, provided by Hopsworks AB, and a few \glspl{PA} validated through a literature study. \\ Hopsworks's \glspl{IN} are:
\begin{itemize}
    \item[IN1 :] The Hopsworks feature store, supported by the legacy pipeline, exhibits high latency (over one minute) and low throughput in write operations for small-scale data (from 1 GB to 100 GB). These performances indicate the potential of employing Spark alternatives in a small-scale data domain.
    \item[IN2 :] Hopsworks actively looks to customer needs and software's integration capabilities. Improving the read and write operations performance on their feature store and/or integrate their software with additional table formats, like Iceberg, is benefitted by all Hopsworks feature store users.
\end{itemize}
The \glspl{PA} will be validated in Chapter \ref{ch:background_and_related_work}. Those \glspl{PA} are: 
\begin{itemize}
    \item[PA1 :] Python is the most popular programming language and the most used in data science workflows. \gls{ML} and \gls{AI} developers prefer Python tools to work. This popularity means that high-performance Python libraries will typically be preferred over alternatives (even more efficient) that are \gls{JVM} or other environments -based.
    \item[PA2 :] Spark has been proved to perform worse than other options when processing data in small-scale (from 1 GB to 100 GB) use scenarios. Alternatives to Spark-based system, as the current legacy system, could strongly improve reading and writing operations on the Hopsworks feature store. However, different alternatives might perform differently.
\end{itemize}

This thesis work fulfill the \glspl{IN} following a system implementation and evaluation guided by its \glspl{G}. Initally, an integration between PyIceberg and \glspl{HopsFS} will be implemented \cite{niaziHopsFSScalingHierarchical2017}, followed by test to validate the approach. Then, an evaluation structure will be designed and used to compare the performances of the current legacy system, the newly integrated PyIceberg pipeline and the delta-rs pipeline developed in the related work. These experiments will involve datasets of varying sizes (from 1 GB to 100 GB) and evaluate critical performance metrics such as read and write latency, measured in seconds, and throughput, measured in rows per second. The latency will be directly measured, wile the throughput will be computed from the former. Those two metrics were chosen they most affect the computational time of accessing table formats, and can thus be used as a fair comparator between pipelines.