There are three crucial components for a comprehensive understanding of this work: the evolution of data infrastructure to data lakehouses, the role of Spark as a data management tool, and the rise of Python as the most used programming language in the data science field.

\smallskip

The term "data lakehouse" was used by Databricks in 2020 \cite{WhatLakehouse2020} to characterize a new architectural standard developing across the industry. This novel paradigm integrates the data lake's capacity to store and manage unstructured data with the \gls{ACID} features characteristic of data warehouses.
Data warehouses became the standard in the 1990s and early 2000s \cite{chaudhuriOverviewDataWarehousing1997}, facilitating firms in deriving business intelligence insights from data coming from various structured data sources. The architectural issues of this technology became evident at the end of the 2010s, with the rise of Big Data, characterized by increasing volumes, variety, and velocity of data, including significant amounts of unstructured information \cite{ederUnstructuredData802008}. 
Data lakes were thus introduced, as a more flexible and scalable solution, serving as a central repository for all data. They enable the development of more intricate built-upon architectures, including data warehouses for \gls{BI} and \gls{ML} pipelines. While being more appropriate for unstructured data, this architecture entails several complications and expenses associated with the need for duplicated data (data lake and data warehouse) and complex \gls{ELT}/\gls{ETL} pipelines.
Data lakehouse solutions addressed the challenges of data lakes by combining the best of both worlds: the flexibility and scalability of data lakes with the data governance, reliability, and performance of data warehouses. This is achieved by integrating data management and performance capabilities directly into open data formats like Parquet \cite{DremelMadeSimple}. Three pivotal technologies facilitated this paradigm:
\begin{itemize}
    \item A metadata layer, providing data lineage and facilitating efficient data discovery and access.
    \item An optimized query engine, leveraging techniques like \gls{RAM}/\gls{SSD} caching and advanced query optimization.
    \item A user-friendly \gls{API}, simplifying data access and integration with \gls{ML} and \gls{AI} applications.
\end{itemize}
Uber first open-sourced this architectural design with Hudi in 2017 \cite{rajaperumalUberEngineeringIncremental2017}, followed by Netflix in 2018, with Iceberg \cite{IcebergExamples2024}, and ultimately by Databricks, with Delta Lake in 2020 \cite{armbrustDeltaLakeHighperformance2020}.

\smallskip

Spark is a distributed computing platform designed to facilitate large-scale, data-intensive applications \cite{zaharia2010spark}. Developed as an improvement over Hadoop MapReduce (from now on just MapReduce), Spark addresses its limitations, such as high latency due to disk I/O and limited support for iterative algorithms. Spark achieves significantly higher performance by leveraging in-memory computing, eliminating the need for frequent disk access to store intermediate results. This, coupled with its DAG execution model, \glspl{RDD}, and support for multiple processing models (batch, stream, \gls{ML}, graph), has established Spark as the de facto standard for many data-intensive workloads.
While Spark offers a powerful and versatile platform, it may not always be the most suitable choice for all scenarios. For instance, Apache Flink \cite{carboneApacheFlinkStream}, specifically designed for stream and real-time processing, excels in low-latency applications where Spark may exhibit higher latency. Similarly, for small-scale datasets (from 1 GB to 100 GB), the overhead associated with launching and managing a Spark cluster can be substantial. In such cases, in-memory \gls{DBMS} like DuckDB \cite{raasveldtDuckDBEmbeddableAnalytical2019} and Polars \cite{vinkWroteOneFastest2021} offer compelling alternatives. These solutions, optimized for smaller datasets, provide high-performance \gls{OLAP} capabilities and DataFrame operations within an embedded environment, delivering significantly faster results compared to initiating a Spark cluster for equivalent tasks.
This project investigates the feasibility of employing alternatives to Spark for efficient data processing on small-scale datasets, exploring their potential advantages in terms of performance.

\smallskip

Python reigns supreme as the programming language of choice for data scientists \cite{Python_CS-R9526}. Its user-friendliness, high-level abstraction, and emphasis on clear code expression initially attracted a large and enthusiastic community \cite{StackOverflowDeveloper}. This supportive community, in turn, fueled the development of a vast ecosystem of libraries and \glspl{API} specifically designed for data science tasks. Over three decades, Python has become the de facto standard in this domain, with popular libraries like NumPy, Pandas, PyTorch, and TensorFlow, forming the backbone of countless data science projects. Python is regarded as the most popular programming language based on the volume of search results for the query (\textit{+"<language> programming"}) across 25 distinct search engines~\footnote{Evaluation methodology defined at \url{https://www.tiobe.com/tiobe-index/programminglanguages_definition/}}. The TIOBE index \cite{TIOBEIndex} and 2024 GitHub report \cite{PythonTopLanguage} underscore the current trends, clearly demonstrating Python's ascendance, also evident from the following milestones:
\begin{itemize}
    \item Python surpasses C in 2021.
    \item Python surpasses Java in 2022.
    \item Python surpasses JavaScript in 2024.
\end{itemize}
These results emphasize the need for offering Python \glspl{API}, especially for programmers and data scientists, to augment interaction and broaden the framework's possibilities.