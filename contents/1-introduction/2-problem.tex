The Hopsworks feature store \cite{10.1145/3626246.3653389} first used Hudi for its offline feature store, since in 2017 it was the first data lakehouse to be open-sourced. Hopsworks AB thus actively implemented new technologies for their software, to better cope with customer's needs. In the legacy system, Spark serves as the query engine, executing queries (read, write, or delete) on the offline feature store. The system demonstrated that a write operation on a tiny dataset, including 1 GB or less, takes one or more minutes to finalise.

This adversely affects Hopsworks' standard use case, which operates between testing scanarios on tiny data volumes (from 1 GB to 10 GB) and production scenario on higher volumes, despite still with relative small volumes (from 10 GB to 100 GB).

The fundamental hypothesis of this study is that the prolonged transaction time is a problem determined by Spark. This has prompted Hopsworks to implement Spark alternatives~\cite{Khazanchi1801362} for data ingestion in their Hudi system, and to search for alternatives to the Hudi-Spark tandem \cite{manfrediReducingReadWrite2024}.
Iceberg provides support for several \cite{OngoingEvolutionTableFormat} alternative to Spark, and Iceberg tables can be accessed directly from Python, via the PyIceberg~\footnote{PyIceberg repository accessible at \url{https://github.com/apache/iceberg-python}} library.
However, the Hopsworks feature store has not been yet integrated with Iceberg, and its underlying file system, \gls{HopsFS}~\cite{niaziHopsFSScalingHierarchical2017}, and several implementations are possible \cite{shiranApacheIcebergDefinitive2024}.


\subsection{Research Questions} 
\label{subsec:research_questions}

This research project aims to assess and compare the performance of the legacy system, relying on Hudi and Spark, against newly developed systems providing an alternative to Spark, and a comprehensive comparison of the latters. Those systems features the Rust delta-rs library~\footnote{Project repository available at \url{https://github.com/delta-io/delta-rs}}, which operates on Delta Lake tables, and the PyIceberg library~\footnotemark[\value{footnote}], which operates on Iceberg tables, in both cases hosted on \gls{HopsFS}. To do this, an implementation for the interaction of Iceberg with the Hopsworks feature store must be designed. Accordingly, this study tackles the following two \glspl{RQ}:

\begin{enumerate}
    \item[RQ1:] What are the differences in read and write latency and throughput on the Hopsworks offline feature store, between the existing legacy system and the PyIceberg alternative?
    \item[RQ2:] What are the differences in read and write latency and throughput on the Hopsworks offline feature store, between the new PyIceberg and the delta-rs alternatives?
\end{enumerate}

It is fundamental to notice that, while the measured performance is expected to be comparable if PyIceberg and/or delta-rs are included into the Hopsworks client in the future, they are officially not functioning on the offline feature store but only using the same file system, \gls{HopsFS}.