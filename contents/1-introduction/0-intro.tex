Data lakehouses' adoption reached in 2024 a critical mass, with more than 50\% of organizations running their analytics on these platform, and projection up to 67\% within 2027 \cite{StateDataLakehouse2025}. The data lakehouse architecture \cite{lakehouse2021} is cost efficient compared to traditional data warehouse and data lakes \cite{DatalakehouseCostEfficiency}, and bridges those architectures by providing the scalability of data lakes together with analytical computations and \gls{ACID} properties of data warehouses \cite{lakehouse2021}. This surge is also strongly related to the AI adoption acceleration \cite{SurgeAI2024}, which put pressure on storage solutions and data processing capabilities \cite{StateDataLakehouse2025}. In support to this, data lakehouse systems include data partitioning, reducing the query computational needs, support schemas evolution and provide "time travel" capabilities, enabling data versioning over time \cite{crociDataLakehouseHype2022}.

From the first appearance of data lakehouses \cite{WhatLakehouse2020}, three primary applications arose \cite{ApacheHudiVs}: 
\begin{enumerate} 
    \item \textbf{Apache Hudi}: introduced by Uber in 2016, then supported by Alibaba, Bytedance, Uber and Tencent \cite{rajaperumalUberEngineeringIncremental2017}.
    \item \textbf{Apache Iceberg}: open sourced by Netflix in 2018, now used by Airbnb, Apple, Expedia, LenkedIn, Lyft \cite{IcebergExamples2024}.
    \item \textbf{Delta Lake}: open sourced by Databricks in 2019 \cite{armbrustDeltaLakeHighperformance2020}, supported now by Databricks and Microsoft.
\end{enumerate}

Originally developed as an alternative to Apache Hive (from now on, Hive), Apache Iceberg (from now on, Iceberg) was designed to manage bigger datasets that underwent frequent changes beyond Hive's capabilities \cite{shiranApacheIcebergDefinitive2024}. The metadata management of Iceberg is fundamental to its design, enabling data warehouse-like capabilities via cloud object storage. Iceberg has consolidated its position in 2024 \cite{IcebergNewHadoop}, being recently included in all the major big data vendors' solutions \cite{BigDataVendorIceberg}.

Apache Hudi (from now on, Hudi), Iceberg and Delta Lake were initially designed to support a specific engine, respectively Hive, Trino and Apache Spark (from now on, Spark) \cite{ApacheHudiVs}. These three solutions have been improved to better address industry needs and flexibility issues, and now provides integrations for several engines \cite{OngoingEvolutionTableFormat}. This evolution, while beneficial, highlights a potential limitation: even solutions which have gained widespread adoption might not be the optimal solution in all the cases. For example, integrating the system with Spark, a data query and processing engine \cite{zahariaApacheSparkUnified2016}, works well for processing massive datasets (1 TB+) on the cloud, however it is yet unclear if it works well for processing smaller datasets (1 GB to 100 GB) \cite{Khazanchi1801362}. 

