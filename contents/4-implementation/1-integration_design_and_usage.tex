The first step of the system integration process consisted of analyzing the Hopsworks system and the PyIceberg tools, as described in Section \ref{sec:system_integration}. This permitted to identify which parts had to be integrated to satisfy the requirements, thus to be able to read and write on Iceberg Table hosted on \gls{HopsFS}, via the PyIceberg library. This step outlined the need of a catalog, a query engine, and a FileIO. While the formers are fundamental components of any Data lakehouse architecture explained in Section \ref{subsec:datalakehouse_architecture}, the latter is a pluggable module for performing \gls{CRUD} operations on files, specifically required by PyIceberg.

PyIceberg, being a rather recently developed library, does not provides yet all the features integrations of older Iceberg libraries \cite{iceberg_tech_docs}, such the Java \gls{API}. In addition, despite \gls{HopsFS} expose the same methods of \gls{HDFS}, the environment where HopsFS was mounted, described in Section \ref{subsec:experimental_env}, did not allow to use some catalogs. Sections \ref{subsec:integration_catalog_choice}-\ref{subsec:integration_engine_choice} describe the choices taken for both catalog (SQL Catalog) and query engine (DuckDB), describing the reason behind those choices and the problem encountered~\footnote{Code available at: https://github.com/SebastianoMeneghin/apache-iceberg-as-offline-feature-store}. Regarding the FileIO, since there was a single compatible option (PyArrowFileIO), this component could not be subject of any design comparison. Lastly, Section \ref{subsec:integration_usage} describe how instanciate the integrated system, and how to perform operations over it.

\subsection{Catalog choice}
\label{subsec:integration_catalog_choice}
At time of development, the available catalogs were:
\begin{itemize}
    \item \textbf{REST}: it is supported by \gls{HopsFS}. However, since this would have need to develop the interface from scratch, thus was discarded as not fulfilling the maintainability not-functional requirement, described in Section \ref{subsec:integration_reqs}.
    \item \textbf{\gls{AWS} Glue}: it is supported by \gls{HDFS}, but it did not pass the integration test with \gls{HopsFS}, thus was discared. Additionally, since it is a proprietary solution of \gls{AWS}, this would have lowered the reproducibility of the experiments later conducted, due to the additional costs of this solution.
    \item \textbf{\gls{AWS} DynamoDB}: it is supported by \gls{HopsFS}. Was however discarded, for the same reproducibility reason explained above.
    \item \textbf{\gls{HMS}}: it is supported by \gls{HopsFS}. \gls{HMS} is however a complex tool, developed to be tightly integrated with MapReduce and Spark environment, and perhaps perform its best in large-scale data scenarios. This was discared since not matching with purpose of avoiding Spark and its environment, and the industrial use case described in Section \ref{subsec:method_use_case}. Furthermore, this did not fulfill the maintainability not-functional requirement.
    \item \textbf{SQL Catalog}: it is supported by \gls{HopsFS}, and it could be instanciated on a SQLite database supported by PyIceberg. This was the choice for the system integration, as it is an open-source catalog, the most light-weight option among the alternatives, and needs few lines of code to be used, fulfilling both the not-functional and the functional requirements. This solution suits perfectly small-scale scenarios such this thesis' use case, but it does not fit a large-scale scenario. Furthemore, the specific SQLite database is not built for concurrency.
\end{itemize}

\subsection{Query engine choice}
\label{subsec:integration_engine_choice}
In the choice of the query engine, all the candidates are known to be suitable for both \gls{HopsFS}, since this all of these engines are supported by Hopsworks AI Data Platoform, which uses HopsFS are data storage layer. At time of development, the available query engines were:
\begin{itemize}
    \item \textbf{\gls{AWS} Athena and Snowflake}: both were discarded since they are proprietary solutions. This would have lowered the reproducibility of the experiments later conducted, due to the additional costs of this solution.
    \item \textbf{Spark}: this was discarded since it direclty violates the purpose of this project, i.e. create a Spark alternative to read and write data on \gls{OTF} stored on \gls{HopsFS}.
    \item \textbf{Presto, Trino, Flink}: were discarded as designed to perform their best in large-scale data scenarios, thus it did not suit the  industrial use case described in Section \ref{subsec:method_use_case}. Additionally, it did not fulfill the maintainability not-functional requirement, describe in Section \ref{subsec:integration_reqs}.
    \item \textbf{DuckDB}: was the chioce for the system integration, as it is a portable open-source \gls{OLAP}-\gls{DBMS}, which proved to be the best performing engine in related work on small-scale scenarios \cite{raasveldtDuckDBEmbeddableAnalytical2019,Khazanchi1801362}.
\end{itemize}


\subsection{Usage}
\label{subsec:integration_usage}
Once selected PyArrowFileIO as FileIO, SQLite as support to SQL Catalog, and DuckDB as query engine, all the libraries and dependencies are directly managed by the installation of the PyIceberg library, using the command \verb|pip install pyiceberg[pyarrow,duckdb,sql-lite]|, as described on PyIceberg documentation \cite{iceberg_tech_docs}. This integration supports all PyIceberg methods, but this Section will focus only on the methods used for the experiments. Listing \ref{lst:ch4_instanciate_catalog} describe how to instanciate an Iceberg catalog using SQLite, to enable metadata management on \gls{HopsFS} (or \gls{HDFS}), and how to create a namespace and a table within the namespace. Following this, an example of write operation on \gls{HopsFS} (or \gls{HDFS}) is described in Listing \ref{lst:ch4_iceberg_write}, while Listing \ref{lst:ch4_iceberg_read} provides an example of read operation on \gls{HopsFS} (or \gls{HDFS}).


%%%% INSTANCIATE CATALOG
\begin{minipage}{\textwidth}
    \begin{python}[caption={[Instanciate Iceberg catalog with SQLite] Instanciating an Iceberg catalog using SQLite}, label={lst:ch4_instanciate_catalog}, basicstyle=\small]
    from pyiceberg.catalog.sql import SqlCatalog

    catalog = ("default",**{
            "uri" : "sqlite:///catalog_path",
            "warehouse" : "hdfs_path", 
            "hdfs.host" : "hdfs.host" }) 
    catalog.create_namespace("ns")
    table = catalog.create_table(
                    "ns.table", 
                    schema=your_df.schema,
                    location="hdfs_path",)
    \end{python}
\end{minipage}
\medskip


%%%% WRITE WITH PYICEBERG
\begin{minipage}{\textwidth}
    \begin{python}[caption={[Writing with IcedHops] Writing a DataFrame with IcedHops on an Iceberg Table stored on \gls{HopsFS} (or \gls{HDFS}).}, label={lst:ch4_iceberg_write}, basicstyle=\small]
    import pandas as pd
    from pyiceberg.catalog import load_catalog

    df = pd.DataFrame({"num": [1, 2, 3], 
                       "letter": ["a", "b", "c"]})
    catalog = load_catalog()
    table   = catalog.load_table("ns.table")
    table.append(df)
    \end{python}
\end{minipage}
\medskip


%%%% READ WITH PYICEBERG
\begin{minipage}{\textwidth}
    \begin{python}[caption={[Reading with IcedHops] Reading a table with IcedHops from an Iceberg Table stored on \gls{HopsFS} (or \gls{HDFS}).}, label={lst:ch4_iceberg_read}, basicstyle=\small]
    from pyiceberg.catalog import load_catalog

    catalog = load_catalog()
    table   = catalog.load_table("ns.table")
    df      = table.scan().to_arrow()
    \end{python}
\end{minipage}
\medskip