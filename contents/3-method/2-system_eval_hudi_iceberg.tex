This Section explains the method and principles used for the system evaluation processes, measuring and comparing the performance (latency, in seconds, and throughput, in rows/second) of reading and writing Hudi and Iceberg tables on \gls{HopsFS}. Those operations are conducted respectively on the current legacy system and on the PyIceberg-based system, integrated in this thesis work.

\subsection{Evaluation process - RQ1 - Hudi vs. Iceberg}
\label{subsec:eval_process_hudi_iceberg}
This evaluation process will follow a sequential approach described in Figure~\ref{fig:method_experiments}. Each step of this process is related to one of the \glspl{G}3-6 associated with the \gls{RQ}1 in Section \ref{sec:intro_goals}, to which this process partially answer. The relationships between each process activity and \glspl{G} are here explained:
\begin{enumerate}
    \item \textbf{Design experiments}: this activity maps perfectly to \gls{G}3, designing the experiments that will be conducted to evaluate the performance difference in performance between the current legacy access to Apache Hudi compared to the PyIceberg library-based access to Icbeerg Tables in \gls{HopsFS}. 
    \item \textbf{Perform experiments}: this activity maps perfectly to \gls{G}4, using the integration detail (\gls{D}3-partial) to develop and conduct the designed experiments on the analyzed systems. Here, data is collected as latency, expressed in seconds.
    \item \textbf{Transform data according to metrics}: this activity is requisite to fulfill \gls{G}5, and \gls{G}8 of \gls{RQ}2. The activity is conducted because throughput is not directly measured, but it is computed from latency following the formula here below:
    \[ Throughput \; (rows/second) = \frac{Number \; of \; rows \; (rows)}{Latency \;(seconds)}\]
    \item \textbf{Visualize results}: this activity maps perfectly to \gls{G}5, visualizing the experiments' result according latency, measured in seconds, and throughput, measured in rows/second. This activity also generates \gls{D}1, the experiment results complemented with tables and histograms, presented in Chapter \ref{ch:results_and_analysis}.
    \item \textbf{Analyze results}: this activity maps perfectly to \gls{G}6, analyzing and interpreting the results delivered in \gls{D}1. This activity contributes to \gls{D}3, generating the analysis of experimental results, presented in Chapter \ref{ch:results_and_analysis}.
\end{enumerate}
\begin{figure}[!ht]
    \begin{center}
    \includegraphics[width=0.8\textwidth]{figures/3-method/method_exp.png}
    \caption[System evaluation process - Hudi vs. Iceberg]{Diagram of the system evaluation process partially answering \gls{RQ}1. Each activity is associated to specific \gls{G}. The process produces two \glspl{D}, the experiments results (\gls{D}1) and a results analysis (\gls{D}3-partial).}
    \label{fig:method_experiments}
    \end{center}
\end{figure}


%%%% INDUSTRIAL USE CASE
\subsection{Industrial use case}
\label{subsec:method_use_case}

Several choices must be made for a system evaluation: which data will be used, which environment will run the experiments, and which metrics will be used to evaluate the system. Those choices depends on the scenarios for which the system is created. Thus, this Subsection describes the typical use case for this system. Follwing, the other Subsections describe which were the decision taken accordingly. While conducting their research in Hopsworks, the author outlined a typical industrial use case for the Hopsworks feature store, by reading internal documentation and discussing with several employees on their customer needs and trends. The use case is described by:
\begin{itemize}  
  \item \textbf{Table size}: most of the Hospworks' customers' workloads were limited (from 1M to 100M rows), with only few clients needing support for massive workloads (more than 1BN rows). Thus, this project opted to improve performance for the smaller workloads (from 100k to 100M rows). The datatset selected are presented in Section~\ref{subsec:experimental_data}.
  \item \textbf{Type of data}: the Hopsworks feature store works only with structured data (e.g., numbers, strings), thus experiment design and selected datasets embody this scenario.
  \item \textbf{Rows over storage size}: In the experimental part of this thesis, in order to have a realible unit measures for table size, the number of rows will be over the storage size (bytes). Perhaps, in the structured data domain of this use case, storage size (bytes) is neither linked to a table structure nor to a storage structure, arguably making this not reliable (i.e, table with a lot of rows and few columns, and a table with few ros and a lot of columns occupies the same memory).
  \item \textbf{Client configuration}: the client configuration is modeled to reflect typical customers' clients' computational and storage capabilities. Thus, the configuration are limited between one and eight \gls{CPU} cores, \gls{RAM} just sufficient for system needs and common \gls{SSD} storages. The experimental environment is further detailed in Section~\ref{subsec:experimental_env}.
\end{itemize}



%%%% EXPERIMENTAL DATA %%%%
\subsection{Experimental data}
\label{subsec:experimental_data}

The datasets that will be used to perform read and write experiments come from \glsentryshort{TPC}-H benchmark suite~\footnote{Benchmark suite website available at \url{https://www.tpc.org/tpch/}}. \glsentryshort{TPC}-H is a decision support benchmark by \gls{TPC}, that consists in a collection of business-oriented queries in specific industry sectors \cite{transactionprocessingperformancecounciltpcTPCH_v301pdf1993}, which became the de-facto standard for experiments on data storage system. Perhaps, it has been used in related studies \cite{raasveldtDuckDBEmbeddableAnalytical2019,behmPhotonFastQuery2022,manfrediReducingReadWrite2024}. 

The \glsentryshort{TPC}-H benchmark contains eight tables, and any part of the data can be generated via the \glsentryshort{TPC}-H data generation tool~\footnote{Available at \url{https://www.tpc.org/tpc_documents_current_versions/current_specifications5.asp}}. The two tables that will be used are the SUPPLIER and the LINEITEM, respectively, the smallest (10k rows) and largest (60M rows) tables. The size (number of rows) of a table depends on the \gls{SF}, that can be varied to progressivly change in the table size. The SUPPLIER table has seven columns, while the LINEITEM table has sixteen. This difference influences the average size of memory each row occupies. Below for each table their columns are listed, specifying which data type they store.
\begin{itemize}
  \item SUPPLIER
  \begin{itemize}
    \item S\_SUPPKEY : identifier
    \item S\_NAME : fixed text, size 25
    \item S\_ADDRESS : variable text, size 40
    \item S\_NATIONKEY : identifier
    \item S\_PHONE : fixed text, size 15
    \item S\_ACCTBAL : decimal
    \item S\_COMMENT : variable text, size 101
  \end{itemize}
  \item LINEITEM
  \begin{itemize}
    \item L\_ORDERKEY : identifier
    \item L\_PARTKEY : identifier
    \item L\_SUPPKEY : identifier
    \item L\_LINENUMBER : integer
    \item L\_QUANTITY : decimal
    \item L\_EXTENDEDPRICE : decimal
    \item L\_DISCOUNT : decimal
    \item L\_TAX : decimal
    \item L\_RETURNFLAG : fixed text, size 1
    \item L\_LINESTATUS : fixed text, size 1
    \item L\_SHIPDATE : date
    \item L\_COMMITDATE : date
    \item L\_RECEIPTDATE : date
    \item L\_SHIPINSTRUCT : fixed text, size 25
    \item L\_SHIPMODE : fixed text, size 10
    \item L\_COMMENT : variable text, size 44
  \end{itemize} 
\end{itemize}

Considering the different structure of the two tables used (i.e., number of columns and data types) comparison across different tables cannot be done using the the selected metrics, i.e., latency (seconds) and throughput(rows/second). For this reason, the system evaluation will only consider same tables on different configuration. This project used five table variations to benchmark the system integration. \gls{SF} was varied to obtain a table at each significant order of magnitude from 10k to 60M rows. These are the tables:
\begin{enumerate}
    \item \textit{supplier\_sf1}: size = 10000 rows
    \item \textit{supplier\_sf10}: size = 100000 rows
    \item \textit{supplier\_sf100}: size = 1000000 rows
    \item \textit{lineitem\_sf1}: size = 6000000 rows
    \item \textit{lineitem\_sf10}: size = 60000000 rows
\end{enumerate}

Lastly, despite no information are given about the row and storage size (bytes) ratio, in accordance to Section \ref{subsec:method_use_case}, this Section presentes comprehensive information on the data, including how to retrieve it and how it is composed. These gives to the reader the ability of calculating the storage occupancy of each table, depending on their prefered unit of measure.



%%%% EXPERIMENTAL DESIGN
\subsection{Experimental design}
\label{subsec:experimental_design}

The experiments aim to highlight the differences between the current legacy system and the the newly implemented system based on the PyIceberg library. Two experimental pipelines were designed to isolate the performance of those two systems:
\begin{enumerate}
  \item \textbf{Legacy pipeline}: is the current Hopsworks feature store which stores data in Hudi tables. This system uses a pipeline based on Kafka, and Spark to write data on the Hudi tables, saved on \gls{HopsFS}. The pipeline uses a Spark alternative, DuckDB, and Arrow Flight to read data. This is described in Sections \ref{subsec:back_sys_hudi_write}-\ref{subsec:back_sys_hudi_read}.
  \item \textbf{PyIceberg - \glsentryshort{HopsFS}}: is the system implemented in Chapter \ref{ch:implementation}, which stores data in Iceberg Tables. This systems uses a pipeline based on PyIceberg, SQLite, and Arrow Flight to both write to and read from the Iceberg tables, saved on \gls{HopsFS}. This is described in Sections \ref{subsec:back_sys_iceberg_write}-\ref{subsec:back_sys_iceberg_read}.
\end{enumerate}

The experiments will verify how the performance will change based on different \gls{CPU} resources provided: one, two, four, and eight cores, according to the typical Hopsworks use case (Section \ref{subsec:method_use_case}). Each time, the experimental environment will be modified, creating a new Jupyter server where the host the experiments. The data used for experiments, as described in Section \ref{subsec:experimental_data}, will come from two different tables, modified according to a \gls{SF}, for a total of five times for each table. For the writing experiments conducted on the legacy system, different parts of the whole writing process will be measured, to verify how different parts of the legacy system will scale in the different environments described above. Those two parts are the upload and the materialization, which are explained in Section \ref{subsec:back_sys_hudi_write}. This will also help defining whether and what part of the legacy pipeline is a bottleneck.

In conclusion, the experiments conducted will be a total of two (pipelines) times four (\gls{CPU} configurations) times five (tables) times two (read and write operations), thus eighty experiments, performed each fifty times, to ensure statistical significancy to the results.



%%%% EXPERIMENTAL ENVIRONMENT
\subsection{Experimental environment}
\label{subsec:experimental_env}

The experimental environment consists of a physical machine in Hopsworks AB' offices, virtualized to enable remote shared development. The \gls{CPU} details of the machine are present in Listing \ref{lst:cpu_snurran}, noting that only eight cores at maximum were dedicated during the experiments. The machine mounts about 5.4 TBs of \gls{SSD} memory, allowing for fast read and write speed, respectively 2.7 GB/s, and 1.9 GB/s (measured with a simple \textit{dd} bash command). The experimental environment will be set up with a Jupyter Server of different CPU cores, depending on the experiment. The Jupyter server is allocated by default with 2048MB, which will be adjusted during the experiments, according to the needs (see Section \todo{add resource usage reference here}).

Notica that, despite this experimental environment is virtualized in isolation, it runs on shared resources, thus experiments result might vary depending on the machine's load. To mitigate this, all experiments will be run when the machine load is low (less than 50\% of \gls{CPU} and \gls{RAM} usage).

\begin{lstlisting}[language=bash, caption={[Experimental environment details]Output of a \textit{lscpu} bash command on the machine.}, label={lst:cpu_snurran}, frame=tb, basicstyle=\small]
Architecture:            x86_64
  CPU op-mode(s):        32-bit, 64-bit
  Address sizes:         48 bits physical, 
                         48 bits virtual
  Byte Order:            Little Endian
CPU(s):                  32
  On-line CPU(s) list:   0-31
Vendor ID:               AuthenticAMD
  Model name:            AMD Ryzen Threadripper 
                         PRO 5955WX 16-Cores
    CPU family:          25
    Model:               8
    Thread(s) per core:  2
    Core(s) per socket:  16
    Socket(s):           1
    Stepping:            2
    Frequency boost:     enabled
    CPU max MHz:         7031.2500
    CPU min MHz:         1800.0000
    BogoMIPS:            7985.56
Virtualization features: 
  Virtualization:        AMD-V
Caches (sum of all):     
  L1d:                   512 KiB (16 instances)
  L1i:                   512 KiB (16 instances)
  L2:                    8 MiB (16 instances)
  L3:                    64 MiB (2 instances)
\end{lstlisting}



%%%% EVALUATION FRAMEWORK
\subsection{Evaluation framework}
\label{subsec:method_eval_framework}

The system evaluation framework will evaluate the different system on three key aspects:
\begin{enumerate}
    \item \textbf{Functional requirements}: will be measured by verifying the success or failure of running an experiment. This will not happen by design, since the system integration phase stops only when all functional requirements are met. Those are escribed in Section \ref{subsec:integration_reqs}:
    \item \textbf{Non-functional requirements}: Consistency and maintainability are mainly addressed during integration, while scalability is measured during the system evaluation experiments. The metric used for measuring this requirement is the throughput, as defined in \gls{RQ}1.
    \item \textbf{How does the legacy system compare to PyIceberg pipelines?} this question answers directly \gls{RQ}1, measuring the throughput of the pipelines defined in Section \ref{subsec:experimental_design}. Results are then compared using a visual approach.
\end{enumerate} 



%%%% RELIABILITY AND VALIDITY
\subsection{Reliability and validity}
\label{subsec:method_reliability_validity}

Results are significant according to their reliability and validity. In this project work, each experiment will be performed fifty times to ensure the reliability of the experiment results on the system performance. This number was agreed to balance the results' consistency and the limited resources available (in terms of time and computing resources).

Due to the complex nature of the pipelines used, the data distribution of results could vary from one experiment to the other. This hampers the possibility of comparing results, significantly impacting the relevance of the results analysis. A bootstrapping technique will be used to restore the validity of the data collected. Data will be resampled with substitution a thousand times, then an average with a confidence interval for each experiment will be calculated. This will benefit the accuracy of the results presented.