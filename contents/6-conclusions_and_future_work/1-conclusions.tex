This thesis work tackled two \glspl{RQ} defined in Section \ref{subsec:research_questions}. These were: 
\begin{enumerate}
    \item[RQ1:] What are the differences in read and write latency and throughput on the Hopsworks offline feature store, between the existing legacy system and the PyIceberg alternative?
    \item[RQ2:] What are the differences in read and write latency and throughput on the Hopsworks offline feature store, between the new PyIceberg system and the delta-rs alternative?
\end{enumerate}
The work conducted in this thesis answered the first \gls{RQ} by performing a system integration, and then evaluating the newly implemented system. Thus, the second \gls{RQ} was answered by evaluating the newly implemented system against the delta-rs alternative.

The first step was integrating \gls{HopsFS} with the PyIceberg library. To complete this task, the technologies supported by \gls{HopsFS} and PyIceberg were evaluated and the promising ones tested, according to the functional and non-functional requirements, until the best technology set was selected. Each was tested in a dedicated script~\footnote{Code available at: https://github.com/SebastianoMeneghin/apache-iceberg-as-offline-feature-store}, within the experimental environment. This contributed to the creation of the integration details, described in Section~\ref{sec:integration_design_and_usage}, where the reasoning and findings behind the author's choice for SQLCatalog, DuckDB and PyArrowFileIO are explained. This can be used by developers to districate in similar complex system architectures, together with the thorough data lakehouse comparison tackled in the Section \ref{subsec:datalakehouse_comparison}. Additionally, most of the significance of this work lies in the creation of a production-ready solution, which overcame system dependencies and use case limitations, while fulfilling all specified requirements. The integration of the PyIceberg system into the production Hopsworks feature store shortly after the thesis's release perhaps serves as a powerful testament to its practical value and real-world applicability.

The second and third steps were comparing the legacy system with the newly implemented PyIceberg system, and the latter with the delta-rs alternative. Experiments were designed and conducted to measure the performance of read and write operations on those system, according to two metrics, i.e., latency (in seconds) and throughput (in rows/second), accounting for 350 hours of testing. The results presented in Section \ref{sec:major_results} showed the new system accessing Iceberg tables on \gls{HopsFS} has a latency at least fifteen times lower, for both read and write operations, for the tables contained from 10K to 6M rows. The write experiments demonstrate that the PyIceberg system significantly outperforms the legacy system in all cases, with write latency more than forty times lower across the board. This trend becomes even more striking for larger tables (6M and 60M rows), where the PyIceberg pipeline achieves write latencies that respectively are 140 and 110 times lower of those observed in the legacy system. Thexe experiments were key to identify a critical bottleneck in current Hopsworks feature store architecture, where Kafka accounted for 95\% of the wrtie latency. In the other hand, the decrease of performance gap with the growth in size of the tables suggests that for even larger tables a Spark-based system might perform better, but more experiments are needed to verify this hypothesis. A separate comparison between PyIceberg and delta-rs reveals that while PyIceberg consistently achieves lower latencies, the extent of this advantage depends on table size. For the smallest table (10K rows), the two pipelines perform almost identically, whereas for the largest table (60M rows), PyIceberg exhibits a performance edge of nearly seven times lower latency. Similarly, read performance experiments highlight considerable advantages of the PyIceberg system over the legacy system. The latency reduction ranges from 55\% for the largest table (60M rows) to as much as sixty times lower for the 100K rows table, with a consistent trend across all table sizes. This observed reduction in the performance gap is likely a result of the legacy system's use of a Spark alternative for reading, which already provides some improvements, but does not optimize resource usage as effectively as PyIceberg, especially at smaller data scales. In contrast, a comparison between PyIceberg and delta-rs shows that their read performance is nearly identical, except for the smallest table (10K rows), where PyIceberg achieves three times lower latency. This indicates that while PyIceberg is superior for write operations, its advantage diminishes in read operations, with delta-rs performing similarly. Experiments with increasing \gls{CPU} cores reveal notable differences in scalability between the three systems. The legacy system shows minimal gains from additional \gls{CPU} resources, while PyIceberg significantly benefits from multi-core execution only on reading operations, reaching 39\% performance gains with the larger tables (6M and 60M rows), and a modest 5\% for writing operations. The delta-rs, however, scales significantly better, achieving 20-30\% performance gains in write operations with the larger tables (6M and 60M rows), as well as latency reductions of 55\% for the 1M rows table and 50\% for the larger tables (6M and 60M rows) in read operations.

Overall, these findings reinforce the recommendation to adopt the newly integrated PyIceberg system in the defined use case, as it provides a substantial improvement over the legacy system, either as a replacement or as an extension for Iceberg table users. At the same time, the comparison between PyIceberg and delta-rs suggests that PyIceberg represents overall a better option for single-core machines, particularly for write operations, but delta-rs might be preferable in scenarios where multi-core scalability is crucial.