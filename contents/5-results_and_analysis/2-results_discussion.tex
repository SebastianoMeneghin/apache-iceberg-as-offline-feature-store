This Section discusses the major results, described in detail in Section \ref{sec:major_results}, explaining what they mean, which are their implication for Hopsworks AB, and for the research area. Following, this Section contains the author's considerations over the legacy system and the PyIceberg library, which were developed while conducting this thesis research. Additional consideration regarding the delta-rs library can be found in the related work's discussion \cite{manfrediReducingReadWrite2024}.



%%%%% DISCUSSION ON MAJOR RESULTS
\subsection{Discussion on major results}
The results presented in Section \ref{sec:major_results} disclose a substantial difference in the performance (both latency and throughput) of the legacy system, and the newly implemented system using the PyIceberg library. Despite the two systems behaved were tested on two different tasks, i.e. write and read operations, the PyIceberg-based system integratedin this thesis shown from 15 to 140 times lower latency than the legacy system, in all experiments conducted using tables from 10K to 6M rows. Furthermore, the results reveal that the PyIceberg and delta-rs systems differs generally much less, with the PyIceberg system outperforming the delta-rs system only in write operations with table bigger than 1M rows, where the system latency was up to seven times lower than its counterpart. These findings confirm the hypothesis that alternatives to a Spark-based system exist, and that those are preferable when operating on small tables (from 10K to 60M rows). Furthermore, the large performance differences observed indicate substantial room for optimization in the legacy system, on both read and write operations.

Going more in depth in the results of the write experiments, the newly implemented Iceberg system, has a latency from from 40 to 140 times lower than the legacy system on all the tables sizes. This system performed almost identically to the delta-rs-based system on smaller tables (10k and 100k rows), while outperformed it on larger tables (1M, 6M and 60M rows), with a latency up to seven times lower than its counterpart. In the experiments using multiple \gls{CPU} cores, the delta-rs system experienced latency decreases up to 30\% , while the PyIceberg system latency decreased of maximum 5\%. However, despite the better scalability results, the delta-rs system still underperform compared to the PyIceberg, for all the table sizes and \gls{CPU} settings used in this thesis research. The above findings clearly demonstrate that the new Iceberg-based system, in the industrial use case defined in Section \ref{subsec:method_use_case}, is a better alternative to legacy Spark-based system, while also confirming the need for Spark alternatives when operating on small tables (from 10K to 60M rows). Those alternatives could not only increase the performances, but also reduce computational costs, system complexity, and deployability, since managing a Spark cluster for small size data is expensive and more complex, and requires as well \gls{JVM} dependencies, which cannot be deployed from any Python environment manager, such as pip, or conda.

Considering now the read experiments, the results show smaller performance differences, between the legacy system and the new Iceberg system. When considering the largest table (60M rows), the new system exhibits only 55\% latency reduction, however for all the other tables the Iceberg system has a latency from 15 to 56 times lower than the legacy system latency. The delta-rs pipeline follows a very similar trend, despite slighlty underperforming the Iceberg pipeline, with a maximum of 10\% latency increase. The reason behind this different trend between new system and legacy system's performance, compared to what seen for the write operation, is that the legacy system, despite it is already using an alternative to Spark, cannot efficiently scale down resources when reading small size tables. The Spark alternative employs a combination of Arrow Flight and DuckDB, which is instanciated within the Hopsworks cluster with a predefined amount of dedicated resources. Furthermore, in the experiments using multiple \gls{CPU} cores, the delta-rs system scales better than the PyIceberg system, experiencing latency decreases of respectively 55\% and 39\%, while the legacy system is not impacted at all. The above outcomes shows that Hopsworks AB has still room for improvement even for the read operation, where employing PyIceberg or delta-rs system would increase the system overall flexibility to different scales of data and to multiple \gls{CPU} cores settings, at the price of moving computation on the client side.

All the above findings impact significantly Hopsworks AI, direclty affecting the Hopsworks feature store, which is their main product. The results recommend the adoption of one of those two new systems, over the current system, using Apache Hudi and Spark to implement an offline feature store, or the extend this system with one or both the new systems. This would allow the users of the feature store, depending on their use case (data scale), their computational resources (\glspl{CPU}), and their habits, to choose the preferred datalakehouse format to use. This would resonate with the Hopsworks AB objective of having products accessible from and integrable with multiple tools, and could be the starting point of possible further integration with currently developing technologies, such Apache XTable~\footnote{XTable repository available at \url{https://xtable.apache.org/}}. For a complete replacement of the legacy system across different use cases, further experiments should be conducted. The author envisions that beyond a certain data threshold, the Spark-based system may still outperform the new solutions due to its scalable distributed nature.

Perhaps the industrial use case described in Section \ref{subsec:method_use_case} limits the results and findings to specific data scales, computational resources, and operations. Modifying one or more of these variables could lead to different outcomes, meaning the findings cannot be fully generalized. Additionally, since this thesis was conducted in collaboration with the company developing the evaluated product, some degree of bias is inevitable. Nonetheless, the results align with research expectations, reinforcing the idea that Spark alternatives should be considered in scenarios of small size (10K to 60M rows) data processing. Additionally, these findings highlight the need for further research and experimentation on Spark alternatives, to which thousands developers already contributes everyday.



%%%%% CONSIDERATIONS ON THE LEGACY SYSTEM
\subsection{Considerations on the legacy system}
The legacy system, described in Sections \ref{subsec:back_sys_hudi_write}-\ref{subsec:back_sys_hudi_read}, consists of a multi-layered architecture that employs Spark as compute engine and Hudi as \gls{OTF}. This system is modeled to support multiple users, each having different use cases and resources. This could create several bottlenecks, depending on the scenario, that goes far beyong the high latency of instanciating a Spark cluster. An example for this issue is uses Kafka, which is used as single data entry-point in the Hopsworks feature store. Using Kafka as single entry-point ensures data consistency between the online feature store, working with streaming data, and the offline feature store, working on batch data. However, this creates a performance bottleneck for this infrastructure as data increases, needing to make a piece the full table in single rows, to save each of those as a single message. This was clearly visible in the experiments conducted in this thesis work, described in Figure \ref{fig:hudi_virtualiz_breakdown} and Table \ref{tbl:hudi_virtualiz_breakdown_cpu_perc}, where Kafka accounted for more than 95\% of the latency of the write operations. Now that these experiments outlined this problem, more research is needed to find an effective solution. Some approaches might reduce the overall latencies: (1) implement micro-batching mechanism, altogether with using columns-based format for data exchange; (2) enable client multi-processing, to support multiple settings with multiple \gls{CPU} cores; (3) refine trade-off between partitions, topic replicas and brokers.



%%%%% CONSIDERATIONS ON THE PYICEBERG LIBRARY
\subsection{Considerations on the PyIceberg library}
The system integration between Iceberg and \gls{HopsFS}, described in Section \ref{sec:integration_design_and_usage}, was possible thanks to the PyIceberg library, developed by the Iceberg open-source community for Python access to Iceberg Table. It was possible to create a Spark alternative system thanks to the high level of integrability of the library and the overarching \gls{OTF} (compared to its counterparts, as highlighted in \ref{tbl:lakehouse_comparison}), and thanks to several other online and book sources documenting this framework and library. The author's recommendation on this regard are: (1) the Iceberg and PyIceberg maintainers should revise their current documentation and integrate it with new findings, thus developers can directly access all the information needed from Iceberg webpage, without the need of buying a book, or of relying on documentation created by software companies that integrates Iceberg in their products; (2) intensively investigate solution for efficient local resources management, that might make Iceberg libraries better adaptable to changing local environment, as delta-rs shown to be able to, during the experiments described in Section \ref{sec:major_results}.


\todo{maybe add consideration on Iceberg vs. delta-rs, if not already added in Conclusions.}