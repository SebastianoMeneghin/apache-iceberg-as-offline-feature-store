This Section describes the technologies used to query, cache, and process data in this project. The category of query engine includes mainly Apache Spark and DuckDB, but technologies operating at the same abstraction level of those are here presented, namely Apache Kafka, Arrow Flight and Catalogs.

\subsection{Apache Spark}
Apache Spark is an open-source distributed computing framework designed for large-scale data processing~\cite{zahariaApacheSparkUnified2016}. It builds upon MapReduce, a distributed programming model developed by Google for handling massive datasets~\cite{dean2004mapreduce}, later adapted into Hadoop MapReduce by Yahoo! engineers~\cite{borthakurHadoopDistributedFile2005}. Spark enhances this model using \glspl{RDD}\cite{Zaharia:EECS-2011-82}, a disitrbuted memory abstraction that enables lazy in-memory computation, diffently by on-disk MapReduce's computation, that is tracked through lineage graphs. This increases fault tolernace \cite{Zaharia:EECS-2011-82} and allow to manage even bigger scale computations.

Spark supports various workloads beyond batch processing, leveraging an in-memory execution model for efficiency. Its core components include SparkSQL for querying structured data, Spark Streaming for real-time processing, MLlib for scalable \gls{ML}, and GraphX for large-scale graph processing. With support for iterative computations, Spark is particularly well-suited for \gls{ML} and graph analytics. It also provides \glspl{API} for Scala, Java, Python (via PySpark), and R, making it accessible to a broad range of users. Despite its advantages, Spark has limitations. Its in-memory execution requires significant RAM, and \gls{JVM}-based execution can lead to performance overhead due to garbage collection. Tuning performance involves fine-tuning configurations, which can be complex. Additionally, Spark Streaming's micro-batch processing introduces higher latency compared to other frameworks, making it not the best solution for small-scale datasets processing \cite{BenchmarkResultsSpark}.

\subsection{Apache Kafka}
\label{subsec:back_apache_kafka}
Apache Kafka is a robust, open-source distributed platform for handingle data streaming, that has become a cornerstone of modern data architectures~\cite{krepsKafkaDistributedMessaging2011}. Kafka supports high throughput data ingestion and processing, making it exceptionally well-suited for handling massive volumes of data in real, given its ability to manage and process continuous streams of data with very low latency. Kafka's architecture, which emphasizes fault tolerance, scalability, and durability, allows it to handle the demands of mission-critical systems that require continuous data flow and processing. \todo{Maybe add here schema of Kafka Architecture} The key components of Kafka architecture are:

\begin{enumerate}
    \item \textbf{Producer}: an application that publishes data, labeling into specific topic that group similar messages.
    \item \textbf{ZooKeeper}: the responsible for managing the Kafka cluster, including broker(s) information and topic message tracking, tracked with an offset for each topic.
    \item \textbf{Broker}: a processing node, or server, that handles data for specific topics. It receives messages from producers and delivers them to consumers upon request, enabling asynchronous communication. When for a single topic there are multiple brokers, one is elected as a leader, and the others replicate its content.
    \item \textbf{Consumer}: an application that subscribes to specific topics to receive and process data. Multiple consumers can subscribe to the same topic.
\end{enumerate}

Kafka enables applications to behave as producers and consumers, without the need of developing any synchronization protocol. This enables producers to reach high throughput, as they can broadcast messages without waiting for any acknoledgements or availability singal from the consumers. Due to its distributed architecture, a Kafka ecosystem can be optimised according to specific needs, allowing several brokers, producers and consumers to coexist.



\subsection{Catalogs}
\label{subsec:back_catalogs}
The term catalog is used in the data domain in multiple contexts, in each of those with a different definitions. In the data lakehouse context, thus in this project context, a catalog is a technical catalog also called metastore \cite{shiranApacheIcebergDefinitive2024}. As explained in Section \ref{subsec:datalakehouse_architecture}, a catalog plays an important role in tracking tables and their metadata. It is, at a minumum, the source if truth for a table's current metadata location. This is in contrast to a federated catalog, which is a central portal for data discovery and collaboration, since it tracks datasets across multiple data stores. It focuses on business needs such data governance and documentation, and sets standards for authentication and authorisation \cite{blueCatalogsRESTCatalog2024}. A federated catalog may point to tables from Cassandra, Postgres, Hiv, and other systems.

Made clarity over the catalog definition regarding this project, there are several technologies providing such tool. When data lakehouses were first created \cite{rajaperumalUberEngineeringIncremental2017,shiranApacheIcebergDefinitive2024}, the \gls{HMS} was the most popular catalog. \gls{HMS} is the technical catalog for Hive tables, but it can actually belong to both catalog categories, since it may also track \gls{JDBC} tables, and other datasets. However, scale challenges led developers and big tech companies to develope their own more scalable tools, which contributed to the development of data lakehouses technology itself, such the case of Iceberg \cite{IcebergNewHadoop}.

Data lakehouse technologies support different Catalogs, as shown in Table \ref{table:lakehouse_comparison}. Depending on the specific business need or on the available cloud resources and ecosystem, possible "pluggable" catalogs are:
\begin{itemize}
    \item \textbf{SQL Catalog}: is a C library, with Python bindings (SQLite), that provides lightweight disk-base database, that allows accessing the database with nonstandard variants of \gls{SQL}, where metadata can be saved. This does not require a separate server process, thus it is great in embedded applications, but it is not suitable for large-scale applications.
    \item \textbf{\gls{AWS} Glue}: provides a Data Catalog that serves as a managed metastore within the \gls{AWS} ecosystem. It integrates well with other\gls{AWS} services, thus it is a common choice for data lakehouses built on \gls{AWS}.
    \item \textbf{DynamoDB}: it is primarily a No~\gls{SQL} databases that due to its scalability and performance, can be used as a metastore. This is less common compared to other options.
    \item \textbf{\gls{JDBC}}: per s√© is not a catalog, but it's the standard Java \gls{API} for connecting to relational databases. In this context, \gls{JDBC} would be used to connect to and interact with database systems that stores metadata.
    \item \textbf{Nessie}: acts as a versioned metastore, providing Git-like capabilities for managing data lake tables
    \item \textbf{Databricks Unity}: is Databricks' unified governance solution, thus it is a federated catalog, which however includes also technical catalog capabilities. It provides a centralized metastore specifically designed for the Databricks Lakehouse Platform, thus is the most common option for Delta Lake-backed lakehouses \cite{AnnouncingDeltaLake2023}.
\end{itemize}

As data lakehouse frameworks grew to support more and more languages and engines, pluggable catalogs started causing some practical problems, related to compatibility. It proved indeed difficult, for commercial offerings, to support many different clients and catalog features. To overcome these problems, some developers, like Iceberg's community \cite{blueCatalogsRESTCatalog2024}, created also a REST catalog protocol, a common \gls{API} for interacting with ant catalog. The advantages of such protocol are are multiple: (1) one client implementation, for new languages or engines, can support any catalog; (2) secure table sharing is enabled, using credential vending or remote signing; (3) the amount of failures is reduced, since server-side deconfliction and retries are supported.

When implementing a new data lakehouse, the catalog decision depends on the specific integrations and feature proper of each catalog option, also considering how this can be configured on the top of the previously added layer, as shown in Table \ref{fig:lakehouse_schema}.

\subsection{Duck DB}
DuckDB \cite{raasveldtDuckDBEmbeddableAnalytical2019} is an open-source, embeddable, \gls{OLAP} \gls{DBMS} designed for efficient processing of small-scale datasets (from 1GB to 100GB) within the same process as the application using it.  This embedded, in-process operation, inspired by SQLite's success, simplifies deployment and eliminates the overhead of inter-process communication, leading to high responsiveness and low latency.  DuckDB requires no external dependencies and compiles into a single amalgamation file, enhancing portability across major operating systems and architectures, including web browsers via DuckDB-Wasm.  It offers \gls{API} for various languages, such Java, C, C++, Go, Rust and Python and supports complex \gls{SQL} queries, including window functions and \gls{ACID} properties through Multi-Version \gls{CC}.  

Data is stored in persistent, single-file databases, and secondary indexes enhance query performance.  DuckDB's columnar-vectorized query execution engine, a key design choice for \gls{OLAP} workloads, processes data in batches (vectors), significantly improving performance compared to traditional row-by-row processing systems.  While optimized for analytical queries processing significant portions of datasets, DuckDB is not designed for massive data volumes (1TB or more) that require disk-based processing, as its core processing relies on in-memory operations.  However, its extensible architecture allows for adding features like support for Parquet, JSON, and cloud storage protocols as extensions.

\subsection{Arrow Flight}
Arrow Flight is an high-performance framework designed for efficient data transfer over networks, mostly utilising Arrow tables \cite{wesmIntroducingApacheArrow2019}. This protocol facilitates the transmission of large data volumes stored in a specific format, such as Arrow tables, without the necessity of serialisation or deserialisation for transfer. This significantly accelerates the data transfer, making Arrow Flight highly efficient. Arrow Flight is engineered for cross-platform compatibility and supports many programming languages, including C++, Python, and Java. The protocol further facilitates parallelism, enhancing transmission speeds by using numerous nodes in parallel networks. The Arrow Flight protocol is constructed upon gRPC, facilitating standardisation and simplifying the construction of connectors.